# 14. FastAPI para ProducciÃ³n
 
<a id="00-prerrequisitos"></a>
 
## 0.0 Prerrequisitos
 
- Tener un proyecto con FastAPI ejecutable (local o en contenedor).
- Conocer validaciÃ³n con Pydantic (modelos request/response).
- Haber completado el mÃ³dulo 13 (Docker) para empaquetar y ejecutar la API.
 
---
 
<a id="01-protocolo-e-como-estudiar-este-modulo"></a>
 
## 0.1 ğŸ§  Protocolo E: CÃ³mo estudiar este mÃ³dulo
 
- **Antes de empezar**: abre **[Protocolo E](study_tools/PROTOCOLO_E.md)** y define el output mÃ­nimo: un servicio que levanta y responde en `/health` y `/predict`.
- **Durante el debugging**: si te atoras >15 min (schema, serializaciÃ³n, modelo no carga, 4xx/5xx), registra el caso en **[Diario de Errores](study_tools/DIARIO_ERRORES.md)**.
- **Al cierre de semana**: usa **[Cierre Semanal](study_tools/CIERRE_SEMANAL.md)** para auditar documentaciÃ³n (OpenAPI), manejo de errores y compatibilidad training-serving.
 
---
 
<a id="02-entregables-verificables-minimo-viable"></a>
 
## 0.2 âœ… Entregables verificables (mÃ­nimo viable)
 
- [ ] Endpoint `/health` estable (sin depender de cÃ³mputo pesado).
- [ ] Endpoint `/predict` con request/response validados (Pydantic).
- [ ] DocumentaciÃ³n accesible en `/docs` (Swagger) y `/openapi.json`.
- [ ] Manejo de errores consistente (`HTTPException` + cÃ³digos).
- [ ] ConversiÃ³n explÃ­cita a tipos nativos (`float`, `int`) para evitar problemas de serializaciÃ³n.
 
---
 
<a id="03-puente-teoria-codigo-portafolio"></a>
 
## 0.3 ğŸ§© Puente teorÃ­a â†” cÃ³digo (Portafolio)
 
- **Concepto**: contrato API (schemas) + loading de modelo (startup) + observabilidad bÃ¡sica
- **Archivo**: `app/fastapi_app.py`, `app/schemas.py`
- **Prueba**: `uvicorn app.fastapi_app:app --reload` y `curl http://localhost:8000/health`
 
---
 
## ğŸ¯ Objetivo del MÃ³dulo
 
Construir APIs de ML robustas, documentadas y production-ready como las del portafolio.

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘  FastAPI = El framework ideal para ML APIs                                   â•‘
â•‘                                                                              â•‘
â•‘  âœ… Type hints nativos (Pydantic)                                            â•‘
â•‘  âœ… DocumentaciÃ³n automÃ¡tica (Swagger/OpenAPI)                               â•‘
â•‘  âœ… Async support (alto throughput)                                          â•‘
â•‘  âœ… ValidaciÃ³n automÃ¡tica de requests                                        â•‘
â•‘  âœ… Dependency Injection built-in                                            â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“‹ Contenido
 
- **0.0** [Prerrequisitos](#00-prerrequisitos)
- **0.1** [Protocolo E: CÃ³mo estudiar este mÃ³dulo](#01-protocolo-e-como-estudiar-este-modulo)
- **0.2** [Entregables verificables (mÃ­nimo viable)](#02-entregables-verificables-minimo-viable)
- **0.3** [Puente teorÃ­a â†” cÃ³digo (Portafolio)](#03-puente-teoria-codigo-portafolio)
- **14.1** [Estructura de una API ML](#141-estructura-de-una-api-ml)
- **14.2** [Schemas con Pydantic](#142-schemas-con-pydantic)
- **14.3** [Endpoints de PredicciÃ³n](#143-endpoints-de-prediccion)
- **14.4** [Error Handling](#144-error-handling)
- **14.5** [CÃ³digo Real del Portafolio](#145-codigo-real-del-portafolio)
- **14.6** [ğŸ”¬ IngenierÃ­a Inversa: API ProducciÃ³n Real](#146-ingenieria-inversa-fastapi) â­ NUEVO
- [Errores habituales](#errores-habituales)
- [âœ… Checkpoint](#checkpoint)
- [âœ… Ejercicio](#ejercicio)
 
---

<a id="141-estructura-de-una-api-ml"></a>

### ğŸ§  Mapa Mental de Conceptos: FastAPI para ML

```
                          â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                          â•‘      FASTAPI PARA ML PRODUCTION      â•‘
                          â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                            â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                                  â–¼                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ENDPOINTS     â”‚              â”‚    SCHEMAS       â”‚              â”‚   LIFECYCLE      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                 â”‚                                 â”‚
â”œâ”€ /health                        â”œâ”€ PredictionRequest          â”œâ”€ Startup: load model
â”œâ”€ /predict                       â”œâ”€ PredictionResponse         â”œâ”€ Shutdown: cleanup
â”œâ”€ /batch                         â”œâ”€ ValidaciÃ³n Pydantic        â””â”€ Global state
â””â”€ /docs (auto)                   â””â”€ OpenAPI spec
```

**TÃ©rminos clave:**

| TÃ©rmino | Significado | Ejemplo |
|---------|-------------|---------|
| **Schema** | Modelo Pydantic para validar I/O | `PredictionRequest` |
| **Lifespan** | Startup/shutdown hooks | Cargar modelo una vez |
| **HTTPException** | Error HTTP estructurado | `HTTPException(404, "Not found")` |
| **Dependency** | InyecciÃ³n de dependencias | ConexiÃ³n DB, auth |

---

### ğŸ’» Ejercicio Puente: API MÃ­nima

```python
from fastapi import FastAPI

app = FastAPI()  # Crea la aplicaciÃ³n FastAPI.

@app.get("/health")  # Endpoint GET para health check.
def health():
    return {"status": "healthy"}  # Respuesta JSON indicando que el servicio estÃ¡ activo.

@app.post("/predict")  # Endpoint POST para predicciones.
def predict(data: dict):
    # TU TAREA: Cargar modelo y predecir
    return {"prediction": 0.85}  # Respuesta con la predicciÃ³n del modelo.
```

**Ejecutar:** `uvicorn app:app --reload`
**Ver docs:** `http://localhost:8000/docs`

---


### ğŸ’» Ejercicio Puente: APIs ML

> **Meta**: Practica el concepto antes de aplicarlo al portafolio.

**Ejercicio bÃ¡sico:**
1. Lee la secciÃ³n teÃ³rica siguiente
2. Identifica los patrones clave del cÃ³digo de ejemplo
3. Replica el patrÃ³n en un proyecto de prueba

---

### ğŸ› ï¸ PrÃ¡ctica del Portafolio: FastAPI en BankChurn

> **Tarea**: Aplicar este mÃ³dulo en BankChurn-Predictor.

```bash
cd BankChurn-Predictor
# Explora el cÃ³digo relacionado con APIs ML
```

**Checklist:**
- [ ] LocalicÃ© el cÃ³digo relevante
- [ ] EntendÃ­ la implementaciÃ³n actual
- [ ] IdentifiquÃ© posibles mejoras

---

### âœ… Checkpoint de Conocimiento

**Pregunta 1**: Â¿CuÃ¡l es el objetivo principal de FastAPI?

**Pregunta 2**: Â¿CÃ³mo se implementa en el portafolio?

**ğŸ”§ Escenario Debugging**: Si algo falla en APIs ML, Â¿cuÃ¡l serÃ­a tu primer paso de diagnÃ³stico?


## 14.1 Estructura de una API ML

### AnatomÃ­a TÃ­pica

```python
# app/fastapi_app.py - Estructura profesional

from contextlib import asynccontextmanager
from pathlib import Path

import joblib
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

from .schemas import PredictionRequest, PredictionResponse, HealthResponse


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LIFECYCLE: Cargar modelo al iniciar
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

model = None                             # Variable global: accesible desde todos los endpoints.

@asynccontextmanager                     # Decorador para crear context manager async.
async def lifespan(app: FastAPI):        # FunciÃ³n que gestiona startup/shutdown de la app.
    """Lifecycle: carga modelo al iniciar, limpia al cerrar."""
    global model                         # global: permite modificar la variable global desde aquÃ­.
    
    # Startup: cargar modelo
    model_path = Path("artifacts/model.joblib")  # Ruta al modelo serializado.
    if model_path.exists():              # Verifica que el archivo existe antes de cargar.
        model = joblib.load(model_path)  # Deserializa el pipeline completo.
        print(f"âœ… Modelo cargado: {model_path}")
    else:
        print(f"âš ï¸ Modelo no encontrado: {model_path}")  # Warning, no crash.
    
    yield                                # yield: aquÃ­ la app estÃ¡ corriendo y recibiendo requests.
    
    # Shutdown: limpiar recursos
    model = None                         # Libera memoria al cerrar.
    print("ğŸ›‘ App cerrada")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# APP SETUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

app = FastAPI(                           # Crea instancia de la aplicaciÃ³n FastAPI.
    title="BankChurn Predictor API",     # TÃ­tulo en Swagger UI (/docs).
    description="API para predicciÃ³n de churn de clientes bancarios",
    version="1.0.0",                     # VersiÃ³n de la API (semver).
    lifespan=lifespan,                   # Asocia el lifecycle manager definido arriba.
)

# CORS para permitir requests desde frontend
app.add_middleware(                      # Middleware: procesa requests antes/despuÃ©s de endpoints.
    CORSMiddleware,                      # Cross-Origin Resource Sharing: permite requests desde otros dominios.
    allow_origins=["*"],                 # "*" permite todo. En prod: ["https://midominio.com"].
    allow_credentials=True,              # Permite enviar cookies/auth headers.
    allow_methods=["*"],                 # Permite todos los mÃ©todos HTTP (GET, POST, etc.).
    allow_headers=["*"],                 # Permite todos los headers.
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/health", response_model=HealthResponse)  # GET /health â†’ devuelve HealthResponse.
async def health_check():                # async: permite I/O no bloqueante (mejor concurrencia).
    """Health check endpoint para load balancers/k8s."""
    return HealthResponse(               # Pydantic valida que el response cumpla el schema.
        status="healthy" if model is not None else "degraded",  # Ternario: condiciÃ³n ? si : no.
        model_loaded=model is not None,
        version="1.0.0"
    )


@app.post("/predict", response_model=PredictionResponse)  # POST /predict con body JSON.
async def predict(request: PredictionRequest):  # request: Pydantic valida el body automÃ¡ticamente.
    """Predice probabilidad de churn para un cliente."""
    if model is None:                    # VerificaciÃ³n defensiva.
        raise HTTPException(status_code=503, detail="Modelo no disponible")  # 503: Service Unavailable.
    
    # Convertir request a DataFrame
    import pandas as pd                  # Import dentro de funciÃ³n (lazy load, ok en endpoints).
    df = pd.DataFrame([request.dict()])  # dict(): convierte Pydantic model a diccionario.
    
    # Predecir
    proba = model.predict_proba(df)[0, 1]  # [0, 1]: fila 0, columna 1 (prob clase positiva).
    prediction = int(proba >= 0.5)       # Umbral 0.5: convierte probabilidad a 0/1.
    
    return PredictionResponse(           # Response tipado y validado.
        prediction=prediction,
        probability=round(proba, 4),     # round: 4 decimales de precisiÃ³n.
        risk_level="high" if proba >= 0.7 else "medium" if proba >= 0.3 else "low"  # Ternario encadenado.
    )
```

---

<a id="142-schemas-con-pydantic"></a>

## 14.2 Schemas con Pydantic

### Request/Response Models

```python
# app/schemas.py

from typing import Literal, Optional       # Literal: valores especÃ­ficos; Optional: puede ser None.
from pydantic import BaseModel, Field, validator  # BaseModel: clase base para schemas.


class PredictionRequest(BaseModel):        # Hereda de BaseModel: obtiene validaciÃ³n automÃ¡tica.
    """Schema para request de predicciÃ³n.
    
    Pydantic valida automÃ¡ticamente:
    - Tipos correctos
    - Rangos vÃ¡lidos
    - Valores permitidos
    """
    
    CreditScore: int = Field(..., ge=300, le=850, description="Credit score del cliente")
    # Field(...): ... significa REQUERIDO. ge=300: mayor o igual. le=850: menor o igual.
    Geography: Literal["France", "Germany", "Spain"] = Field(..., description="PaÃ­s")
    # Literal: SOLO acepta estos 3 valores exactos. Otros â†’ ValidationError.
    Gender: Literal["Male", "Female"] = Field(..., description="GÃ©nero")
    Age: int = Field(..., ge=18, le=100, description="Edad")
    Tenure: int = Field(..., ge=0, le=10, description="AÃ±os como cliente")
    Balance: float = Field(..., ge=0, description="Balance en cuenta")  # ge=0: no negativo.
    NumOfProducts: int = Field(..., ge=1, le=4, description="NÃºmero de productos")
    HasCrCard: Literal[0, 1] = Field(..., description="Tiene tarjeta de crÃ©dito")  # Binario.
    IsActiveMember: Literal[0, 1] = Field(..., description="Es miembro activo")
    EstimatedSalary: float = Field(..., ge=0, description="Salario estimado")
    
    class Config:                          # Config: configuraciÃ³n del modelo Pydantic.
        json_schema_extra = {              # Ejemplo para Swagger UI (/docs).
            "example": {
                "CreditScore": 650,
                "Geography": "France",
                "Gender": "Female",
                "Age": 40,
                "Tenure": 3,
                "Balance": 60000.0,
                "NumOfProducts": 2,
                "HasCrCard": 1,
                "IsActiveMember": 1,
                "EstimatedSalary": 50000.0
            }
        }


class PredictionResponse(BaseModel):
    """Schema para response de predicciÃ³n."""
    
    prediction: Literal[0, 1] = Field(..., description="0=No churn, 1=Churn")
    probability: float = Field(..., ge=0, le=1, description="Probabilidad de churn")
    risk_level: Literal["low", "medium", "high"] = Field(..., description="Nivel de riesgo")


class HealthResponse(BaseModel):
    """Schema para health check."""
    
    status: Literal["healthy", "degraded", "unhealthy"]
    model_loaded: bool
    version: str


class BatchPredictionRequest(BaseModel):
    """Schema para predicciÃ³n en batch."""
    
    customers: list[PredictionRequest] = Field(
        ..., 
        min_items=1, 
        max_items=1000,
        description="Lista de clientes (mÃ¡x 1000)"
    )


class BatchPredictionResponse(BaseModel):
    """Schema para response de batch."""
    
    predictions: list[PredictionResponse]
    processed: int
    errors: int = 0
```

---

<a id="143-endpoints-de-prediccion"></a>

## 14.3 Endpoints de PredicciÃ³n

### Single Prediction

```python
@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """
    Predice probabilidad de churn para UN cliente.
    
    - **CreditScore**: Score crediticio (300-850)
    - **Geography**: PaÃ­s (France, Germany, Spain)
    - **Gender**: GÃ©nero
    - **Age**: Edad (18-100)
    - ... etc
    
    Returns:
    - **prediction**: 0 (no churn) o 1 (churn)
    - **probability**: Probabilidad [0, 1]
    - **risk_level**: low/medium/high
    """
    if model is None:
        raise HTTPException(
            status_code=503, 
            detail="Modelo no disponible. Reinicie el servicio."
        )
    
    try:
        import pandas as pd
        df = pd.DataFrame([request.model_dump()])
        
        proba = model.predict_proba(df)[0, 1]
        prediction = int(proba >= 0.5)
        
        if proba >= 0.7:
            risk = "high"
        elif proba >= 0.3:
            risk = "medium"
        else:
            risk = "low"
        
        return PredictionResponse(
            prediction=prediction,
            probability=round(float(proba), 4),
            risk_level=risk
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error en predicciÃ³n: {str(e)}")
```

### Batch Prediction

```python
@app.post("/predict/batch", response_model=BatchPredictionResponse)
async def predict_batch(request: BatchPredictionRequest):
    """
    Predice churn para mÃºltiples clientes (mÃ¡x 1000).
    
    Ãštil para scoring masivo de cartera.
    """
    if model is None:
        raise HTTPException(status_code=503, detail="Modelo no disponible")
    
    import pandas as pd
    
    results = []
    errors = 0
    
    # Convertir todos los requests a DataFrame (mÃ¡s eficiente)
    data = [c.model_dump() for c in request.customers]
    df = pd.DataFrame(data)
    
    try:
        probas = model.predict_proba(df)[:, 1]
        
        for proba in probas:
            prediction = int(proba >= 0.5)
            risk = "high" if proba >= 0.7 else "medium" if proba >= 0.3 else "low"
            
            results.append(PredictionResponse(
                prediction=prediction,
                probability=round(float(proba), 4),
                risk_level=risk
            ))
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error en batch: {str(e)}")
    
    return BatchPredictionResponse(
        predictions=results,
        processed=len(results),
        errors=errors
    )
```

---

<a id="144-error-handling"></a>

## 14.4 Error Handling

### Custom Exception Handlers

```python
from fastapi import Request
from fastapi.responses import JSONResponse

class ModelNotLoadedError(Exception):
    """Modelo no cargado."""
    pass

class InvalidInputError(Exception):
    """Input invÃ¡lido."""
    pass


@app.exception_handler(ModelNotLoadedError)
async def model_not_loaded_handler(request: Request, exc: ModelNotLoadedError):
    return JSONResponse(
        status_code=503,
        content={
            "error": "service_unavailable",
            "message": "El modelo no estÃ¡ cargado. Intente mÃ¡s tarde.",
            "retry_after": 30
        }
    )


@app.exception_handler(InvalidInputError)
async def invalid_input_handler(request: Request, exc: InvalidInputError):
    return JSONResponse(
        status_code=400,
        content={
            "error": "invalid_input",
            "message": str(exc),
            "hint": "Verifique que todos los campos tengan valores vÃ¡lidos"
        }
    )


# Catch-all para errores no manejados
@app.exception_handler(Exception)
async def generic_exception_handler(request: Request, exc: Exception):
    return JSONResponse(
        status_code=500,
        content={
            "error": "internal_error",
            "message": "Error interno del servidor",
            "detail": str(exc) if app.debug else None
        }
    )
```

---

<a id="145-codigo-real-del-portafolio"></a>

## 14.5 CÃ³digo Real del Portafolio

### app/fastapi_app.py (BankChurn - Simplificado)

```python
"""FastAPI application for BankChurn prediction service."""

from __future__ import annotations

import logging
import os
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Literal

import joblib
import pandas as pd
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SCHEMAS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CustomerInput(BaseModel):
    CreditScore: int = Field(..., ge=300, le=850)
    Geography: str
    Gender: str
    Age: int = Field(..., ge=18, le=100)
    Tenure: int = Field(..., ge=0, le=10)
    Balance: float = Field(..., ge=0)
    NumOfProducts: int = Field(..., ge=1, le=4)
    HasCrCard: int = Field(..., ge=0, le=1)
    IsActiveMember: int = Field(..., ge=0, le=1)
    EstimatedSalary: float = Field(..., ge=0)


class PredictionOutput(BaseModel):
    prediction: int
    probability: float
    risk_level: str


class HealthOutput(BaseModel):
    status: str
    model_loaded: bool


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# APP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

model = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global model
    
    # Buscar modelo en varias ubicaciones
    paths = [
        Path("models/model_v1.0.0.pkl"),
        Path("artifacts/model.joblib"),
        Path(os.getenv("MODEL_PATH", "model.joblib")),
    ]
    
    for path in paths:
        if path.exists():
            model = joblib.load(path)
            logger.info(f"Modelo cargado: {path}")
            break
    
    if model is None:
        logger.warning("âš ï¸ NingÃºn modelo encontrado")
    
    yield
    model = None


app = FastAPI(
    title="BankChurn Predictor",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health", response_model=HealthOutput)
async def health():
    return HealthOutput(
        status="healthy" if model else "degraded",
        model_loaded=model is not None
    )


@app.post("/predict", response_model=PredictionOutput)
async def predict(customer: CustomerInput):
    if model is None:
        raise HTTPException(503, "Modelo no disponible")
    
    df = pd.DataFrame([customer.model_dump()])
    proba = model.predict_proba(df)[0, 1]
    
    return PredictionOutput(
        prediction=int(proba >= 0.5),
        probability=round(proba, 4),
        risk_level="high" if proba >= 0.7 else "medium" if proba >= 0.3 else "low"
    )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

<a id="146-ingenieria-inversa-fastapi"></a>

## 14.6 ğŸ”¬ IngenierÃ­a Inversa PedagÃ³gica: API de ProducciÃ³n Real

> **Objetivo**: Entender CADA decisiÃ³n detrÃ¡s de la API FastAPI del portafolio.

Esta secciÃ³n disecciona `app/fastapi_app.py` de BankChurn-Predictor, una API ML de producciÃ³n real.

### 14.6.1 ğŸ¯ El "Por QuÃ©" ArquitectÃ³nico

Â¿Por quÃ© la API del portafolio estÃ¡ diseÃ±ada asÃ­?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISIONES ARQUITECTÃ“NICAS DEL PORTAFOLIO                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  PROBLEMA 1: Â¿CÃ³mo cargo el modelo una sola vez sin bloquearlo en cada request? â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  RIESGO: Cargar modelo (~500MB) en cada request = 2-5s de latencia              â”‚
â”‚  DECISIÃ“N: Cargar en `lifespan` (startup), guardar en variable global           â”‚
â”‚  RESULTADO: Primera carga ~3s, requests subsecuentes ~50ms                      â”‚
â”‚  REFERENCIA: fastapi_app.py lÃ­neas 100-107                                      â”‚
â”‚                                                                                 â”‚
â”‚  PROBLEMA 2: Â¿CÃ³mo valido inputs complejos (10+ features) sin cÃ³digo manual?    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  RIESGO: ValidaciÃ³n manual = bugs, inconsistencias, cÃ³digo repetido             â”‚
â”‚  DECISIÃ“N: Pydantic con Field validators para cada feature                      â”‚
â”‚  RESULTADO: ValidaciÃ³n automÃ¡tica, errores descriptivos, docs auto-generadas    â”‚
â”‚  REFERENCIA: fastapi_app.py lÃ­neas 128-155 (CustomerData)                       â”‚
â”‚                                                                                 â”‚
â”‚  PROBLEMA 3: Â¿CÃ³mo expongo mÃ©tricas para Prometheus sin acoplar el cÃ³digo?      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  RIESGO: Sin mÃ©tricas = volar ciego en producciÃ³n                               â”‚
â”‚  DECISIÃ“N: prometheus_client con try/except (graceful degradation)              â”‚
â”‚  RESULTADO: MÃ©tricas si estÃ¡ disponible, fallback a JSON si no                  â”‚
â”‚  REFERENCIA: fastapi_app.py lÃ­neas 25-46, 284-297                               â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 14.6.2 ğŸ” AnatomÃ­a de `app/fastapi_app.py`

**Archivo**: `ML-MLOps-Portfolio/BankChurn-Predictor/app/fastapi_app.py`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE 1: Importaciones con Graceful Degradation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
try:
    from prometheus_client import Counter, Histogram, generate_latest
    PROMETHEUS_AVAILABLE = True
    
    REQUEST_COUNT = Counter(
        "bankchurn_requests_total",        # Nombre de la mÃ©trica.
        "Total HTTP requests",             # DescripciÃ³n.
        ["method", "endpoint", "status"],  # Labels para filtrar.
    )
    REQUEST_LATENCY = Histogram(
        "bankchurn_request_duration_seconds",
        "Request latency in seconds",
        ["endpoint"],
        buckets=[0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0],  # Buckets para percentiles.
    )
except ImportError:
    PROMETHEUS_AVAILABLE = False
# Â¿Por quÃ© try/except para mÃ©tricas?
# - prometheus_client es opcional (puede no estar instalado en dev).
# - La API sigue funcionando sin mÃ©tricas, pero las tiene si estÃ¡n disponibles.
# - PatrÃ³n "graceful degradation": funcionalidad reducida pero sin crash.

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE 2: Lifecycle Management (Carga de Modelo)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
predictor: Optional[ChurnPredictor] = None   # Variable global para el modelo.

@contextlib.asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifecycle."""
    global predictor
    success = load_model_logic()            # Carga modelo al iniciar.
    if not success:
        logger.warning("Application started without model loaded.")
        # NO crashea la app. Endpoint /predict devolverÃ¡ 503.
    yield                                    # App corriendo.
    # Cleanup al cerrar (opcional).
# Â¿Por quÃ© lifespan y no @app.on_event("startup")?
# - on_event estÃ¡ deprecated en FastAPI >= 0.93.
# - lifespan es el patrÃ³n moderno recomendado.
# - Permite cleanup al cerrar (conexiones DB, etc.).

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE 3: Schemas Pydantic con ValidaciÃ³n Estricta
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class CustomerData(BaseModel):
    """Schema para datos de cliente."""
    
    CreditScore: int = Field(..., ge=300, le=850)  # ...: required. ge/le: rangos.
    Geography: str = Field(...)
    Gender: str = Field(...)
    Age: int = Field(..., ge=18, le=100)
    Balance: float = Field(..., ge=0)
    # ... mÃ¡s campos ...
    
    @validator("Geography")
    def validate_geography(cls, v):
        valid = ["France", "Spain", "Germany"]
        if v not in valid:
            raise ValueError(f"Geography must be one of: {valid}")
        return v
# Â¿Por quÃ© validators personalizados?
# - Field solo valida tipos y rangos numÃ©ricos.
# - @validator permite validaciÃ³n de dominio (paÃ­ses vÃ¡lidos, formatos, etc.).
# - Error messages claros para el consumidor de la API.

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE 4: Endpoint /health (Liveness + Readiness)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@app.get("/health", response_model=HealthResponse)
async def health_check():
    uptime = time.time() - start_time
    return HealthResponse(
        status="healthy" if predictor is not None else "degraded",
        model_loaded=predictor is not None,  # Kubernetes readiness check usa esto.
        uptime_seconds=uptime,
        version="1.0.0",
    )
# Â¿Por quÃ© "degraded" en lugar de "unhealthy"?
# - "unhealthy" harÃ­a que K8s mate el pod (liveness fail).
# - "degraded" indica que funciona pero con capacidad reducida.
# - El pod sigue vivo, el equipo puede investigar.

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE 5: Endpoint /predict con MÃ©tricas
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@app.post("/predict", response_model=PredictionResponse)
async def predict_churn(customer: CustomerData):
    if predictor is None:
        if PROMETHEUS_AVAILABLE:
            REQUEST_COUNT.labels(method="POST", endpoint="/predict", status="503").inc()
        raise HTTPException(status_code=503, detail="Model not available")
    
    start_pred = time.time()
    try:
        customer_dict = customer.dict()      # Pydantic model â†’ dict.
        df = pd.DataFrame([customer_dict])   # dict â†’ DataFrame (1 fila).
        
        results = predictor.predict(df, include_proba=True)
        
        prob = float(results.iloc[0]["probability"])  # float() evita numpy.float64.
        pred = int(results.iloc[0]["prediction"])     # int() evita numpy.int64.
        
        pred_time = time.time() - start_pred
        
        # Track metrics
        if PROMETHEUS_AVAILABLE:
            REQUEST_COUNT.labels(method="POST", endpoint="/predict", status="200").inc()
            REQUEST_LATENCY.labels(endpoint="/predict").observe(pred_time)
        
        return PredictionResponse(...)
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
# Â¿Por quÃ© float() y int() explÃ­citos?
# - numpy.float64 no es JSON-serializable directamente.
# - FastAPI/Pydantic pueden fallar al serializar tipos numpy.
# - Convertir a tipos nativos de Python evita "Object of type float64 is not JSON serializable".

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE 6: Endpoint /predict_batch (OptimizaciÃ³n para Volumen)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@app.post("/predict_batch", response_model=BatchPredictionResponse)
async def predict_batch(batch_data: BatchCustomerData):
    # Vectoriza predicciones para eficiencia.
    df = pd.DataFrame([c.dict() for c in batch_data.customers])
    results = predictor.predict(df, include_proba=True)  # 1 llamada, N resultados.
    # ...
# Â¿Por quÃ© endpoint separado para batch?
# - 1 request con 1000 clientes es mÃ¡s eficiente que 1000 requests de 1.
# - El modelo puede vectorizar (GPU/CPU SIMD) las predicciones.
# - Menor overhead de red y serializaciÃ³n.
```

### 14.6.3 ğŸ§ª Laboratorio de ReplicaciÃ³n

**Tu misiÃ³n**: Implementar tu propia API de predicciÃ³n con mÃ©tricas.

1. **Crea el schema de request**:
   ```python
   # schemas.py
   from pydantic import BaseModel, Field, validator
   
   class CustomerRequest(BaseModel):
       credit_score: int = Field(..., ge=300, le=850)
       age: int = Field(..., ge=18, le=100)
       # AÃ±ade mÃ¡s campos segÃºn tu modelo
       
       @validator("credit_score")
       def score_must_be_realistic(cls, v):
           if v < 300:
               raise ValueError("Credit score too low")
           return v
   ```

2. **Implementa el lifecycle**:
   ```python
   # app.py
   from contextlib import asynccontextmanager
   
   @asynccontextmanager
   async def lifespan(app: FastAPI):
       global model
       model = joblib.load("models/best_model.pkl")
       yield
       model = None  # Cleanup
   
   app = FastAPI(lifespan=lifespan)
   ```

3. **AÃ±ade mÃ©tricas Prometheus**:
   ```python
   from prometheus_client import Counter, generate_latest
   
   PREDICTIONS = Counter("predictions_total", "Total predictions", ["result"])
   
   @app.post("/predict")  # Endpoint POST para predicciones.
   async def predict(request: CustomerRequest):
       # ... predicciÃ³n ...
       PREDICTIONS.labels(result="churn" if pred == 1 else "no_churn").inc()
       return {"prediction": pred}
   ```

### 14.6.4 ğŸš¨ Troubleshooting Preventivo

| SÃ­ntoma | Causa Probable | SoluciÃ³n |
|---------|----------------|----------|
| **"Object of type float64 is not JSON serializable"** | Retornas tipos numpy sin convertir | Usa `float(value)`, `int(value)` antes de retornar. |
| **503 "Model not available"** | Modelo no se cargÃ³ en startup | Verifica path del modelo y logs de startup. |
| **422 Unprocessable Entity** | Request no cumple schema Pydantic | Revisa el error detallado en response body. |
| **Latencia alta en /predict** | Modelo se carga en cada request | Mueve carga a `lifespan`, guarda en variable global. |
| **MÃ©tricas no aparecen en /metrics** | prometheus_client no instalado | `pip install prometheus_client` o verifica try/except. |

---

<a id="errores-habituales"></a>

## ğŸ§¨ Errores habituales y cÃ³mo depurarlos en FastAPI para ML

FastAPI te da mucho â€œgratisâ€, pero en APIs de ML los fallos suelen venir de modelos no cargados, esquemas desalineados o problemas de tipos/serializaciÃ³n.

Si alguno de estos errores te tomÃ³ **>15 minutos**, regÃ­stralo en el **[Diario de Errores](study_tools/DIARIO_ERRORES.md)** y aplica el flujo de **rescate cognitivo** de **[Protocolo E](study_tools/PROTOCOLO_E.md)**.

### 1) El modelo no se carga (503 constantes)

**SÃ­ntomas tÃ­picos**

- El endpoint `/predict` responde `503 Modelo no disponible`.
- Logs con mensajes tipo `Modelo no encontrado` o `NingÃºn modelo encontrado`.

**CÃ³mo identificarlo**

- Revisa la funciÃ³n `lifespan` o cÃ³digo de startup: Â¿la ruta del modelo (`models/`, `artifacts/`) existe dentro del contenedor?
- Comprueba variables de entorno como `MODEL_PATH`.

**CÃ³mo corregirlo**

- Asegura rutas consistentes entre entrenamiento, Dockerfile y FastAPI.
- En local, imprime (`logger.info`) la ruta exacta desde la que intentas cargar y verifica que el archivo estÃ© ahÃ­.

---

### 2) Esquema Pydantic desalineado con el pipeline

**SÃ­ntomas tÃ­picos**

- Errores `KeyError` o `Column not found` al predecir.
- El modelo espera columnas con ciertos nombres pero el `PredictionRequest` usa otros.

**CÃ³mo identificarlo**

- Compara los campos del schema (`CreditScore`, `Geography`, etc.) con las columnas que el pipeline de sklearn espera.

**CÃ³mo corregirlo**

- Usa los **mismos nombres de features** que en el training pipeline.
- Si renombraste columnas en feature engineering, refleja esos cambios en el schema y en la transformaciÃ³n de entrada antes de llamar al modelo.

---

### 3) Problemas de tipos y serializaciÃ³n

**SÃ­ntomas tÃ­picos**

- Errores `TypeError: Object of type ... is not JSON serializable`.
- Respuestas con valores `NaN` o `Infinity` que rompen el cliente.

**CÃ³mo identificarlo**

- Revisa el tipo real de lo que devuelves en `PredictionResponse` (por ejemplo, `numpy.float32` en vez de `float`).

**CÃ³mo corregirlo**

- Convierte explÃ­citamente a tipos nativos de Python (`float`, `int`, `str`).
- AsegÃºrate de que no devuelves `NaN` o `inf` (redondea o reemplaza por valores vÃ¡lidos).

---

### 4) CORS o healthcheck mal configurados

**SÃ­ntomas tÃ­picos**

- El frontend no puede llamar al API por errores de CORS.
- Kubernetes/Compose marcan el servicio como unhealthy.

**CÃ³mo identificarlo**

- Revisa configuraciÃ³n de `CORSMiddleware` y el endpoint `/health`.

**CÃ³mo corregirlo**

- En desarrollo puedes usar `allow_origins=["*"]`, pero en producciÃ³n limita a tus dominios.
- Verifica que `/health` no dependa de modelos pesados para responder rÃ¡pido y con 200.

---

### 5) PatrÃ³n general de debugging en APIs de ML

1. Llama al endpoint con `curl` o `httpie` usando el `example` del schema.
2. Mira los logs del servidor (uvicorn) para ver tracebacks completos.
3. Verifica rutas de modelo y variables de entorno que afectan al loading.
4. AsegÃºrate de que lo que entra/sale del API coincide con lo que tu modelo entrenado espera.

Con esta disciplina, tu API FastAPI pasarÃ¡ de â€œfunciona solo en localâ€ a estar lista para producciÃ³n.

---

<a id="ejercicio"></a>

## âœ… Ejercicio

1. Implementa `/predict/batch` para procesar mÃºltiples clientes
2. AÃ±ade endpoint `/model/info` que retorne metadata del modelo
3. Implementa rate limiting bÃ¡sico

---

## ğŸ“¦ CÃ³mo se UsÃ³ en el Portafolio

Cada proyecto tiene una API FastAPI en `app/fastapi_app.py`:

### API de BankChurn

```python
# BankChurn-Predictor/app/fastapi_app.py (estructura)
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="BankChurn Predictor API")

class PredictionRequest(BaseModel):
    CreditScore: int
    Geography: str
    Gender: str
    Age: int
    Balance: float
    # ... mÃ¡s features

class PredictionResponse(BaseModel):
    prediction: int
    probability: float
    risk_level: str

@app.get("/health")  # Endpoint GET para health check.
async def health():
    return {"status": "healthy", "model_loaded": model is not None}

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    features = request.dict()
    df = pd.DataFrame([features])
    prediction = pipeline.predict(df)[0]
    probability = pipeline.predict_proba(df)[0, 1]
    return PredictionResponse(
        prediction=int(prediction),
        probability=float(probability),
        risk_level="high" if probability > 0.7 else "low"
    )
```

### APIs por Proyecto

| Proyecto | Endpoint Principal | Tipo |
|----------|-------------------|------|
| BankChurn | `/predict` | ClasificaciÃ³n binaria |
| CarVision | `/predict` | RegresiÃ³n |
| TelecomAI | `/predict` | ClasificaciÃ³n multiclase |

### ğŸ”§ Ejercicio: Prueba las APIs Reales

```bash
# 1. Inicia API de BankChurn
cd BankChurn-Predictor
uvicorn app.fastapi_app:app --reload

# 2. Prueba con curl
curl http://localhost:8000/health

curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"CreditScore": 650, "Geography": "France", ...}'

# 3. Ve docs interactivos
# http://localhost:8000/docs
```

---

<a id="checkpoint"></a>

## âœ… Checkpoint

- [ ] `/health` responde rÃ¡pido (no hace inferencia ni carga pesada)
- [ ] `/predict` valida request/response con Pydantic
- [ ] Devuelves tipos nativos (sin `numpy.float32`, sin `NaN/inf`)
- [ ] Errores esperables se manejan con `HTTPException` y cÃ³digos correctos
- [ ] `/docs` y `/openapi.json` son accesibles

---

## ğŸ’¼ Consejos Profesionales

> **Recomendaciones para destacar en entrevistas y proyectos reales**

### Para Entrevistas

1. **Pydantic + FastAPI**: Explica cÃ³mo la validaciÃ³n automÃ¡tica reduce cÃ³digo.

2. **Async vs Sync**: CuÃ¡ndo usar cada uno (IO-bound vs CPU-bound).

3. **OpenAPI/Swagger**: DocumentaciÃ³n automÃ¡tica como feature de FastAPI.

### Para Proyectos Reales

| SituaciÃ³n | Consejo |
|-----------|---------|
| ML Serving | Carga modelo en startup, no en cada request |
| ValidaciÃ³n | Usa Pydantic para input/output schemas |
| Errores | HTTPException con cÃ³digos y mensajes claros |
| ProducciÃ³n | Gunicorn + Uvicorn workers |

### Endpoints Esenciales para ML

```python
/health          â†’ Liveness check
/ready           â†’ Readiness check (modelo cargado)
/predict         â†’ Inferencia principal
/predict/batch   â†’ Inferencia batch
/model/info      â†’ VersiÃ³n, mÃ©tricas, metadata
```


---

## ğŸ“º Recursos Externos del MÃ³dulo

> ğŸ·ï¸ Sistema: ğŸ”´ Obligatorio | ğŸŸ¡ Recomendado | ğŸŸ¢ Complementario

### ğŸ¬ Videos

| ğŸ·ï¸ | TÃ­tulo | Canal | DuraciÃ³n | Link |
|:--:|:-------|:------|:--------:|:-----|
| ğŸ”´ | **FastAPI Full Course** | SebastiÃ¡n RamÃ­rez | 1h | [YouTube](https://www.youtube.com/watch?v=0sOvCWFmrtA) |
| ğŸ”´ | **ML APIs with FastAPI** | ArjanCodes | 30 min | [YouTube](https://www.youtube.com/watch?v=kBIX3_cMHzE) |
| ğŸŸ¡ | **Pydantic V2 Tutorial** | ArjanCodes | 25 min | [YouTube](https://www.youtube.com/watch?v=502XOB0u8OY) |

### ğŸ“„ DocumentaciÃ³n

| ğŸ·ï¸ | Recurso | DescripciÃ³n |
|:--:|:--------|:------------|
| ğŸ”´ | [FastAPI Docs](https://fastapi.tiangolo.com/) | DocumentaciÃ³n oficial |
| ğŸŸ¡ | [Pydantic v2](https://docs.pydantic.dev/latest/) | ValidaciÃ³n de datos |

---

## âš–ï¸ DecisiÃ³n TÃ©cnica: ADR-004 FastAPI

**Contexto**: Necesitamos framework para APIs de inferencia ML.

**DecisiÃ³n**: Usar FastAPI como framework para todas las APIs.

**Alternativas Consideradas**:
- **Flask**: Simple pero sync, validaciÃ³n manual
- **Django REST**: Overkill para microservicios ML
- **gRPC**: MÃ¡s rÃ¡pido pero mÃ¡s complejo

**Consecuencias**:
- âœ… ValidaciÃ³n automÃ¡tica con Pydantic
- âœ… Docs OpenAPI auto-generadas
- âœ… Async nativo para alto throughput
- âŒ Framework relativamente nuevo

---

## ğŸ”§ Ejercicios del MÃ³dulo

### Ejercicio 14.1: Schemas Pydantic
**Objetivo**: Definir schemas de request/response.
**Dificultad**: â­â­

```python
from pydantic import BaseModel, Field

# TU TAREA: Crear schemas para endpoint /predict
# Request: customer features
# Response: prediction + probability + model_version
```

<details>
<summary>ğŸ’¡ Ver soluciÃ³n</summary>

```python
from pydantic import BaseModel, Field
from typing import Optional

class PredictRequest(BaseModel):
    """Schema de entrada para predicciÃ³n."""
    credit_score: int = Field(..., ge=300, le=850, description="Credit score")
    age: int = Field(..., ge=18, le=100, description="Customer age")
    tenure: int = Field(..., ge=0, le=50, description="Years as customer")
    balance: float = Field(..., ge=0, description="Account balance")
    num_products: int = Field(..., ge=1, le=4, description="Number of products")
    has_credit_card: bool = Field(default=True)
    is_active_member: bool = Field(default=True)
    
    model_config = {
        "json_schema_extra": {
            "examples": [{
                "credit_score": 650,
                "age": 35,
                "tenure": 5,
                "balance": 50000.0,
                "num_products": 2,
                "has_credit_card": True,
                "is_active_member": True
            }]
        }
    }

class PredictResponse(BaseModel):
    """Schema de salida para predicciÃ³n."""
    prediction: int = Field(..., description="0=No churn, 1=Churn")
    probability: float = Field(..., ge=0, le=1, description="Churn probability")
    risk_level: str = Field(..., description="low/medium/high")
    model_version: str = Field(..., description="Model version used")
    
    model_config = {
        "json_schema_extra": {
            "examples": [{
                "prediction": 1,
                "probability": 0.73,
                "risk_level": "high",
                "model_version": "1.2.0"
            }]
        }
    }
```
</details>

---

### Ejercicio 14.2: Endpoint Completo
**Objetivo**: Implementar endpoint /predict con manejo de errores.
**Dificultad**: â­â­â­

```python
# TU TAREA: Implementar endpoint que:
# 1. Reciba PredictRequest validado
# 2. Cargue modelo (cached)
# 3. Haga predicciÃ³n
# 4. Devuelva PredictResponse
# 5. Maneje errores apropiadamente
```

<details>
<summary>ğŸ’¡ Ver soluciÃ³n</summary>

```python
from fastapi import FastAPI, HTTPException
from functools import lru_cache
import joblib

app = FastAPI(title="Churn Prediction API")

@lru_cache()
def load_model():
    """Carga modelo una sola vez."""
    try:
        return joblib.load("artifacts/model.joblib")
    except FileNotFoundError:
        raise RuntimeError("Model not found")

@app.get("/health")  # Endpoint GET para health check.
async def health():
    return {"status": "healthy"}  # Respuesta JSON indicando que el servicio estÃ¡ activo.

@app.post("/predict", response_model=PredictResponse)
async def predict(request: PredictRequest):
    """Predice probabilidad de churn."""
    try:
        model = load_model()
    except RuntimeError as e:
        raise HTTPException(status_code=503, detail=str(e))
    
    # Preparar features
    features = [[
        request.credit_score,
        request.age,
        request.tenure,
        request.balance,
        request.num_products,
        int(request.has_credit_card),
        int(request.is_active_member)
    ]]
    
    try:
        prediction = int(model.predict(features)[0])
        probability = float(model.predict_proba(features)[0][1])
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction error: {e}")
    
    # Determinar nivel de riesgo
    risk_level = "high" if probability > 0.7 else "medium" if probability > 0.3 else "low"
    
    return PredictResponse(
        prediction=prediction,
        probability=probability,
        risk_level=risk_level,
        model_version="1.0.0"
    )
```
</details>

---

## ğŸ”— Glosario del MÃ³dulo

| TÃ©rmino | DefiniciÃ³n |
|---------|------------|
| **FastAPI** | Framework web async para APIs Python con validaciÃ³n automÃ¡tica |
| **Pydantic** | LibrerÃ­a de validaciÃ³n de datos usando type hints |
| **OpenAPI** | EspecificaciÃ³n estÃ¡ndar para documentar APIs (antes Swagger) |
| **@lru_cache** | Decorator para cachear resultados de funciones |

---

## ğŸª¤ La Trampa â€” Errores Comunes de Este MÃ³dulo

### Trampa 1: API sin validaciÃ³n de entrada

**SÃ­ntoma**:
```python
@app.post("/predict")
def predict(data: dict):  # âŒ Acepta cualquier cosa
    return model.predict(data["features"])
```

**SoluciÃ³n**:
```python
from pydantic import BaseModel, Field

class PredictRequest(BaseModel):
    features: list[float] = Field(..., min_items=4, max_items=4)

@app.post("/predict")
def predict(request: PredictRequest):  # âœ… Validado
    return model.predict([request.features])
```

---

### Trampa 2: Modelo cargado en cada request

**SÃ­ntoma**: API lenta porque carga el modelo en cada request.

**SoluciÃ³n**:
```python
@app.on_event("startup")
async def load_model():
    global model
    model = joblib.load("model.pkl")

# O con dependency injection
from functools import lru_cache

@lru_cache
def get_model():
    return joblib.load("model.pkl")

@app.post("/predict")
def predict(request: PredictRequest, model = Depends(get_model)):
    return model.predict(...)
```

---

### Trampa 3: Logs sin contexto de request

**SÃ­ntoma**: Logs sin forma de correlacionar quÃ© request fallÃ³.

**SoluciÃ³n**: AÃ±adir request_id con middleware:
```python
class RequestIDMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        request_id = str(uuid.uuid4())[:8]
        with logger.contextualize(request_id=request_id):
            response = await call_next(request)
            response.headers["X-Request-ID"] = request_id
        return response
```

---

## ğŸ“ Quiz del MÃ³dulo â€” Semanas 19-20

### Quiz Semana 19: FastAPI

#### Pregunta 1 (25 pts)
Â¿Por quÃ© usar Pydantic schemas en lugar de `dict` para requests?

<details>
<summary>âœ… Respuesta</summary>

1. **ValidaciÃ³n automÃ¡tica**: Tipos, rangos, formatos
2. **DocumentaciÃ³n**: OpenAPI generada automÃ¡ticamente
3. **Seguridad**: Rechaza payloads malformados antes de llegar al cÃ³digo
4. **Autocompletado**: IDE sabe quÃ© campos existen
</details>

#### Pregunta 2 (25 pts)
Â¿CÃ³mo evitas cargar el modelo en cada request?

<details>
<summary>âœ… Respuesta</summary>

Usar `@app.on_event("startup")` o `@lru_cache`:
```python
@lru_cache
def get_model():
    return joblib.load("model.pkl")

@app.post("/predict")
def predict(model = Depends(get_model)):
    ...
```
</details>

#### Pregunta 3 (25 pts)
Â¿Por quÃ© es importante el endpoint `/health`?

<details>
<summary>âœ… Respuesta</summary>

1. **Load balancers**: Verifican si el servicio estÃ¡ vivo
2. **Kubernetes**: Probes de readiness/liveness
3. **Monitoring**: Alertas si el servicio no responde
4. **Debugging**: Verificar conectividad bÃ¡sica
</details>

#### ğŸ”§ Ejercicio PrÃ¡ctico (25 pts)

Crea un endpoint `/predict` con schema de entrada validado (age 18-100, balance â‰¥0) y respuesta estructurada (prediction, probability, risk_level).

<details>
<summary>âœ… SoluciÃ³n</summary>

```python
from pydantic import BaseModel, Field
from fastapi import FastAPI

class PredictRequest(BaseModel):
    age: int = Field(..., ge=18, le=100)
    balance: float = Field(..., ge=0)

class PredictResponse(BaseModel):
    prediction: int
    probability: float
    risk_level: str

@app.post("/predict", response_model=PredictResponse)
def predict(request: PredictRequest):
    features = [[request.age, request.balance]]
    pred = model.predict(features)[0]
    prob = model.predict_proba(features)[0][1]
    risk = "high" if prob > 0.7 else "medium" if prob > 0.3 else "low"
    return PredictResponse(prediction=pred, probability=prob, risk_level=risk)
```
</details>

---

<div align="center">

**Siguiente mÃ³dulo** â†’ [15. Streamlit](15_STREAMLIT.md)

---

[â† Volver al Ãndice](00_INDICE.md)

</div>
