# üéØ Simulacro de Entrevista Mid-Level ML Engineer
## Portafolio MLOps ‚Äî 60 Preguntas T√©cnicas

**Nivel**: Mid (2-4 a√±os de experiencia)  
**Versi√≥n**: 1.0 | Diciembre 2025

---

## üìã √çndice

1. [Pipelines y Arquitectura](#1-pipelines-y-arquitectura-preguntas-1-15)
2. [MLOps Pr√°ctico](#2-mlops-pr√°ctico-preguntas-16-30)
3. [Testing y Calidad](#3-testing-y-calidad-preguntas-31-40)
4. [Deployment y APIs](#4-deployment-y-apis-preguntas-41-50)
5. [Escenarios Pr√°cticos](#5-escenarios-pr√°cticos-preguntas-51-60)

---

## üéØ ¬øQu√© se espera de un Mid-Level?

| S√≠ se espera | No se espera (a√∫n) |
|--------------|-------------------|
| Dise√±ar pipelines end-to-end | Arquitecturas distribuidas complejas |
| Implementar CI/CD funcional | Optimizaci√≥n de infraestructura a escala |
| Debugging aut√≥nomo | Mentoring de equipos |
| Code reviews | Decisiones de arquitectura cr√≠ticas |
| Escribir tests comprehensivos | Dise√±o de sistemas desde cero |

---

# 1. Pipelines y Arquitectura (Preguntas 1-15) {#1-pipelines-y-arquitectura-preguntas-1-15}

## Pregunta 1: Pipeline Unificado
**¬øPor qu√© usar un Pipeline unificado en lugar de artefactos separados?**

### Respuesta:
```python
# ‚ùå Antes: artefactos separados
preprocessor = joblib.load("preprocessor.pkl")  # Cargar preprocesador.
model = joblib.load("model.pkl")                # Cargar modelo por separado.
X = preprocessor.transform(X)                   # Transformar manualmente.
pred = model.predict(X)                         # Predecir.

# ‚úÖ Despu√©s: pipeline unificado
pipe = joblib.load("pipeline.joblib")           # TODO en un archivo.
pred = pipe.predict(X)                          # Una llamada hace todo.
```

**Beneficios**:
1. Elimina training-serving skew
2. Single source of truth
3. Versionado simple
4. Deploy m√°s limpio

---

## Pregunta 2: ColumnTransformer
**Explica el ColumnTransformer del portafolio.**

### Respuesta:
```python
preprocessor = ColumnTransformer([               # Aplica transformaciones por tipo de columna.
    ('num', Pipeline([                           # Pipeline para num√©ricas.
        ('imputer', SimpleImputer(strategy='median')),  # Imputa con mediana.
        ('scaler', StandardScaler())             # Estandariza.
    ]), numerical_cols),
    ('cat', Pipeline([                           # Pipeline para categ√≥ricas.
        ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))  # One-hot encoding.
    ]), categorical_cols),
], remainder='drop')                             # Elimina columnas no especificadas.
```

**Procesa columnas en paralelo**: num√©ricas y categ√≥ricas tienen transformaciones distintas.

---

## Pregunta 3: Custom Transformer
**¬øCu√°ndo crear un transformer personalizado?**

### Respuesta:
```python
class FeatureEngineer(BaseEstimator, TransformerMixin):  # Hereda para compatibilidad con Pipeline.
    def fit(self, X, y=None):                    # fit: aprende de datos (aqu√≠ no hace nada).
        return self                              # Retorna self para encadenar.
    
    def transform(self, X):                      # transform: aplica transformaci√≥n.
        X = X.copy()                             # Copia para no modificar original.
        X['vehicle_age'] = 2024 - X['model_year']  # Crea feature derivada.
        return X
```

**Cu√°ndo usar**:
- L√≥gica de negocio espec√≠fica
- Features derivadas
- Transformaciones no est√°ndar

---

## Pregunta 4: Estratified Split
**¬øPor qu√© stratify=y en train_test_split?**

### Respuesta:
```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42  # stratify: mantiene proporci√≥n de clases.
)
```

Con clases desbalanceadas (80/20 churn), `stratify=y` garantiza que train y test mantengan la misma proporci√≥n. Sin esto, un split aleatorio podr√≠a dar 85/15 en train y 70/30 en test.

---

## Pregunta 5: Hyperparameter Tuning
**¬øC√≥mo optimizas hiperpar√°metros?**

### Respuesta:
```python
from sklearn.model_selection import RandomizedSearchCV  # B√∫squeda aleatoria de hiperpar√°metros.

param_dist = {
    'model__n_estimators': [50, 100, 200],       # model__: accede a params del paso 'model'.
    'model__max_depth': [5, 10, 20, None]
}

search = RandomizedSearchCV(
    pipe, param_dist, n_iter=20, cv=5, scoring='f1'  # n_iter: 20 combinaciones aleatorias.
)
search.fit(X_train, y_train)                     # Prueba combinaciones con CV.
print(search.best_params_)                       # Mejores hiperpar√°metros encontrados.
```

**GridSearch vs RandomizedSearch**: Random es m√°s eficiente con muchos par√°metros.

---

## Pregunta 6: M√©tricas de Negocio
**¬øC√≥mo traduces m√©tricas ML a valor de negocio?**

### Respuesta:
```python
# Costo de falsos negativos (cliente que churns sin detectar)
cost_fn = 500                                    # Costo de adquirir cliente nuevo.

# Costo de falsos positivos (retenci√≥n innecesaria)
cost_fp = 50                                     # Costo de campa√±a de retenci√≥n.

# Costo total
total_cost = (FN * cost_fn) + (FP * cost_fp)     # M√©trica de negocio: minimizar esto.
```

Optimizar para **minimizar costo total**, no solo accuracy.

---

## Pregunta 7: Ensemble Methods
**Explica VotingClassifier con soft voting.**

### Respuesta:
```python
ensemble = VotingClassifier([                    # Combina m√∫ltiples modelos.
    ('lr', LogisticRegression()),                # Modelo lineal.
    ('rf', RandomForestClassifier())             # Modelo no-lineal.
], voting='soft', weights=[0.4, 0.6])            # soft: promedia probabilidades. weights: RF pesa m√°s.
```

- **Soft voting**: Promedia probabilidades (mejor que votos binarios)
- **Weights**: RF tiene m√°s peso porque tiene mejor AUC individual
- **Complementariedad**: LR lineal + RF no-lineal = menor varianza

---

## Pregunta 8: Cross-Validation Avanzado
**¬øCu√°ndo usar TimeSeriesSplit vs StratifiedKFold?**

### Respuesta:
| Tipo | Usar cuando |
|------|-------------|
| StratifiedKFold | Clasificaci√≥n con clases desbalanceadas |
| TimeSeriesSplit | Datos temporales (evitar data leakage temporal) |
| GroupKFold | Datos con grupos (ej: m√∫ltiples muestras por paciente) |

```python
from sklearn.model_selection import TimeSeriesSplit  # CV para datos temporales.
tscv = TimeSeriesSplit(n_splits=5)               # 5 splits temporales.
# Train: [1,2,3], Test: [4]                      # Nunca usa datos futuros para entrenar.
# Train: [1,2,3,4], Test: [5]                    # El train crece, test siempre es "futuro".
```

---

## Pregunta 9: Feature Importance
**¬øC√≥mo explicas qu√© features son importantes?**

### Respuesta:
```python
# 1. Importancia de RF
importances = model.feature_importances_         # Importancia basada en reducci√≥n de impureza.

# 2. Permutation importance (m√°s robusto)
from sklearn.inspection import permutation_importance
perm = permutation_importance(model, X_test, y_test)  # Permuta features y mide impacto.

# 3. SHAP (m√°s interpretable)
import shap
explainer = shap.TreeExplainer(model)            # Explainer para modelos de √°rboles.
shap_values = explainer.shap_values(X_test)      # Contribuci√≥n de cada feature por predicci√≥n.
```

---

## Pregunta 10: Handling Categorical High Cardinality
**¬øC√≥mo manejas categor√≠as con muchos valores √∫nicos?**

### Respuesta:
```python
# 1. Target encoding (con cuidado de leakage)
from category_encoders import TargetEncoder      # Codifica con media del target.
encoder = TargetEncoder()                        # ‚ö†Ô∏è Usar solo en train para evitar leakage.

# 2. Frequency encoding
X['brand_freq'] = X['brand'].map(X['brand'].value_counts(normalize=True))  # Frecuencia relativa.

# 3. Grouping rare categories
X['brand'] = X['brand'].apply(lambda x: x if freq[x] > 0.01 else 'Other')  # Agrupa raras en 'Other'.
```

---

## Pregunta 11: Reproducibilidad
**¬øC√≥mo garantizas experimentos reproducibles?**

### Respuesta:
```python
# 1. Seeds
SEED = 42
np.random.seed(SEED)                             # Seed numpy.
random.seed(SEED)                                # Seed random.

# 2. Config versionada
config = BankChurnConfig.from_yaml("configs/config.yaml")  # Pydantic valida.

# 3. MLflow tracking
mlflow.log_params(config.model.dict())           # Guarda hiperpar√°metros.
mlflow.log_artifact("configs/config.yaml")       # Guarda archivo de config.

# 4. Dependencias fijas
# pyproject.toml con versiones espec√≠ficas       # sklearn==1.3.0, no sklearn.
```

---

## Pregunta 12: Data Validation
**¬øC√≥mo validas datos de entrada en producci√≥n?**

### Respuesta:
```python
from pydantic import BaseModel, Field, validator  # Pydantic para validaci√≥n.

class PredictionInput(BaseModel):                # Schema de entrada.
    credit_score: int = Field(ge=300, le=850)    # ge/le: rangos v√°lidos.
    age: int = Field(ge=18, le=100)
    geography: str
    
    @validator('geography')                      # Validador custom.
    def validate_geography(cls, v):              # v: valor a validar.
        valid = ['France', 'Germany', 'Spain']
        if v not in valid:
            raise ValueError(f'Must be one of {valid}')  # Error descriptivo.
        return v                                 # Retorna valor validado.
```

Pydantic valida antes de que llegue al modelo.

---

## Pregunta 13: Config Management
**¬øPor qu√© Pydantic para configuraci√≥n?**

### Respuesta:
```python
class ModelConfig(BaseModel):
    model_type: Literal["rf", "lr", "xgb"]        # Solo estos valores permitidos.
    n_estimators: int = Field(ge=10, le=1000)     # Rango v√°lido.
    
    @validator('n_estimators')                    # Validaci√≥n cross-field.
    def validate_estimators(cls, v, values):     # values: otros campos ya validados.
        if values.get('model_type') == 'lr' and v != 1:
            raise ValueError('LR no usa n_estimators')  # L√≥gica de negocio.
        return v
```

**Beneficios**: Validaci√≥n autom√°tica, tipos claros, errores descriptivos, documentaci√≥n impl√≠cita.

---

## Pregunta 14: Artifact Management
**¬øC√≥mo organizas artefactos del modelo?**

### Respuesta:
```
artifacts/
‚îú‚îÄ‚îÄ pipeline.joblib       # Modelo + preprocessor
‚îú‚îÄ‚îÄ training_results.json # M√©tricas
‚îú‚îÄ‚îÄ config.yaml          # Config usada
‚îî‚îÄ‚îÄ feature_names.json   # Features esperadas
```

```python
# Guardar
joblib.dump(pipe, 'artifacts/pipeline.joblib')   # Serializa pipeline completo.
with open('artifacts/training_results.json', 'w') as f:
    json.dump(metrics, f)                        # Guarda m√©tricas como JSON.
```

---

## Pregunta 15: Model Versioning
**¬øC√≥mo versionas modelos?**

### Respuesta:
```python
# 1. MLflow Model Registry
mlflow.sklearn.log_model(pipe, "model")          # Guarda modelo en MLflow.
# Registrar como v1, v2, etc.                    # UI permite promover a staging/production.

# 2. Naming convention
model_name = f"bankchurn_v{version}_{timestamp}.joblib"  # Nombre descriptivo.

# 3. Git tags
git tag -a v1.0.0 -m "Model v1.0.0: AUC 0.85"   # Asocia versi√≥n de c√≥digo con modelo.
```

---

# 2. MLOps Pr√°ctico (Preguntas 16-30) {#2-mlops-pr√°ctico-preguntas-16-30}

## Pregunta 16: MLflow Tracking
**¬øC√≥mo usas MLflow para tracking?**

### Respuesta:
```python
import mlflow

with mlflow.start_run():                         # Context manager: crea y cierra run.
    mlflow.log_params({"n_estimators": 100, "max_depth": 10})  # Hiperpar√°metros.
    mlflow.log_metrics({"auc": 0.85, "f1": 0.78})  # M√©tricas de evaluaci√≥n.
    mlflow.sklearn.log_model(pipe, "model")      # Guarda modelo serializado.
    mlflow.log_artifact("configs/config.yaml")   # Guarda archivos adicionales.
```

---

## Pregunta 17: DVC
**¬øPara qu√© usas DVC?**

### Respuesta:
```bash
# Trackear datos
dvc add data/raw/Churn.csv                       # Crea .dvc file, a√±ade datos a .gitignore.

# Push a remote
dvc push                                         # Sube datos a remote (S3, GCS, etc.).

# Pull datos
dvc pull                                         # Descarga datos del remote.
```

**Beneficio**: Versionar datos grandes sin subirlos a Git.

---

## Pregunta 18: GitHub Actions CI
**Explica el workflow CI del portafolio.**

### Respuesta:
```yaml
name: CI
on: [push, pull_request]                         # Triggers del workflow.

jobs:
  test:
    runs-on: ubuntu-latest                       # Runner de GitHub.
    steps:
      - uses: actions/checkout@v4                # Clona el repo.
      - run: pip install -e ".[dev]"             # Instala dependencias.
      - run: pytest tests/ --cov=src             # Ejecuta tests con coverage.
      - run: ruff check src/                     # Linting.
```

**Flujo**: Push ‚Üí Install ‚Üí Test ‚Üí Lint ‚Üí Pass/Fail badge.

---

## Pregunta 19: Pre-commit Hooks
**¬øQu√© hooks usas?**

### Respuesta:
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit  # Linter r√°pido.
    hooks:
      - id: ruff                                 # Verifica estilo.
      - id: ruff-format                          # Auto-formatea.
  - repo: https://github.com/pre-commit/mirrors-mypy
    hooks:
      - id: mypy                                 # Verificaci√≥n de tipos.
```

Ejecutan autom√°ticamente antes de cada commit.

---

## Pregunta 20: Docker Multi-stage
**Explica el Dockerfile del portafolio.**

### Respuesta:
```dockerfile
# Build stage
FROM python:3.11-slim AS builder                # Stage de compilaci√≥n.
COPY requirements.txt .
RUN pip wheel --no-cache-dir -w /wheels -r requirements.txt  # Genera wheels.

# Runtime stage
FROM python:3.11-slim                           # Imagen limpia, sin herramientas de build.
COPY --from=builder /wheels /wheels             # Copia solo wheels del builder.
RUN pip install --no-cache /wheels/*            # Instala sin compilar.
COPY . /app
USER nonroot                                    # Seguridad: no root.
CMD ["uvicorn", "app:app", "--host", "0.0.0.0"] # Comando de inicio.
```

**Multi-stage**: Build pesado en stage 1, runtime ligero en stage 2.

---

## Pregunta 21: Training-Serving Skew
**¬øQu√© es training-serving skew y c√≥mo lo evitas?**

### Respuesta:
Training-serving skew ocurre cuando el modelo ve datos diferentes en producci√≥n vs entrenamiento.

**Causas comunes**:
```python
# ‚ùå MAL: Preprocesamiento diferente
# Training
X_train['age_normalized'] = (X_train['age'] - X_train['age'].mean()) / X_train['age'].std()  # Stats de train.

# Serving (usa stats de producci√≥n, no de training!)
X_prod['age_normalized'] = (X_prod['age'] - X_prod['age'].mean()) / X_prod['age'].std()  # Stats de prod ‚Üí SKEW.
```

**Soluci√≥n: Pipeline unificado**:
```python
# ‚úÖ BIEN: Todo en un pipeline
pipe = Pipeline([
    ('scaler', StandardScaler()),                # Guarda mean/std de training internamente.
    ('model', RandomForestClassifier())
])
pipe.fit(X_train, y_train)                       # fit: aprende stats de train.
joblib.dump(pipe, 'model.joblib')                # Serializa TODO junto.

# En producci√≥n: mismo pipeline
pipe = joblib.load('model.joblib')               # Carga con stats de train.
pred = pipe.predict(X_new)                       # transform usa stats originales.
```

---

## Pregunta 22: Data Drift Detection
**¬øC√≥mo detectas data drift en producci√≥n?**

### Respuesta:
```python
from evidently.metrics import DataDriftTable     # Evidently: librer√≠a de monitoreo ML.
from evidently.report import Report

# Comparar distribuciones
report = Report(metrics=[DataDriftTable()])      # M√©trica de drift.
report.run(reference_data=X_train, current_data=X_prod)  # Compara train vs producci√≥n.
report.save_html("drift_report.html")            # Genera reporte visual.
```

**M√©todos estad√≠sticos**:
| M√©todo | Uso | Umbral t√≠pico |
|--------|-----|---------------|
| **PSI** (Population Stability Index) | Categ√≥ricas | >0.2 = drift significativo |
| **KS-test** (Kolmogorov-Smirnov) | Num√©ricas | p-value < 0.05 |
| **JS Divergence** | Distribuciones | >0.1 = drift |

**En el portafolio**: Configurable en `16_OBSERVABILIDAD.md`.

---

## Pregunta 23: M√©tricas de Producci√≥n
**¬øQu√© m√©tricas monitoreas en producci√≥n?**

### Respuesta:
```python
# Prometheus metrics en FastAPI
from prometheus_client import Counter, Histogram  # Cliente Prometheus para Python.

PREDICTIONS = Counter('predictions_total', 'Total predictions', ['model_version'])  # Contador.
LATENCY = Histogram('prediction_latency_seconds', 'Prediction latency')  # Histograma.

@app.post("/predict")
async def predict(data: Input):
    with LATENCY.time():                         # Mide tiempo autom√°ticamente.
        result = model.predict(data)
    PREDICTIONS.labels(model_version="v1.2").inc()  # Incrementa contador con label.
    return result
```

**M√©tricas clave**:
| Categor√≠a | M√©tricas |
|-----------|----------|
| **Rendimiento** | Latencia p50/p95/p99, throughput |
| **Disponibilidad** | Error rate, uptime |
| **ML espec√≠ficas** | Prediction distribution, feature distributions |
| **Negocio** | Conversiones, costos evitados |

---

## Pregunta 24: Rollback de Modelos
**¬øC√≥mo haces rollback si un modelo falla?**

### Respuesta:
```python
# 1. Versionado de modelos
models/
‚îú‚îÄ‚îÄ v1.0.0/pipeline.joblib  # ‚Üê Rollback aqu√≠
‚îú‚îÄ‚îÄ v1.1.0/pipeline.joblib
‚îî‚îÄ‚îÄ v1.2.0/pipeline.joblib  # Actual (fallando)

# 2. Blue-Green deployment
# deployment.yaml
spec:
  replicas: 2
  selector:
    matchLabels:
      version: v1.1.0  # Cambiar a versi√≥n anterior

# 3. Con MLflow
client = MlflowClient()
client.transition_model_version_stage(
    name="bankchurn",
    version=3,
    stage="Production"  # Promover versi√≥n anterior
)
```

**Proceso de rollback**:
1. Detectar degradaci√≥n (alertas de m√©tricas)
2. Cambiar variable de entorno o config
3. Reiniciar pods / recargar modelo
4. Verificar m√©tricas post-rollback

---

## Pregunta 25: A/B Testing en ML
**¬øC√≥mo implementas A/B testing para modelos?**

### Respuesta:
```python
import random

@app.post("/predict")
async def predict(data: Input, user_id: str):
    # Asignar bucket consistente por usuario
    bucket = hash(user_id) % 100                 # Hash determinista: mismo user = mismo bucket.
    
    if bucket < 10:                              # 10% tr√°fico al challenger.
        model = model_v2                         # Challenger: modelo nuevo.
        version = "v2"
    else:
        model = model_v1                         # Champion: modelo actual.
        version = "v1"
    
    result = model.predict(data)
    
    # Logging para an√°lisis
    log_prediction(user_id, version, result)     # Guarda para comparar m√©tricas.
    
    return {"prediction": result, "model_version": version}
```

**M√©tricas a comparar**:
- Accuracy/F1 en cohortes
- M√©tricas de negocio (conversi√≥n, revenue)
- Latencia y error rate

---

## Pregunta 26: Manejo de Secrets
**¬øC√≥mo manejas secrets y credenciales?**

### Respuesta:
```python
# ‚ùå MAL: Hardcoded
API_KEY = "TU_API_KEY_AQUI"                      # NUNCA hacer esto: queda en Git.

# ‚úÖ BIEN: Variables de entorno
import os
API_KEY = os.getenv("API_KEY")                   # Lee de variable de entorno.

# ‚úÖ MEJOR: python-dotenv
from dotenv import load_dotenv
load_dotenv()                                    # Carga variables de .env al entorno.
API_KEY = os.getenv("API_KEY")                   # Ahora disponible.
```

**.env (nunca en Git)**:
```bash
# .env (valores de ejemplo)
API_KEY=REEMPLAZAR_EN_ENTORNO_REAL
DB_PASSWORD=REEMPLAZAR_EN_ENTORNO_REAL
```

**.gitignore**:
```gitignore
.env
.env.*
!.env.example
```

**En CI/CD**: GitHub Secrets ‚Üí `${{ secrets.API_KEY }}`

---

## Pregunta 27: Feature Store
**¬øQu√© es un feature store y cu√°ndo usarlo?**

### Respuesta:
Feature store = repositorio centralizado de features reutilizables.

```python
# Sin feature store (problema)
# Equipo A: calcula age_bucket de una forma
# Equipo B: calcula age_bucket de otra forma
# ‚Üí Inconsistencia                              # Cada equipo tiene su versi√≥n.

# Con feature store (soluci√≥n)
from feast import FeatureStore                   # Feast: feature store open source.

store = FeatureStore(repo_path=".")              # Conecta al store.
features = store.get_online_features(            # Obtiene features en tiempo real.
    features=["customer:age_bucket", "customer:tenure_months"],  # Formato: tabla:feature.
    entity_rows=[{"customer_id": "C123"}]        # Entidad a buscar.
)
```

**Cu√°ndo usar**:
| Situaci√≥n | Feature Store |
|-----------|---------------|
| 1-2 modelos, equipo peque√±o | No necesario |
| M√∫ltiples modelos, features compartidas | Recomendado |
| Features en tiempo real | Muy recomendado |

---

## Pregunta 28: Escalado de Inferencia
**¬øC√≥mo escalas inferencia para alto tr√°fico?**

### Respuesta:
```yaml
# Kubernetes HPA (Horizontal Pod Autoscaler)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler               # Escala pods autom√°ticamente.
metadata:
  name: bankchurn-api
spec:
  scaleTargetRef:                           # Deployment a escalar.
    apiVersion: apps/v1
    kind: Deployment
    name: bankchurn-api
  minReplicas: 2                            # M√≠nimo 2 pods siempre.
  maxReplicas: 10                           # M√°ximo 10 pods.
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70              # Escala cuando CPU > 70%.
```

**Estrategias**:
| Estrategia | Cu√°ndo |
|------------|--------|
| **HPA** | Tr√°fico variable, latencia cr√≠tica |
| **Batch processing** | Alto volumen, latencia flexible |
| **Caching** | Inputs repetidos frecuentes |
| **Model optimization** | Latencia muy baja requerida |

---

## Pregunta 29: Logging en ML
**¬øQu√© informaci√≥n loggeas en producci√≥n?**

### Respuesta:
```python
import logging
import json

logger = logging.getLogger(__name__)             # Logger por m√≥dulo.

@app.post("/predict")
async def predict(data: Input):
    request_id = str(uuid.uuid4())               # ID √∫nico para trazar request.
    
    # Log de entrada
    logger.info(json.dumps({                     # JSON estructurado para an√°lisis.
        "event": "prediction_request",
        "request_id": request_id,
        "features": data.dict(),                 # Features de entrada.
        "timestamp": datetime.utcnow().isoformat()
    }))
    
    start = time.time()                          # Medir latencia.
    result = model.predict(data)
    latency = time.time() - start
    
    # Log de salida
    logger.info(json.dumps({
        "event": "prediction_response",
        "request_id": request_id,                # Mismo ID para correlacionar.
        "prediction": result,
        "probability": float(proba),
        "latency_ms": latency * 1000,            # Latencia en ms.
        "model_version": "v1.2.0"                # Versi√≥n para debugging.
    }))
    
    return result
```

**Logs esenciales**: request_id, inputs, outputs, latencia, versi√≥n, errores.

---

## Pregunta 30: Retraining Autom√°tico
**¬øC√≥mo automatizas el retraining?**

### Respuesta:
```yaml
# GitHub Actions scheduled workflow
name: Weekly Retrain
on:
  schedule:
    - cron: '0 2 * * 0'                         # Domingos 2am UTC.
  workflow_dispatch:                            # Permite trigger manual.

jobs:
  retrain:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: pip install -e ".[dev]"
      - run: python main.py --config configs/config.yaml  # Entrena modelo.
      - run: python scripts/evaluate.py --threshold 0.80  # Valida m√©tricas.
      - run: |                                  # Deploy condicional.
          if [ $? -eq 0 ]; then                 # Si evaluaci√≥n pas√≥.
            echo "Model passed threshold, deploying..."
            # Deploy logic
          fi
```

**Triggers de retraining**:
| Trigger | Implementaci√≥n |
|---------|----------------|
| **Scheduled** | Cron jobs, Airflow |
| **Data drift** | Alerta ‚Üí trigger workflow |
| **Performance degradation** | M√©tricas bajo umbral |
| **New data volume** | X nuevos registros |

---

# 3. Testing y Calidad (Preguntas 31-40) {#3-testing-y-calidad-preguntas-31-40}

## Pregunta 31: Tipos de Tests
**¬øQu√© tipos de tests tiene el portafolio?**

### Respuesta:
```python
# Unit test - prueba una funci√≥n aislada
def test_feature_engineer():
    fe = FeatureEngineer()
    result = fe.transform(sample_df)             # Prueba transform.
    assert 'vehicle_age' in result.columns       # Verifica output esperado.

# Integration test - prueba m√∫ltiples componentes juntos
def test_training_pipeline():
    trainer = Trainer(config)
    trainer.fit(X, y)                            # Prueba flujo completo.
    assert trainer.model_ is not None            # Verifica que modelo existe.

# API test - prueba endpoint HTTP
def test_predict_endpoint():
    response = client.post("/predict", json=sample_input)  # Request HTTP.
    assert response.status_code == 200           # Verifica respuesta exitosa.
```

---

## Pregunta 32: Fixtures
**¬øC√≥mo usas fixtures en pytest?**

### Respuesta:
```python
@pytest.fixture                                  # Fixture: setup reutilizable.
def sample_data():
    return pd.DataFrame({                        # Datos de prueba.
        'CreditScore': [650, 700],
        'Age': [35, 45],
        'Exited': [0, 1]
    })

@pytest.fixture
def trained_model(sample_data):                  # Fixture puede usar otra fixture.
    trainer = Trainer(config)
    trainer.fit(sample_data)                     # Entrena con datos de prueba.
    return trainer

def test_predict(trained_model, sample_data):    # Test recibe fixtures como args.
    preds = trained_model.predict(sample_data)
    assert len(preds) == len(sample_data)        # Verifica cantidad de predicciones.
```

---

## Pregunta 33: Coverage
**¬øCu√°nto coverage es suficiente?**

### Respuesta:
```bash
pytest tests/ --cov=src --cov-report=html       # --cov: mide coverage de src/. --cov-report: genera HTML.
```

| Nivel | Coverage | Comentario |
|-------|----------|------------|
| M√≠nimo | 70% | Lo b√°sico |
| Bueno | 80% | Est√°ndar industria |
| Excelente | 90%+ | C√≥digo cr√≠tico |

**El portafolio tiene 79% en BankChurn.**

---

## Pregunta 34: Property-Based Testing
**¬øQu√© es property-based testing?**

### Respuesta:
En lugar de casos espec√≠ficos, defines **propiedades** que siempre deben cumplirse.

```python
from hypothesis import given, strategies as st   # Hypothesis: property-based testing.

@given(                                          # @given genera casos de prueba autom√°ticos.
    credit_score=st.integers(min_value=300, max_value=850),  # Estrategia: enteros en rango.
    age=st.integers(min_value=18, max_value=100)
)
def test_prediction_is_valid(credit_score, age):
    """Propiedad: la predicci√≥n siempre es 0 o 1."""  # Define invariante.
    input_data = {"credit_score": credit_score, "age": age}
    pred = model.predict(pd.DataFrame([input_data]))
    assert pred[0] in [0, 1]                     # Debe cumplirse para TODOS los inputs.

@given(df=st.data())                             # st.data(): permite draw din√°mico.
def test_feature_engineer_preserves_rows(df):
    """Propiedad: FeatureEngineer no cambia n√∫mero de filas."""
    sample = df.draw(st.dataframes(columns=[     # Genera DataFrame aleatorio.
        st.column("age", dtype=int),
        st.column("salary", dtype=float)
    ]))
    result = fe.transform(sample)
    assert len(result) == len(sample)            # Filas deben preservarse.
```

**Ventaja**: Encuentra edge cases que no pensaste.

---

## Pregunta 35: Testing de Modelos ML
**¬øC√≥mo testeas que un modelo funciona correctamente?**

### Respuesta:
```python
# 1. Test de smoke: modelo carga y predice
def test_model_loads_and_predicts():
    model = joblib.load("artifacts/pipeline.joblib")  # Carga modelo.
    sample = pd.DataFrame([{"CreditScore": 650, "Age": 35}])
    pred = model.predict(sample)                 # Debe poder predecir.
    assert len(pred) == 1                        # Una predicci√≥n por fila.

# 2. Test de formato de salida
def test_prediction_format():
    pred = model.predict(X_test)
    assert pred.shape == (len(X_test),)          # Shape correcto.
    assert set(pred).issubset({0, 1})            # Solo valores v√°lidos.

# 3. Test de rendimiento m√≠nimo
def test_model_performance():
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    assert accuracy >= 0.75, f"Accuracy {accuracy} below threshold"  # Umbral m√≠nimo.

# 4. Test de invarianza
def test_prediction_deterministic():
    pred1 = model.predict(X_test)
    pred2 = model.predict(X_test)
    assert np.array_equal(pred1, pred2)          # Misma entrada = misma salida.
```

---

## Pregunta 36: Mocking
**¬øQu√© es mocking y cu√°ndo usarlo?**

### Respuesta:
Mocking = reemplazar dependencias reales con objetos simulados.

```python
from unittest.mock import Mock, patch           # Mock: simula objetos. patch: reemplaza.

# Mockear llamada a API externa
@patch('myapp.external_api.get_customer_data')  # Reemplaza esta funci√≥n.
def test_predict_with_external_data(mock_api):  # mock_api: objeto mock.
    # Configurar mock
    mock_api.return_value = {"credit_score": 700, "age": 45}  # Respuesta simulada.
    
    # Test usa el mock en lugar de API real
    result = predict_for_customer("C123")       # No llama API real.
    
    # Verificar que se llam√≥
    mock_api.assert_called_once_with("C123")    # Verifica argumentos.
    assert result is not None

# Mockear modelo para test de API
@patch('app.fastapi_app.model')                 # Reemplaza el modelo.
def test_predict_endpoint(mock_model):
    mock_model.predict.return_value = np.array([1])  # Simula predicci√≥n.
    mock_model.predict_proba.return_value = np.array([[0.2, 0.8]])
    
    response = client.post("/predict", json=sample_input)
    assert response.json()["prediction"] == 1   # Usa valor del mock.
```

**Cu√°ndo usar**: APIs externas, base de datos, servicios lentos.

---

## Pregunta 37: Testing de APIs
**¬øC√≥mo testeas endpoints de FastAPI?**

### Respuesta:
```python
from fastapi.testclient import TestClient       # Cliente para tests sin servidor.
from app.fastapi_app import app

client = TestClient(app)                        # Crea cliente de prueba.

def test_health_endpoint():
    response = client.get("/health")            # GET request.
    assert response.status_code == 200          # 200 OK.
    assert response.json()["status"] == "healthy"  # Verifica contenido.

def test_predict_valid_input():
    response = client.post("/predict", json={   # POST con JSON body.
        "credit_score": 650,
        "age": 35,
        "geography": "France"
    })
    assert response.status_code == 200
    assert "prediction" in response.json()      # Verifica campos de respuesta.
    assert "probability" in response.json()

def test_predict_invalid_input():
    response = client.post("/predict", json={
        "credit_score": 9999,                   # Fuera de rango: Pydantic rechaza.
        "age": 35
    })
    assert response.status_code == 422          # 422: Validation error.

def test_predict_missing_field():
    response = client.post("/predict", json={
        "credit_score": 650                     # Falta age: campo requerido.
    })
    assert response.status_code == 422
```

---

## Pregunta 38: Parametrized Tests
**¬øC√≥mo evitas duplicaci√≥n en tests?**

### Respuesta:
```python
import pytest

@pytest.mark.parametrize("credit_score,age,expected", [  # Lista de casos.
    (300, 18, 0),                               # M√≠nimos v√°lidos.
    (850, 100, 1),                              # M√°ximos v√°lidos.
    (650, 45, 0),                               # Caso t√≠pico.
])
def test_prediction_cases(credit_score, age, expected):  # Se ejecuta 3 veces.
    input_data = {"credit_score": credit_score, "age": age}
    pred = model.predict(pd.DataFrame([input_data]))
    assert pred[0] in [0, 1]                    # Verificamos formato, no valor exacto.

@pytest.mark.parametrize("invalid_input,expected_error", [  # Casos de error.
    ({"credit_score": -1}, "greater than or equal to 300"),
    ({"credit_score": 1000}, "less than or equal to 850"),
    ({"age": 5}, "greater than or equal to 18"),
])
def test_validation_errors(invalid_input, expected_error):
    response = client.post("/predict", json=invalid_input)
    assert response.status_code == 422          # Todos deben dar 422.
    assert expected_error in str(response.json())  # Mensaje de error esperado.
```

---

## Pregunta 39: Testing de Edge Cases
**¬øC√≥mo testeas edge cases en ML?**

### Respuesta:
```python
# 1. Inputs vac√≠os
def test_empty_dataframe():
    df = pd.DataFrame()                          # DataFrame sin filas.
    with pytest.raises(ValueError):              # Debe lanzar error.
        model.predict(df)

# 2. Nulls
def test_missing_values():
    df = pd.DataFrame([{"CreditScore": None, "Age": 35}])  # NaN en feature.
    # Pipeline debe manejar o fallar graciosamente
    result = model.predict(df)                   # SimpleImputer deber√≠a manejar.

# 3. Outliers extremos
def test_extreme_values():
    df = pd.DataFrame([{
        "CreditScore": 850,
        "Age": 100,
        "Balance": 1_000_000_000                 # Outlier extremo.
    }])
    pred = model.predict(df)
    assert pred[0] in [0, 1]                     # Debe predecir sin fallar.

# 4. Tipos incorrectos
def test_wrong_types():
    with pytest.raises(Exception):               # Debe fallar con tipo incorrecto.
        model.predict("not a dataframe")         # String en vez de DataFrame.

# 5. Columnas faltantes
def test_missing_columns():
    df = pd.DataFrame([{"CreditScore": 650}])   # Falta Age.
    with pytest.raises(KeyError):                # Pipeline necesita todas las columnas.
        model.predict(df)
```

---

## Pregunta 40: Test-Driven Development (TDD)
**¬øC√≥mo aplicas TDD en ML?**

### Respuesta:
TDD: Escribir test ‚Üí Ver que falla ‚Üí Implementar ‚Üí Ver que pasa ‚Üí Refactorizar.

```python
# 1. Escribir test primero
def test_feature_engineer_creates_age_bucket():
    df = pd.DataFrame({"age": [25, 45, 65]})     # Datos de prueba.
    fe = FeatureEngineer()
    result = fe.transform(df)
    
    assert "age_bucket" in result.columns        # Verifica que crea columna.
    assert list(result["age_bucket"]) == ["young", "middle", "senior"]  # Valores esperados.

# 2. Test falla (FeatureEngineer no existe a√∫n)  # Red: test falla.
# 3. Implementar m√≠nimo para pasar               # Green: c√≥digo m√≠nimo.
class FeatureEngineer(BaseEstimator, TransformerMixin):
    def transform(self, X):
        X = X.copy()
        X["age_bucket"] = pd.cut(                # pd.cut: binning.
            X["age"], 
            bins=[0, 30, 50, 100],               # Rangos de edad.
            labels=["young", "middle", "senior"] # Etiquetas.
        )
        return X

# 4. Test pasa ‚úì                                 # Verificar que pasa.
# 5. Refactorizar si es necesario               # Mejorar sin romper tests.
```

**En ML, TDD es √∫til para**:
- Feature engineering (definir comportamiento esperado)
- Validaci√≥n de datos
- APIs

---

# 4. Deployment y APIs (Preguntas 41-50) {#4-deployment-y-apis-preguntas-41-50}

## Pregunta 41: FastAPI Basics
**Muestra un endpoint de predicci√≥n.**

### Respuesta:
```python
from fastapi import FastAPI                      # Framework web.
from pydantic import BaseModel                   # Validaci√≥n de datos.

app = FastAPI()                                  # Crea aplicaci√≥n.

class Input(BaseModel):                          # Schema de entrada.
    credit_score: int
    age: int

@app.post("/predict")                            # Endpoint POST.
def predict(data: Input):                        # FastAPI valida autom√°ticamente.
    X = pd.DataFrame([data.dict()])              # Convierte a DataFrame.
    pred = model.predict(X)                      # Predice.
    return {"prediction": int(pred[0])}          # Retorna JSON.
```

---

## Pregunta 42: Health Checks
**¬øPor qu√© tener /health endpoint?**

### Respuesta:
```python
@app.get("/health")                              # GET endpoint para health checks.
def health():
    return {
        "status": "healthy",                     # Estado general.
        "model_loaded": model is not None,       # Verifica que modelo carg√≥.
        "version": "1.0.0"                       # Versi√≥n para debugging.
    }
```

Kubernetes usa esto para saber si el pod est√° listo.

---

## Pregunta 43: Uvicorn y ASGI
**¬øQu√© es uvicorn y por qu√© usarlo?**

### Respuesta:
Uvicorn = servidor ASGI (Asynchronous Server Gateway Interface) de alto rendimiento.

```bash
# Desarrollo
uvicorn app.fastapi_app:app --reload --port 8000  # --reload: hot reload con cambios.

# Producci√≥n
uvicorn app.fastapi_app:app --host 0.0.0.0 --port 8000 --workers 4  # --workers: procesos paralelos.
```

**Configuraci√≥n para producci√≥n**:
```python
# Con gunicorn + uvicorn workers
gunicorn app.fastapi_app:app \                   # gunicorn: gestor de procesos.
    --workers 4 \                                # 4 procesos worker.
    --worker-class uvicorn.workers.UvicornWorker \  # Workers ASGI.
    --bind 0.0.0.0:8000                          # Puerto y host.
```

**ASGI vs WSGI**:
| WSGI | ASGI |
|------|------|
| Sync only | Async + Sync |
| Flask, Django | FastAPI, Starlette |
| Una request a la vez por worker | M√∫ltiples requests concurrentes |

---

## Pregunta 44: CORS Configuration
**¬øC√≥mo manejas CORS en FastAPI?**

### Respuesta:
CORS = Cross-Origin Resource Sharing. Necesario cuando frontend y backend est√°n en dominios distintos.

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware  # Middleware para CORS.

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=[                              # Or√≠genes permitidos.
        "http://localhost:3000",                 # React dev.
        "https://myapp.example.com",             # Producci√≥n.
    ],
    allow_credentials=True,                      # Permite cookies.
    allow_methods=["GET", "POST"],               # M√©todos HTTP permitidos.
    allow_headers=["*"],                         # Headers permitidos.
)
```

**En producci√≥n**: Especificar or√≠genes exactos, no usar `"*"`.

---

## Pregunta 45: Async en FastAPI
**¬øCu√°ndo usar async def vs def?**

### Respuesta:
```python
# Sync: operaciones CPU-bound o librer√≠as sync
@app.post("/predict")
def predict(data: Input):                       # def: s√≠ncrono.
    result = model.predict(data)                 # sklearn es sync, no usar async.
    return {"prediction": result}

# Async: operaciones I/O-bound
@app.get("/external-data")
async def get_external():                        # async def: as√≠ncrono.
    async with httpx.AsyncClient() as client:   # Cliente HTTP as√≠ncrono.
        response = await client.get("https://api.example.com/data")  # await: espera sin bloquear.
    return response.json()
```

**Regla general**:
| Operaci√≥n | Usar |
|-----------|------|
| sklearn, pandas, joblib | `def` (sync) |
| HTTP requests, DB async | `async def` |
| File I/O masivo | `async def` con aiofiles |

---

## Pregunta 46: Model Caching
**¬øC√≥mo evitas cargar el modelo en cada request?**

### Respuesta:
```python
# FastAPI: lru_cache
from functools import lru_cache                  # Cache de funciones.

@lru_cache()                                     # Cachea resultado de la funci√≥n.
def get_model():
    return joblib.load("artifacts/pipeline.joblib")  # Solo carga una vez.

@app.post("/predict")
def predict(data: Input):
    model = get_model()                          # Primera vez: carga. Resto: cache.
    return model.predict(data)

# Alternativa: cargar al inicio
model = None

@app.on_event("startup")                         # Se ejecuta al iniciar app.
async def load_model():
    global model                                 # Modifica variable global.
    model = joblib.load("artifacts/pipeline.joblib")
```

**Streamlit**:
```python
@st.cache_resource                               # Cache persistente entre reruns.
def load_model():
    return joblib.load("artifacts/pipeline.joblib")  # Solo carga una vez.

model = load_model()                             # Cacheado entre interacciones.
```

---

## Pregunta 47: Streamlit Dashboard
**¬øC√≥mo creas un dashboard de predicci√≥n?**

### Respuesta:
```python
import streamlit as st
import pandas as pd

st.title("üè¶ BankChurn Predictor")                # T√≠tulo de la app.

# Sidebar para inputs
st.sidebar.header("Customer Data")               # Header en sidebar.
credit_score = st.sidebar.slider("Credit Score", 300, 850, 650)  # Slider con rango.
age = st.sidebar.slider("Age", 18, 100, 35)      # Valor default: 35.
geography = st.sidebar.selectbox("Geography", ["France", "Germany", "Spain"])  # Dropdown.

# Cargar modelo (cacheado)
@st.cache_resource                               # Cache para recursos pesados.
def load_model():
    return joblib.load("artifacts/pipeline.joblib")

model = load_model()                             # Carga una sola vez.

# Predicci√≥n
if st.sidebar.button("Predict"):                 # Bot√≥n que dispara predicci√≥n.
    input_df = pd.DataFrame([{                   # Crea DataFrame de entrada.
        "CreditScore": credit_score,
        "Age": age,
        "Geography": geography
    }])
    
    prediction = model.predict(input_df)[0]      # Predicci√≥n binaria.
    proba = model.predict_proba(input_df)[0, 1]  # Probabilidad de clase 1.
    
    col1, col2 = st.columns(2)                   # Layout en 2 columnas.
    col1.metric("Prediction", "Churn" if prediction else "Stay")  # Muestra resultado.
    col2.metric("Probability", f"{proba:.1%}")   # Probabilidad formateada.
```

---

## Pregunta 48: Docker Compose para ML
**¬øC√≥mo orquestas m√∫ltiples servicios?**

### Respuesta:
```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: .                                     # Construye desde Dockerfile local.
    ports:
      - "8000:8000"                              # host:container.
    environment:
      - MODEL_PATH=/app/artifacts/pipeline.joblib  # Variable de entorno.
    volumes:
      - ./artifacts:/app/artifacts:ro            # ro: read-only.
    depends_on:
      - mlflow                                   # Espera a que mlflow inicie.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]  # Health check.
      interval: 30s                              # Cada 30 segundos.
      timeout: 10s
      retries: 3

  mlflow:
    image: python:3.11-slim                      # Imagen base.
    command: mlflow server --host 0.0.0.0        # Comando de inicio.
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlflow/mlruns                  # Persistencia de experimentos.

  prometheus:
    image: prom/prometheus                       # Imagen oficial.
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml  # Config.
```

```bash
docker-compose up -d                            # -d: detached (background).
docker-compose logs -f api                      # -f: follow logs en tiempo real.
```

---

## Pregunta 49: Kubernetes Deployment
**¬øC√≥mo despliegas en Kubernetes?**

### Respuesta:
```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment                                # Tipo de recurso K8s.
metadata:
  name: bankchurn-api
spec:
  replicas: 3                                   # 3 instancias del pod.
  selector:
    matchLabels:
      app: bankchurn-api                        # Selector para pods.
  template:
    metadata:
      labels:
        app: bankchurn-api
    spec:
      containers:
      - name: api
        image: bankchurn-api:v1.0.0             # Imagen Docker.
        ports:
        - containerPort: 8000                   # Puerto interno.
        resources:
          requests:                             # M√≠nimo garantizado.
            memory: "256Mi"
            cpu: "250m"                         # 0.25 CPU.
          limits:                               # M√°ximo permitido.
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:                          # Verifica si pod est√° vivo.
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30               # Espera antes de primer check.
          periodSeconds: 10                     # Cada 10 segundos.
        readinessProbe:                         # Verifica si puede recibir tr√°fico.
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service                                   # Expone pods como servicio.
metadata:
  name: bankchurn-api
spec:
  selector:
    app: bankchurn-api                          # Conecta con pods del Deployment.
  ports:
  - port: 80                                    # Puerto externo.
    targetPort: 8000                            # Puerto del contenedor.
  type: LoadBalancer                            # Expone externamente con LB.
```

```bash
kubectl apply -f deployment.yaml                # Aplica configuraci√≥n.
kubectl get pods                                # Lista pods.
kubectl logs -f deployment/bankchurn-api        # Ver logs en tiempo real.
```

---

## Pregunta 50: Horizontal Pod Autoscaler
**¬øC√≥mo escalas autom√°ticamente?**

### Respuesta:
```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler                   # Autoescalador horizontal.
metadata:
  name: bankchurn-api-hpa
spec:
  scaleTargetRef:                               # Deployment a escalar.
    apiVersion: apps/v1
    kind: Deployment
    name: bankchurn-api
  minReplicas: 2                                # M√≠nimo 2 pods.
  maxReplicas: 10                               # M√°ximo 10 pods.
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70                  # Escala si CPU > 70%.
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80                  # Escala si memoria > 80%.
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300           # Espera 5 min antes de reducir.
    scaleUp:
      stabilizationWindowSeconds: 60            # Escala up m√°s r√°pido (1 min).
```

```bash
kubectl apply -f hpa.yaml                       # Aplica HPA.
kubectl get hpa                                 # Ver estado del autoescalador.
# NAME                  REFERENCE              TARGETS   MINPODS   MAXPODS   REPLICAS
# bankchurn-api-hpa     Deployment/bankchurn   45%/70%   2         10        3  # 45% actual, 70% target.
```

---

# 5. Escenarios Pr√°cticos (Preguntas 51-60) {#5-escenarios-pr√°cticos-preguntas-51-60}

## Pregunta 51: Debug de Producci√≥n
**El modelo tiene accuracy 85% en dev pero 60% en prod. ¬øPor qu√©?**

### Respuesta:
1. **Data drift**: Distribuci√≥n de datos cambi√≥
2. **Feature mismatch**: Features procesadas diferente
3. **Training-serving skew**: Preprocesamiento distinto
4. **Datos de prod con m√°s ruido**: Edge cases no vistos

**Acciones**: Comparar distribuciones, revisar pipeline, logging de inputs.

---

## Pregunta 52: Code Review
**¬øQu√© buscas en un code review de ML?**

### Respuesta:
- [ ] Data leakage en split/preprocessing
- [ ] Tests para features y modelo
- [ ] Config externalizada (no hardcoded)
- [ ] Type hints y docstrings
- [ ] Reproducibilidad (seeds, versiones)
- [ ] Logging apropiado

---

## Pregunta 53: Explicabilidad del Modelo
**El cliente dice: "No puedo usar tu modelo si no me explicas por qu√© toma las decisiones".**

### Respuesta:
```python
import shap

# 1. SHAP para explicaciones individuales
explainer = shap.TreeExplainer(model)         # Explainer para modelos de √°rboles.
shap_values = explainer.shap_values(X_sample) # Calcula contribuci√≥n de cada feature.

# Waterfall plot para una predicci√≥n
shap.waterfall_plot(shap.Explanation(         # Visualiza contribuciones.
    values=shap_values[0],                    # Valores SHAP de una predicci√≥n.
    base_values=explainer.expected_value,     # Valor base (promedio).
    data=X_sample.iloc[0]                     # Valores reales de features.
))

# 2. Feature importance global
shap.summary_plot(shap_values, X_sample)      # Resumen de todas las predicciones.

# 3. En producci√≥n: incluir en respuesta
@app.post("/predict")
def predict(data: Input):
    pred = model.predict(X)[0]
    
    # Top 3 razones
    shap_vals = explainer.shap_values(X)
    top_features = sorted(                    # Ordena por impacto.
        zip(feature_names, shap_vals[0]),
        key=lambda x: abs(x[1]),              # Valor absoluto del impacto.
        reverse=True
    )[:3]                                     # Solo top 3.
    
    return {
        "prediction": pred,
        "explanation": [                      # Explicaci√≥n en respuesta.
            {"feature": f, "impact": v} 
            for f, v in top_features
        ]
    }
```

---

## Pregunta 54: Optimizaci√≥n de Latencia
**El modelo tarda 500ms por predicci√≥n. El negocio necesita <100ms.**

### Respuesta:
```python
# 1. Profiling: ¬ød√≥nde est√° el cuello de botella?
import cProfile
cProfile.run('model.predict(X_sample)')       # Identifica funciones lentas.

# 2. Opciones de optimizaci√≥n:

# a) Modelo m√°s ligero
from sklearn.linear_model import LogisticRegression  # LR es 10x m√°s r√°pido que RF.

# b) Reducir features
from sklearn.feature_selection import SelectKBest
selector = SelectKBest(k=10)                  # Solo top 10 features = menos c√°lculo.

# c) Batch predictions
@app.post("/predict/batch")
def predict_batch(items: List[Input]):
    X = pd.DataFrame([item.dict() for item in items])
    preds = model.predict(X)                  # Una llamada = menos overhead.
    return {"predictions": preds.tolist()}

# d) Caching de predicciones frecuentes
from functools import lru_cache

@lru_cache(maxsize=1000)                      # Cache √∫ltimas 1000 predicciones.
def predict_cached(credit_score: int, age: int):
    return model.predict([[credit_score, age]])[0]  # Cache hit = instant√°neo.

# e) ONNX para inferencia r√°pida
from skl2onnx import convert_sklearn          # Convierte a formato optimizado.
onnx_model = convert_sklearn(model, initial_types=[...])  # Runtime m√°s r√°pido.
```

**M√©tricas de latencia**:
| Optimizaci√≥n | Latencia t√≠pica |
|--------------|-----------------|
| RF sklearn | 50-200ms |
| LR sklearn | 1-5ms |
| ONNX | 1-10ms |
| Caching (hit) | <1ms |

---

## Pregunta 55: Manejo de PII
**El dataset contiene nombres, emails y tel√©fonos. ¬øC√≥mo lo manejas?**

### Respuesta:
```python
# 1. Identificar columnas PII
pii_columns = ["name", "email", "phone", "ssn", "address"]  # Datos sensibles.

# 2. Anonimizaci√≥n
import hashlib

def anonymize_pii(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    for col in pii_columns:
        if col in df.columns:
            df[col] = df[col].apply(          # Hash irreversible.
                lambda x: hashlib.sha256(str(x).encode()).hexdigest()[:16]
            )                                 # No se puede revertir.
    return df

# 3. Drop antes de training (mejor opci√≥n)
X = df.drop(columns=pii_columns, errors='ignore')  # Elimina PII del modelo.

# 4. En logs: nunca loggear PII
logger.info(f"Prediction for customer {customer_id[:4]}***")  # Mascara ID.

# 5. En respuestas de API: mascarar
def mask_email(email: str) -> str:
    parts = email.split("@")
    return f"{parts[0][:2]}***@{parts[1]}"   # jo***@gmail.com.
```

**Compliance checklist**:
- [ ] PII no est√° en features del modelo
- [ ] PII no aparece en logs
- [ ] PII no se almacena en MLflow/tracking
- [ ] Acceso a datos restringido

---

## Pregunta 56: Fairness y Bias
**Producto detect√≥ que el modelo rechaza m√°s a clientes de cierta regi√≥n.**

### Respuesta:
```python
from fairlearn.metrics import MetricFrame      # Fairlearn: librer√≠a de fairness.
from sklearn.metrics import accuracy_score, recall_score

# 1. Calcular m√©tricas por grupo
metrics = MetricFrame(
    metrics={
        "accuracy": accuracy_score,
        "recall": recall_score
    },
    y_true=y_test,
    y_pred=y_pred,
    sensitive_features=df_test["geography"]   # Variable sensible.
)

print(metrics.by_group)                        # M√©tricas por grupo.
#              accuracy  recall
# geography
# France         0.85     0.80
# Germany        0.83     0.78
# Spain          0.70     0.55                 # ‚Üê Problema: peor para Spain.

# 2. Mitigaci√≥n
from fairlearn.reductions import ExponentiatedGradient  # Algoritmo de mitigaci√≥n.
from fairlearn.constraints import DemographicParity     # Restricci√≥n de fairness.

mitigator = ExponentiatedGradient(
    estimator=base_model,
    constraints=DemographicParity()            # Igualar tasas entre grupos.
)
mitigator.fit(X_train, y_train, sensitive_features=train_geography)

# 3. Monitoreo continuo
# Alertar si la diferencia entre grupos > 10%  # Dashboard de fairness.
```

---

## Pregunta 57: Tests Flaky en CI
**El CI pasa 80% de las veces y falla 20% sin cambios en c√≥digo.**

### Respuesta:
```python
# 1. Problema com√∫n: Random sin seed
# ‚ùå Mal
model = RandomForestClassifier()              # Sin seed: resultados diferentes cada vez.

# ‚úÖ Bien
model = RandomForestClassifier(random_state=42)  # Seed fija: resultados reproducibles.

# 2. Problema: Orden de ejecuci√≥n
# ‚ùå Mal: test depende de otro
def test_predict():
    assert model.predict(X) == [1]            # model de test anterior: dependencia oculta.

# ‚úÖ Bien: tests aislados
@pytest.fixture
def trained_model():                          # Fixture crea modelo fresco.
    m = Model()
    m.fit(X, y)
    return m

def test_predict(trained_model):              # Test recibe su propio modelo.
    assert trained_model.predict(X)

# 3. Problema: Timeouts en CI
# ‚ùå Mal
requests.get("https://external-api.com", timeout=5)  # API externa puede fallar.

# ‚úÖ Bien
@pytest.fixture
def mock_api():                               # Mock evita llamadas externas.
    with patch("myapp.api.get") as mock:
        mock.return_value = {"data": "test"}
        yield mock

# 4. Debug: Correr m√∫ltiples veces
pytest tests/ --count=10                      # pytest-repeat: detecta tests flaky.
```

---

## Pregunta 58: Modelo Grande para Deploy
**El modelo pesa 2GB y tarda 30s en cargar. ¬øC√≥mo optimizas?**

### Respuesta:
```python
# 1. Quantization (reducir precisi√≥n)
import onnxruntime as ort
from onnxruntime.quantization import quantize_dynamic

quantize_dynamic(
    "model.onnx",
    "model_quantized.onnx",
    weight_type=ort.QuantType.QInt8             # 8-bit en vez de 32-bit.
)
# 2GB ‚Üí ~500MB                                  # 4x reducci√≥n de tama√±o.

# 2. Model distillation (modelo m√°s peque√±o que imita al grande)
teacher = load_large_model()                    # Modelo grande y lento.
student = SmallModel()                          # Modelo peque√±o y r√°pido.

# Entrenar student con outputs del teacher
student_preds = student(X)
teacher_preds = teacher(X)                      # Student aprende a imitar teacher.
loss = mse_loss(student_preds, teacher_preds)   # Minimiza diferencia.

# 3. Feature selection (menos features = modelo m√°s peque√±o)
from sklearn.feature_selection import SelectFromModel
selector = SelectFromModel(model, threshold="median")  # Selecciona features importantes.
X_reduced = selector.transform(X)               # Menos columnas = modelo m√°s ligero.

# 4. Lazy loading en API
model = None

@app.on_event("startup")                        # Carga al iniciar, no en cada request.
async def load():
    global model
    model = joblib.load("model.joblib")         # Solo una vez, cacheado.
```

---

## Pregunta 59: Muchos Falsos Positivos
**El modelo predice churn para clientes que claramente no van a irse.**

### Respuesta:
```python
# 1. Ajustar threshold (default=0.5)
y_proba = model.predict_proba(X_test)[:, 1]     # Probabilidades de clase 1.

# Encontrar threshold √≥ptimo
from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(y_test, y_proba)

# Threshold que maximiza F1
f1_scores = 2 * (precision * recall) / (precision + recall)  # F√≥rmula F1.
optimal_threshold = thresholds[np.argmax(f1_scores)]  # Mejor threshold.
print(f"Optimal threshold: {optimal_threshold}")  # Ej: 0.65 en vez de 0.5.

# Usar nuevo threshold
y_pred = (y_proba >= optimal_threshold).astype(int)  # Threshold m√°s alto = menos FP.

# 2. Revisar balance de datos
print(y_train.value_counts(normalize=True))     # Ver proporciones de clases.
# Si muy desbalanceado: SMOTE, class_weight     # T√©cnicas de balanceo.

# 3. Verificar data leakage
# ¬øHay features que "predicen perfectamente"?
for col in X.columns:
    corr = X[col].corr(y)
    if abs(corr) > 0.9:                         # Correlaci√≥n sospechosa.
        print(f"‚ö†Ô∏è {col} tiene correlaci√≥n {corr}")  # Posible leakage.
```

---

## Pregunta 60: Comunicar a Stakeholders No T√©cnicos
**El VP de producto pregunta: "¬øFunciona o no funciona tu modelo?"**

### Respuesta:
```python
# 1. Traducir m√©tricas t√©cnicas a impacto de negocio
"""
‚ùå Mal: "El modelo tiene AUC 0.85 y F1 0.78"  # T√©rminos que no entienden.

‚úÖ Bien:                                        # Lenguaje de negocio.
"Por cada 100 clientes que van a hacer churn:
- Detectamos 78 antes de que se vayan          # Recall en lenguaje simple.
- De los que marcamos como riesgo, 82% efectivamente se iban  # Precision.

Impacto: Si cada cliente perdido cuesta $500,
el modelo puede prevenir $31,200 en p√©rdidas mensuales  # $$$
(78 clientes √ó $500 √ó 80% tasa de retenci√≥n con intervenci√≥n)"
"""

# 2. Visualizaciones claras
import plotly.express as px                     # Plotly: gr√°ficos interactivos.

# Confusion matrix visual
fig = px.imshow(
    [[TN, FP], [FN, TP]],                       # Matriz de confusi√≥n.
    labels=dict(x="Predicted", y="Actual"),
    x=["Stay", "Churn"],
    y=["Stay", "Churn"],
    text_auto=True                              # Muestra n√∫meros en celdas.
)
fig.show()

# 3. Dashboard ejecutivo en Streamlit
st.metric("Clientes en Riesgo", "234", delta="-12 vs mes pasado")  # KPI con delta.
st.metric("Precision Retenci√≥n", "82%", delta="+5%")  # M√©trica clave.
st.metric("Ahorro Estimado", "$45,000/mes")     # Impacto en dinero.
```

**Regla de oro**: Siempre conectar con dinero o KPIs que el stakeholder ya conoce.

---

# üìö Recursos

| Tema | M√≥dulo |
|------|--------|
| Pipelines | [07_SKLEARN_PIPELINES.md](07_SKLEARN_PIPELINES.md) |
| Testing | [11_TESTING_ML.md](11_TESTING_ML.md) |
| CI/CD | [12_CI_CD.md](12_CI_CD.md) |
| Docker | [13_DOCKER.md](13_DOCKER.md) |
| FastAPI | [14_FASTAPI.md](14_FASTAPI.md) |
| MLflow | [10_EXPERIMENT_TRACKING.md](10_EXPERIMENT_TRACKING.md) |

---

<div align="center">

**¬°√âxito en tu entrevista! üöÄ**

[‚Üê Simulacro Junior](SIMULACRO_ENTREVISTA_JUNIOR.md) | [Simulacro Senior ‚Üí](SIMULACRO_ENTREVISTA_SENIOR_PARTE1.md)

</div>
