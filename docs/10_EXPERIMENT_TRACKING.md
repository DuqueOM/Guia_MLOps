# 10. Experiment Tracking con MLflow
 
 <a id="00-prerrequisitos"></a>
 
 ## 0.0 Prerrequisitos
 
 - Tener al menos 1 proyecto del portafolio con:
   - `artifacts/training_results.json` (o equivalente)
   - Un modelo serializado (`model.pkl`, `model.joblib`, etc.)
 - Poder instalar MLflow en tu entorno del proyecto.
 
 ---
 
 <a id="01-protocolo-e-como-estudiar-este-modulo"></a>
 
 ## 0.1 ğŸ§  Protocolo E: CÃ³mo estudiar este mÃ³dulo
 
 - **Antes de empezar**: abre **[Protocolo E](study_tools/PROTOCOLO_E.md)** y define tu *output mÃ­nimo* (1 run visible y comparable).
 - **Durante el debugging**: si te atoras >15 min (tracking_uri, artifacts, registry), registra el caso en **[Diario de Errores](study_tools/DIARIO_ERRORES.md)**.
 - **Al cierre de semana**: usa **[Cierre Semanal](study_tools/CIERRE_SEMANAL.md)** para auditar trazabilidad (runs, params, mÃ©tricas, artifacts).
 
 ---
 
 <a id="02-entregables-verificables-minimo-viable"></a>
 
 ## 0.2 âœ… Entregables verificables (mÃ­nimo viable)
 
 - [ ] Un experimento MLflow con:
   - params (ej. hiperparÃ¡metros o `run_type`)
   - mÃ©tricas (ej. `test_f1`, `test_auc`)
   - artifacts (ej. `training_results.json`, `config.yaml`)
 - [ ] Evidencia (UI o `mlflow ui`) de poder comparar 2 runs.
 - [ ] 1 entrada en **[Diario de Errores](study_tools/DIARIO_ERRORES.md)** si hubo bloqueo real.
 
 ---
 
 <a id="03-puente-teoria-codigo-portafolio"></a>
 
 ## 0.3 ğŸ§© Puente teorÃ­a â†” cÃ³digo (Portafolio)
 
 - **Concepto**: experiment tracking (params/metrics/artifacts + comparaciÃ³n + registry)
 - **Archivo**: `scripts/run_mlflow.py`, `docker-compose.mlflow.yml`, `configs/config.yaml`
 - **Prueba**: entrenar â†’ loguear â†’ abrir UI â†’ comparar runs
 
 ## ğŸ¯ Objetivo del MÃ³dulo
 
 Implementar tracking de experimentos como lo hace el portafolio con `run_mlflow.py`.

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘  SIN MLFLOW:                           CON MLFLOW:                           â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â•‘
â•‘  "Â¿QuÃ© hiperparÃ¡metros usÃ© hace        "MLflow run abc123: RF con            â•‘
â•‘   2 semanas cuando obtuve F1=0.85?"    n_estimators=200, F1=0.85"            â•‘
â•‘                                                                              â•‘
â•‘  "Â¿DÃ³nde guardÃ© ese modelo bueno?"     "Artifacts en run abc123/model.pkl"   â•‘
â•‘                                                                              â•‘
â•‘  "Â¿Por quÃ© este modelo es peor?"       "Comparar runs en UI: diff params"    â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### ğŸ§© CÃ³mo se aplica en este portafolio

- En **BankChurn-Predictor** ya tienes:
  - `scripts/run_mlflow.py` como script de logging posterior al entrenamiento.
  - ConfiguraciÃ³n de MLflow en `configs/config.yaml` y `src/bankchurn/config.py`.
 - El archivo `docker-compose.mlflow.yml` en la raÃ­z del repo levanta un servidor MLflow
   real que puedes usar para practicar este mÃ³dulo.
 - El mismo patrÃ³n de logging puedes aplicarlo a **CarVision** y **TelecomAI**, usando
   sus `artifacts/` y modelos entrenados como fuente de mÃ©tricas y artifacts.

---

## ğŸ“‹ Contenido

- **0.0** [Prerrequisitos](#00-prerrequisitos)
- **0.1** [Protocolo E: CÃ³mo estudiar este mÃ³dulo](#01-protocolo-e-como-estudiar-este-modulo)
- **0.2** [Entregables verificables (mÃ­nimo viable)](#02-entregables-verificables-minimo-viable)
- **0.3** [Puente teorÃ­a â†” cÃ³digo (Portafolio)](#03-puente-teoria-codigo-portafolio)
1. [Conceptos de MLflow](#101-conceptos-de-mlflow)
2. [Setup y ConfiguraciÃ³n](#102-setup-y-configuracion)
3. [Logging de Experimentos](#103-logging-de-experimentos)
4. [Model Registry](#104-model-registry)
5. [CÃ³digo Real del Portafolio](#105-codigo-real-del-portafolio)
- [Errores habituales](#errores-habituales)
- [âœ… Ejercicio](#ejercicio)
- [âœ… Checkpoint](#checkpoint)

---

<a id="101-conceptos-de-mlflow"></a>

## 10.1 Conceptos de MLflow

### Los 4 Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          MLFLOW COMPONENTS                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. TRACKING                    2. PROJECTS                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚  â€¢ Log params, metrics          â€¢ Empaquetar cÃ³digo                         â”‚
â”‚  â€¢ Guardar artifacts            â€¢ MLproject file                            â”‚
â”‚  â€¢ Comparar runs                â€¢ Reproducibilidad                          â”‚
â”‚                                                                             â”‚
â”‚  3. MODELS                      4. REGISTRY                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚
â”‚  â€¢ Formato estÃ¡ndar             â€¢ Versionado de modelos                     â”‚
â”‚  â€¢ Flavors (sklearn, pytorch)   â€¢ Staging â†’ Production                      â”‚
â”‚  â€¢ Serving                      â€¢ Aprobaciones                              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

EN ESTE PORTAFOLIO USAMOS: Tracking + Registry
```

### JerarquÃ­a de MLflow

```
MLflow Server
â””â”€â”€ Experiment: "BankChurn"
    â”œâ”€â”€ Run: abc123 (2024-01-15)
    â”‚   â”œâ”€â”€ Parameters: {n_estimators: 200, max_depth: 10}
    â”‚   â”œâ”€â”€ Metrics: {f1: 0.65, auc: 0.88}
    â”‚   â””â”€â”€ Artifacts: [model.pkl, config.yaml]
    â”‚
    â”œâ”€â”€ Run: def456 (2024-01-16)
    â”‚   â”œâ”€â”€ Parameters: {n_estimators: 200, max_depth: 15}
    â”‚   â”œâ”€â”€ Metrics: {f1: 0.62, auc: 0.86}
    â”‚   â””â”€â”€ Artifacts: [model.pkl, config.yaml]
    â”‚
    â””â”€â”€ Run: ghi789 (2024-01-17) â† MEJOR
        â”œâ”€â”€ Parameters: {n_estimators: 200, max_depth: 10}
        â”œâ”€â”€ Metrics: {f1: 0.65, auc: 0.88}
        â””â”€â”€ Artifacts: [model.pkl, config.yaml]
```

---

<a id="102-setup-y-configuracion"></a>

## 10.2 Setup y ConfiguraciÃ³n

### OpciÃ³n 1: Local (File Store)

```python
# MÃ¡s simple, para desarrollo local
import mlflow

mlflow.set_tracking_uri("file:./mlruns")  # Guarda en carpeta local
mlflow.set_experiment("my-experiment")
```

### OpciÃ³n 2: Servidor MLflow (ProducciÃ³n)

```yaml
# docker-compose.mlflow.yml del portafolio
services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.9.2
    ports:
      - "5000:5000"
    volumes:
      - mlflow-artifacts:/mlflow
    command: >
      mlflow server
      --backend-store-uri sqlite:///mlflow/mlflow.db
      --default-artifact-root /mlflow/artifacts
      --host 0.0.0.0
      --port 5000
```

```python
# Conectar al servidor
import mlflow

mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("BankChurn")
```

### ConfiguraciÃ³n en el Portafolio

```yaml
# configs/config.yaml (BankChurn)
mlflow:
  tracking_uri: "file:./mlruns"      # Local para desarrollo
  experiment_name: "bankchurn"
  enabled: true
```

```python
# src/bankchurn/config.py
class MLflowConfig(BaseModel):
    """MLflow tracking configuration."""
    tracking_uri: str = "file:./mlruns"
    experiment_name: str = "bankchurn"
    enabled: bool = True
```

---

<a id="103-logging-de-experimentos"></a>

## 10.3 Logging de Experimentos

### API BÃ¡sica

```python
import mlflow

# Iniciar un run
with mlflow.start_run(run_name="experiment-1"):
    
    # 1. LOG PARAMETERS (hiperparÃ¡metros, config)
    mlflow.log_param("n_estimators", 200)
    mlflow.log_param("max_depth", 10)
    mlflow.log_params({  # MÃºltiples a la vez
        "learning_rate": 0.1,
        "model_type": "random_forest"
    })
    
    # 2. LOG METRICS (resultados)
    mlflow.log_metric("f1_score", 0.65)
    mlflow.log_metric("auc_roc", 0.88)
    mlflow.log_metrics({  # MÃºltiples a la vez
        "precision": 0.70,
        "recall": 0.61
    })
    
    # 3. LOG ARTIFACTS (archivos)
    mlflow.log_artifact("configs/config.yaml")
    mlflow.log_artifact("artifacts/training_results.json")
    
    # 4. LOG MODEL (modelo serializado con metadata)
    mlflow.sklearn.log_model(
        pipeline,
        artifact_path="model",
        registered_model_name="BankChurnClassifier"
    )
```

### MÃ©tricas por Ã‰poca/Paso

```python
# Para modelos que entrenan por Ã©pocas
for epoch in range(100):
    train_loss = train_one_epoch()
    val_loss = validate()
    
    mlflow.log_metrics({
        "train_loss": train_loss,
        "val_loss": val_loss
    }, step=epoch)  # â† step permite graficar evoluciÃ³n
```

---

<a id="104-model-registry"></a>

## 10.4 Model Registry

### Registrar un Modelo

```python
# Durante el run
mlflow.sklearn.log_model(
    pipeline,
    artifact_path="model",
    registered_model_name="BankChurnClassifier"  # â† Registra automÃ¡ticamente
)

# O despuÃ©s del run
mlflow.register_model(
    model_uri=f"runs:/{run_id}/model",
    name="BankChurnClassifier"
)
```

### Transiciones de Estado

```python
from mlflow.tracking import MlflowClient

client = MlflowClient()

# Promover a Staging
client.transition_model_version_stage(
    name="BankChurnClassifier",
    version=1,
    stage="Staging"
)

# Promover a Production (despuÃ©s de validaciÃ³n)
client.transition_model_version_stage(
    name="BankChurnClassifier",
    version=1,
    stage="Production"
)
```

### Cargar Modelo desde Registry

```python
# Cargar versiÃ³n especÃ­fica
model = mlflow.sklearn.load_model("models:/BankChurnClassifier/1")

# Cargar stage especÃ­fico
model = mlflow.sklearn.load_model("models:/BankChurnClassifier/Production")

# Cargar Ãºltimo modelo (latest)
model = mlflow.sklearn.load_model("models:/BankChurnClassifier/latest")
```

---

<a id="105-codigo-real-del-portafolio"></a>

## 10.5 CÃ³digo Real del Portafolio

### scripts/run_mlflow.py (BankChurn)

```python
#!/usr/bin/env python3
"""Log training results to MLflow.

Este script se ejecuta DESPUÃ‰S del entrenamiento para:
1. Leer resultados de artifacts/training_results.json
2. Calcular mÃ©tricas de negocio (revenue saved, etc.)
3. Loguear todo a MLflow
4. Opcionalmente registrar el modelo

Uso:
    python scripts/run_mlflow.py

Environment Variables:
    MLFLOW_TRACKING_URI: URI del servidor MLflow
    MLFLOW_EXPERIMENT_NAME: Nombre del experimento
"""

from __future__ import annotations

import json
import os
from pathlib import Path

import joblib

try:
    import mlflow
    import mlflow.sklearn
    from mlflow.tracking import MlflowClient
except ImportError:
    mlflow = None

from sklearn.pipeline import Pipeline


def main() -> None:
    # ConfiguraciÃ³n desde environment
    tracking_uri = os.getenv("MLFLOW_TRACKING_URI", "file:./mlruns")
    experiment = os.getenv("MLFLOW_EXPERIMENT_NAME", "BankChurn")
    
    # Cargar resultados del entrenamiento
    results_path = Path("artifacts/training_results.json")
    if not results_path.exists():
        print(f"No se encontrÃ³ {results_path}. Ejecuta training primero.")
        return
    
    data = json.loads(results_path.read_text())
    
    # Extraer mÃ©tricas
    cv = data.get("cv_results", {})
    test = data.get("test_results", {}).get("metrics", {})
    
    metrics = {}
    for k, v in cv.items():
        if isinstance(v, (int, float)):
            metrics[f"cv_{k}"] = float(v)
    for k, v in test.items():
        if isinstance(v, (int, float)):
            metrics[f"test_{k}"] = float(v)
    
    # Calcular mÃ©tricas de negocio
    cm = data.get("test_results", {}).get("confusion_matrix")
    if cm and len(cm) == 2:
        tn, fp = cm[0]
        fn, tp = cm[1]
        
        # ParÃ¡metros de negocio (configurables)
        clv = float(os.getenv("BC_CLV_USD", "2300"))  # Customer Lifetime Value
        retention_rate = float(os.getenv("BC_RETENTION_RATE", "0.3"))
        
        saved_customers = tp * retention_rate
        saved_revenue = saved_customers * clv
        
        metrics.update({
            "biz_detected_churners": float(tp),
            "biz_saved_customers": saved_customers,
            "biz_saved_revenue_usd": saved_revenue,
            "biz_false_positives": float(fp),
            "biz_missed_churners": float(fn),
        })
    
    if mlflow is None:
        print("MLflow no instalado. MÃ©tricas:", metrics)
        return
    
    # Configurar MLflow
    mlflow.set_tracking_uri(tracking_uri)
    mlflow.set_experiment(experiment)
    
    # Crear run
    with mlflow.start_run(run_name="demo-logging"):
        # Log parÃ¡metros
        mlflow.log_params({
            "run_type": "demo",
            "source": "run_mlflow.py"
        })
        
        # Log mÃ©tricas
        mlflow.log_metrics(metrics)
        
        # Log artifacts
        for artifact in [
            Path("artifacts/training_results.json"),
            Path("configs/config.yaml"),
        ]:
            if artifact.exists():
                try:
                    mlflow.log_artifact(str(artifact))
                except PermissionError:
                    print(f"Skipping {artifact}: permission denied")
        
        # Log modelo si existe
        model_path = Path("models/model_v1.0.0.pkl")
        if model_path.exists():
            try:
                obj = joblib.load(model_path)
                if isinstance(obj, dict) and "pipeline" in obj:
                    pipe = obj["pipeline"]
                elif isinstance(obj, Pipeline):
                    pipe = obj
                else:
                    pipe = None
                
                if pipe:
                    mlflow.sklearn.log_model(
                        pipe,
                        artifact_path="model",
                        registered_model_name="BankChurnClassifier"
                    )
            except Exception as e:
                print(f"Model logging skipped: {e}")
        
        print(f"âœ… MLflow run logged to {tracking_uri}")
        print(f"   Experiment: {experiment}")
        print(f"   Metrics: {len(metrics)} logged")


if __name__ == "__main__":
    main()
```

### Makefile Integration

```makefile
# Makefile
.PHONY: mlflow-demo mlflow-ui

mlflow-demo:
@echo "Logging to MLflow..."
MLFLOW_TRACKING_URI=file:./mlruns python scripts/run_mlflow.py

mlflow-ui:
@echo "Starting MLflow UI at http://localhost:5000"
mlflow ui --host 0.0.0.0 --port 5000
```

---

<a id="errores-habituales"></a>

## ğŸ§¨ Errores habituales y cÃ³mo depurarlos en MLflow

MLflow aÃ±ade una capa extra (servidor, rutas, artefactos), asÃ­ que muchos errores son de **configuraciÃ³n** mÃ¡s que de cÃ³digo puro.

Si alguno de estos errores te tomÃ³ **>15 minutos**, regÃ­stralo en el **[Diario de Errores](study_tools/DIARIO_ERRORES.md)** y aplica el flujo de **rescate cognitivo** de **[Protocolo E](study_tools/PROTOCOLO_E.md)**.

### 1) Runs que no aparecen en la UI (tracking_uri/experimento incorrectos)

**SÃ­ntomas tÃ­picos**

- Ejecutas training o `run_mlflow.py` y no ves nada nuevo en `http://localhost:5000`.

**CÃ³mo identificarlo**

- Imprime `mlflow.get_tracking_uri()` y el experimento actual (`mlflow.get_experiment_by_name(...)`).
- Verifica si estÃ¡s usando `file:./mlruns` mientras tienes un servidor en Docker (`http://localhost:5000`).

**CÃ³mo corregirlo**

- Define claramente en config:
  - Desarrollo local â†’ `tracking_uri: "file:./mlruns"`.
  - Demo/stack Docker â†’ `tracking_uri: "http://mlflow:5000"` o `http://localhost:5000`.
- AsegÃºrate de que tanto `ChurnTrainer` como `scripts/run_mlflow.py` lean del mismo origen (YAML/env vars).

---

### 2) Errores al registrar modelos (`MlflowException`, permisos, backend)

**SÃ­ntomas tÃ­picos**

- Al llamar `mlflow.sklearn.log_model(..., registered_model_name=...)` obtienes errores sobre base de datos o registry no configurado.

**CÃ³mo identificarlo**

- Si usas solo `file:./mlruns` sin servidor, el **registry completo** no estÃ¡ disponible.

**CÃ³mo corregirlo**

- Para desarrollo ligero, limita el uso de registry (puedes usar solo tracking + artifacts).
- Para un registry completo, usa el `docker-compose.mlflow.yml` del portafolio con backend SQLite/postgres y apunta `MLFLOW_TRACKING_URI` al servidor.

---

### 3) Artifacts que no se encuentran o no se suben

**SÃ­ntomas tÃ­picos**

- Errores tipo `FileNotFoundError` al hacer `mlflow.log_artifact`.
- No ves `training_results.json` ni `config.yaml` en la pestaÃ±a de artifacts.

**CÃ³mo identificarlo**

- Revisa rutas relativas en `run_mlflow.py` y asegÃºrate de que ejecutas el script desde la raÃ­z del proyecto.

**CÃ³mo corregirlo**

- Usa rutas consistentes (por ejemplo `artifacts/training_results.json`) y verifica que el archivo exista antes de loguearlo.
- Si corres dentro de Docker, revisa que el volumen monte correctamente `artifacts/` y `configs/`.

---

### 4) Problemas con MLflow en Docker (puertos, hostnames, permisos)

**SÃ­ntomas tÃ­picos**

- `ConnectionError` al intentar conectar a `http://localhost:5000` desde un contenedor.
- Logs que muestran errores de permisos en `/mlflow`.

**CÃ³mo identificarlo**

- Examina `docker-compose.mlflow.yml` y las variables de entorno de tus servicios.

**CÃ³mo corregirlo**

- Dentro de un contenedor, usa el hostname del servicio (`http://mlflow:5000`) en lugar de `localhost`.
- AsegÃºrate de que el volumen `mlflow-artifacts` tenga permisos de escritura correctos (usuario del contenedor).

---

### PatrÃ³n general de debugging en MLflow

1. **Comprueba tracking_uri y experimento** antes de iniciar el run.
2. **Valida artifacts y modelos**: que los paths existen y se cargan correctamente.
3. **Reproduce localmente con file store** (`file:./mlruns`) antes de ir a servidor Docker.
4. **Verifica desde la UI** que params, metrics y artifacts coincidan con lo que esperas de tu cÃ³digo.

Con este patrÃ³n, MLflow pasa de ser â€œcaja negraâ€ a una herramienta confiable para explicar, comparar y promover modelos.

---

<a id="ejercicio"></a>

## âœ… Ejercicio: Integrar MLflow en TelecomAI

1. Crea `scripts/run_mlflow.py` para TelecomAI
2. Log las mÃ©tricas: accuracy, f1, precision, recall, roc_auc
3. Calcula mÃ©tricas de negocio (customers retained, revenue saved)
4. Registra el modelo como "TelecomPlanClassifier"

---

<a id="checkpoint"></a>

## âœ… Checkpoint

- [ ] Puedo correr un run end-to-end y verlo en la UI (o en `mlflow ui`).
- [ ] Puedo explicar dÃ³nde quedan:
  - params
  - metrics
  - artifacts
- [ ] Puedo comparar 2 runs y justificar quÃ© cambiÃ³ (params â†’ mÃ©tricas).

---

## ğŸ“¦ CÃ³mo se UsÃ³ en el Portafolio

MLflow estÃ¡ integrado en los 3 proyectos del portafolio:

### ConfiguraciÃ³n MLflow en BankChurn

```python
# BankChurn-Predictor/src/bankchurn/config.py
class MLflowConfig(BaseModel):
    """MLflow tracking configuration."""
    tracking_uri: str = "file:./mlruns"  # Local por defecto
    experiment_name: str = "bankchurn"
    enabled: bool = True
```

### IntegraciÃ³n en Trainer

```python
# BankChurn-Predictor/src/bankchurn/trainer.py (extracto)
def _log_to_mlflow(self):
    """Log experimento a MLflow."""
    if not self.config.mlflow.enabled:
        return
    
    mlflow.set_tracking_uri(self.config.mlflow.tracking_uri)
    mlflow.set_experiment(self.config.mlflow.experiment_name)
    
    with mlflow.start_run():
        # ParÃ¡metros
        mlflow.log_params({
            "model_type": self.config.model.type,
            "test_size": self.config.model.test_size,
            "cv_folds": self.config.model.cv_folds,
        })
        
        # MÃ©tricas
        mlflow.log_metrics(self.metrics_)
        
        # Modelo
        mlflow.sklearn.log_model(self.model_, "model")
```

### Estructura de mlruns/

```
BankChurn-Predictor/
â””â”€â”€ mlruns/
    â”œâ”€â”€ 0/                    # Default experiment
    â””â”€â”€ 123456789/            # bankchurn experiment
        â””â”€â”€ abc123def456/     # Run ID
            â”œâ”€â”€ artifacts/
            â”‚   â””â”€â”€ model/
            â”œâ”€â”€ metrics/
            â”‚   â”œâ”€â”€ accuracy
            â”‚   â”œâ”€â”€ f1_score
            â”‚   â””â”€â”€ roc_auc
            â”œâ”€â”€ params/
            â”‚   â”œâ”€â”€ model_type
            â”‚   â””â”€â”€ cv_folds
            â””â”€â”€ meta.yaml
```

### MLflow por Proyecto

| Proyecto | Tracking URI | Experiment | MÃ©tricas Principales |
|----------|--------------|------------|---------------------|
| BankChurn | `file:./mlruns` | `bankchurn` | accuracy, f1, roc_auc |
| CarVision | `file:./mlruns` | `carvision` | mae, rmse, r2 |
| TelecomAI | `file:./mlruns` | `telecomai` | accuracy, f1_weighted |

### ğŸ”§ Ejercicio: Explora MLflow Real

```bash
# 1. Ve a BankChurn
cd BankChurn-Predictor

# 2. Entrena con MLflow habilitado
python main.py --config configs/config.yaml

# 3. Inicia la UI de MLflow
mlflow ui --backend-store-uri file:./mlruns

# 4. Abre en navegador
# http://localhost:5000

# 5. Explora:
# - Compara runs
# - Ve artifacts
# - Registra modelo en Model Registry
```

---

## ğŸ’¼ Consejos Profesionales

> **Recomendaciones para destacar en entrevistas y proyectos reales**

### Para Entrevistas

1. **MLflow vs W&B vs Neptune**: Conoce trade-offs (MLflow open-source, W&B mejor UI, Neptune escalabilidad).

2. **Model Registry**: Explica stages (Staging â†’ Production â†’ Archived).

3. **Reproducibilidad**: CÃ³mo reconstruir cualquier experimento desde el tracking.

### Para Proyectos Reales

| SituaciÃ³n | Consejo |
|-----------|---------|
| Equipo distribuido | Usa servidor MLflow centralizado |
| Muchos experimentos | Organiza con tags y naming conventions |
| Modelos grandes | Usa artifact storage externo (S3, GCS) |
| ComparaciÃ³n | Siempre registra baseline para comparar |

### QuÃ© Trackear Siempre

- **Params**: HiperparÃ¡metros, versiones de datos
- **Metrics**: Train/val/test, mÃ©tricas de negocio
- **Artifacts**: Modelo, configs, plots, requirements.txt
- **Tags**: Git commit, autor, dataset version


---

## ğŸ“º Recursos Externos Recomendados

> Ver [RECURSOS_POR_MODULO.md](RECURSOS_POR_MODULO.md) para la lista completa.

| ğŸ·ï¸ | Recurso | Tipo |
|:--:|:--------|:-----|
| ğŸ”´ | [MLflow Tutorial - Krish Naik](https://www.youtube.com/watch?v=qdcHHrsXA48) | Video |
| ğŸŸ¡ | [MLflow Complete Course](https://www.youtube.com/watch?v=MHcqGxA6JPs) | Video |

**DocumentaciÃ³n oficial:**
- [MLflow Tracking](https://mlflow.org/docs/latest/tracking.html)
- [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)

---

## ğŸ”— Referencias del Glosario

Ver [21_GLOSARIO.md](21_GLOSARIO.md) para definiciones de:
- **MLflow**: Plataforma de experiment tracking
- **Model Registry**: Registro de versiones de modelos
- **Artifact**: Archivo asociado a un experimento

---

## âœ… Ejercicios

Ver [EJERCICIOS.md](EJERCICIOS.md) - MÃ³dulo 10:
- **10.1**: MLflow bÃ¡sico (params, metrics, model)
- **10.2**: Comparar mÃºltiples experimentos

---

<div align="center">

[â† Training Profesional](09_TRAINING_PROFESIONAL.md) | [Siguiente: Testing ML â†’](11_TESTING_ML.md)

</div>
