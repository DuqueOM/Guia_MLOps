# 16. Observabilidad para ML
 
<a id="00-prerrequisitos"></a>
 
## 0.0 Prerrequisitos
 
- Haber completado el mÃ³dulo 14 (FastAPI) y entender endpoints `/health` y `/predict`.
- Haber completado el mÃ³dulo 13 (Docker) para poder levantar servicios en contenedores.
- Conocer logging bÃ¡sico en Python.
 
---
 
<a id="01-protocolo-e-como-estudiar-este-modulo"></a>
 
## 0.1 ğŸ§  Protocolo E: CÃ³mo estudiar este mÃ³dulo
 
- **Antes de empezar**: abre **[Protocolo E](study_tools/PROTOCOLO_E.md)** y define el output mÃ­nimo: mÃ©tricas visibles en Prometheus + logs JSON + un reporte de drift.
- **Durante el debugging**: si te atoras >15 min (scrape, paneles vacÃ­os, labels, parseo de logs, drift CI), registra el caso en **[Diario de Errores](study_tools/DIARIO_ERRORES.md)**.
- **Al cierre de semana**: usa **[Cierre Semanal](study_tools/CIERRE_SEMANAL.md)** para auditar alertas accionables y seÃ±al Ãºtil (no solo dashboards bonitos).
 
---
 
<a id="02-entregables-verificables-minimo-viable"></a>
 
## 0.2 âœ… Entregables verificables (mÃ­nimo viable)
 
- [ ] Endpoint `/metrics` expuesto y scrapeado por Prometheus.
- [ ] Dashboard (Grafana o equivalente) con latencia, throughput y error rate.
- [ ] Logs estructurados en JSON con campos de negocio (por ejemplo, `model`, `request_id`, `prediction`).
- [ ] Drift detection ejecutable (local o CI) con artefacto de salida (HTML/JSON).
- [ ] Al menos 1 alerta accionable (por ejemplo, error rate o latencia P99).
 
---
 
<a id="03-puente-teoria-codigo-portafolio"></a>
 
## 0.3 ğŸ§© Puente teorÃ­a â†” cÃ³digo (Portafolio)
 
- **Concepto**: seÃ±ales de oro + instrumentaciÃ³n + ML monitoring
- **Archivo**: `app/metrics.py`, `src/logging_config.py`, `monitoring/check_drift.py`
- **Prueba**: `curl http://localhost:8000/metrics` y revisiÃ³n de reportes/artefactos
 
---
 
## ğŸ¯ Objetivo del MÃ³dulo
 
Implementar monitoreo completo: logs, mÃ©tricas, y drift detection como en el portafolio.

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘  "Si no puedo verlo en un dashboard, no sÃ© si estÃ¡ funcionando."             â•‘
â•‘                                        â€” Mentalidad Senior                   â•‘
â•‘                                                                              â•‘
â•‘  OBSERVABILIDAD = LOGS + METRICS + TRACES + ML MONITORING                    â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 
```

---

## ğŸ“‹ Contenido

- **0.0** [Prerrequisitos](#00-prerrequisitos)
- **0.1** [Protocolo E: CÃ³mo estudiar este mÃ³dulo](#01-protocolo-e-como-estudiar-este-modulo)
- **0.2** [Entregables verificables (mÃ­nimo viable)](#02-entregables-verificables-minimo-viable)
- **0.3** [Puente teorÃ­a â†” cÃ³digo (Portafolio)](#03-puente-teoria-codigo-portafolio)
- **16.1** [Las 4 SeÃ±ales de Oro](#161-las-4-senales-de-oro)
- **16.2** [Prometheus + Grafana](#162-prometheus-grafana)
- **16.3** [Logging Estructurado](#163-logging-estructurado)
- **16.4** [Model Monitoring](#164-model-monitoring)
- [Errores habituales](#errores-habituales)
- [âœ… Checkpoint](#checkpoint)
- [âœ… Ejercicio](#ejercicio)

---

<a id="161-las-4-senales-de-oro"></a>
 
## 16.1 Las 4 SeÃ±ales de Oro

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ğŸ“Š LAS 4 SEÃ‘ALES DE ORO (+ ML)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. LATENCIA          Â¿CuÃ¡nto tarda una predicciÃ³n?                         â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       Target: P99 < 100ms                                   â”‚
â”‚                       Alerta: P99 > 200ms                                   â”‚
â”‚                                                                             â”‚
â”‚  2. TRÃFICO           Â¿CuÃ¡ntas requests por segundo?                        â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€          Monitorear: picos, tendencias, anomalÃ­as              â”‚
â”‚                                                                             â”‚
â”‚  3. ERRORES           Â¿QuÃ© porcentaje de requests falla?                    â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€           Target: Error rate < 0.1%                             â”‚
â”‚                       Alerta: Error rate > 1%                               â”‚
â”‚                                                                             â”‚
â”‚  4. SATURACIÃ“N        Â¿CuÃ¡nto recurso queda?                                â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        Alerta: CPU > 80%, Memory > 85%                       â”‚
â”‚                                                                             â”‚
â”‚  + ML-ESPECÃFICO:                                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚  5. DATA DRIFT        Â¿Los datos de entrada cambiaron?                      â”‚
â”‚  6. PREDICTION DRIFT  Â¿Las predicciones cambiaron distribuciÃ³n?             â”‚
â”‚  7. MODEL DECAY       Â¿El accuracy estÃ¡ degradando?                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

<a id="162-prometheus-grafana"></a>
 
## 16.2 Prometheus + Grafana

### ConfiguraciÃ³n del Portafolio

```yaml
# infra/prometheus-config.yaml

global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'bankchurn-api'
    static_configs:
      - targets: ['bankchurn:8000']
    metrics_path: /metrics
  
  - job_name: 'carvision-api'
    static_configs:
      - targets: ['carvision:8000']
    metrics_path: /metrics
  
  - job_name: 'telecom-api'
    static_configs:
      - targets: ['telecom:8000']
    metrics_path: /metrics
```

### MÃ©tricas en FastAPI

```python
# app/metrics.py

from prometheus_client import Counter, Histogram, Gauge, generate_latest  # Tipos de mÃ©tricas Prometheus.
from fastapi import Response              # Response para retornar texto plano.

# MÃ©tricas - Se definen a nivel mÃ³dulo (globales)
PREDICTIONS_TOTAL = Counter(              # Counter: solo incrementa (total acumulado).
    'predictions_total',                  # Nombre de la mÃ©trica (snake_case).
    'Total de predicciones realizadas',   # DescripciÃ³n (aparece en /metrics).
    ['model', 'result']                   # Labels: permiten filtrar por modelo/resultado.
)

PREDICTION_LATENCY = Histogram(           # Histogram: distribuciÃ³n de valores (latencias).
    'prediction_latency_seconds',         # ConvenciÃ³n: unidad en el nombre (_seconds).
    'Latencia de predicciones',
    ['model'],                            # Label para filtrar por modelo.
    buckets=[0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]  # Rangos para calcular percentiles.
)

MODEL_LOADED = Gauge(                     # Gauge: valor que sube/baja (estado actual).
    'model_loaded',                       # 1 si cargado, 0 si no.
    'Indica si el modelo estÃ¡ cargado',
    ['model']
)

PREDICTION_PROBABILITY = Histogram(       # Histogram para monitorear distribuciÃ³n de predicciones.
    'prediction_probability',             # Ãštil para detectar drift en predicciones.
    'DistribuciÃ³n de probabilidades predichas',
    ['model'],
    buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]  # Buckets cada 10%.
)


# Endpoint de mÃ©tricas
@app.get("/metrics")                      # Prometheus hace scrape a este endpoint.
async def metrics():
    return Response(
        content=generate_latest(),        # generate_latest(): serializa todas las mÃ©tricas.
        media_type="text/plain"           # Prometheus espera text/plain.
    )


# Uso en predicciÃ³n
import time                               # Para medir latencia.

@app.post("/predict")
async def predict(request: PredictionRequest):
    start = time.time()                   # Timestamp antes de predecir.
    
    # ... predicciÃ³n ...
    proba = model.predict_proba(df)[0, 1]
    prediction = int(proba >= 0.5)
    
    # Registrar mÃ©tricas
    latency = time.time() - start         # Calcula latencia en segundos.
    PREDICTION_LATENCY.labels(model="bankchurn").observe(latency)  # observe(): registra en histogram.
    PREDICTIONS_TOTAL.labels(model="bankchurn", result=str(prediction)).inc()  # inc(): incrementa counter.
    PREDICTION_PROBABILITY.labels(model="bankchurn").observe(proba)  # Registra prob para detectar drift.
    
    return {"prediction": prediction, "probability": proba}
```

### 16.2.1 Prometheus Alerting Rules

> **Referencia del portafolio**: `infra/prometheus-rules.yaml`

```yaml
# prometheus-rules.yaml
groups:
  - name: ml-service-alerts
    rules:
      # Latencia alta
      - alert: HighPredictionLatency
        expr: histogram_quantile(0.99, rate(prediction_latency_seconds_bucket[5m])) > 0.2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Latencia P99 alta en {{ $labels.model }}"
          description: "P99 latencia es {{ $value }}s (umbral: 200ms)"
          runbook_url: "https://docs.example.com/runbooks/high-latency"

      # Error rate alto
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.01
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Error rate alto en {{ $labels.service }}"
          description: "Error rate es {{ $value | humanizePercentage }}"
          runbook_url: "https://docs.example.com/runbooks/high-error-rate"

      # Drift detectado
      - alert: DataDriftDetected
        expr: ml_drift_score > 0.15
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Data drift detectado en {{ $labels.model }}"
          description: "Drift score es {{ $value }} (umbral: 0.15)"
          runbook_url: "https://docs.example.com/runbooks/data-drift"

      # Servicio caÃ­do
      - alert: ServiceDown
        expr: up{job=~"bankchurn|carvision|telecomai"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Servicio {{ $labels.job }} estÃ¡ caÃ­do"
```

### Buenas prÃ¡cticas para alertas

| PrÃ¡ctica | DescripciÃ³n |
|----------|-------------|
| **Accionable** | Cada alerta debe tener un runbook con pasos concretos |
| **Umbral realista** | Basar umbrales en datos histÃ³ricos, no en intuiciÃ³n |
| **Severidad apropiada** | `critical` solo para lo que requiere acciÃ³n inmediata |
| **Evitar ruido** | Usar `for:` para evitar alertas por spikes temporales |

---

<a id="163-logging-estructurado"></a>
 
## 16.3 Logging Estructurado

### ConfiguraciÃ³n Profesional

```python
# src/logging_config.py

import logging
import json
import sys
from datetime import datetime


class JSONFormatter(logging.Formatter):
    """Formatter que produce logs en JSON para fÃ¡cil parsing."""
    
    def format(self, record):
        log_obj = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        
        # AÃ±adir extras si existen
        if hasattr(record, "request_id"):
            log_obj["request_id"] = record.request_id
        if hasattr(record, "user_id"):
            log_obj["user_id"] = record.user_id
        if hasattr(record, "prediction"):
            log_obj["prediction"] = record.prediction
        
        # AÃ±adir exception si existe
        if record.exc_info:
            log_obj["exception"] = self.formatException(record.exc_info)
        
        return json.dumps(log_obj)


def setup_logging(level: str = "INFO", json_format: bool = True):
    """Configura logging para producciÃ³n."""
    
    root = logging.getLogger()
    root.setLevel(getattr(logging, level.upper()))
    
    handler = logging.StreamHandler(sys.stdout)
    
    if json_format:
        handler.setFormatter(JSONFormatter())
    else:
        handler.setFormatter(logging.Formatter(
            "%(asctime)s | %(levelname)-8s | %(name)s | %(message)s"
        ))
    
    root.addHandler(handler)
    
    # Silenciar loggers ruidosos
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
```

### Logs con Contexto

```python
import logging
import uuid

logger = logging.getLogger(__name__)

@app.post("/predict")
async def predict(request: PredictionRequest):
    request_id = str(uuid.uuid4())[:8]
    
    # Log con contexto
    logger.info(
        "Prediction request received",
        extra={
            "request_id": request_id,
            "credit_score": request.CreditScore,
            "geography": request.Geography,
        }
    )
    
    try:
        prediction = model.predict(...)
        
        logger.info(
            "Prediction completed",
            extra={
                "request_id": request_id,
                "prediction": prediction,
                "latency_ms": latency * 1000,
            }
        )
        
        return {"prediction": prediction}
    
    except Exception as e:
        logger.error(
            f"Prediction failed: {str(e)}",
            extra={"request_id": request_id},
            exc_info=True
        )
        raise
```

---

<a id="164-model-monitoring"></a>
 
## 16.4 Model Monitoring (Drift Detection)

### Script de Drift Detection

```python
# monitoring/check_drift.py - CÃ³digo REAL del portafolio

"""
Detecta drift en datos usando Evidently AI.

Compara datos de referencia (training) con datos actuales (producciÃ³n).
Genera reporte HTML y mÃ©tricas JSON.

Uso:
    python monitoring/check_drift.py --reference data/train.csv --current data/recent.csv
"""

import argparse
import json
from pathlib import Path
from datetime import datetime

import pandas as pd

try:
    from evidently import ColumnMapping
    from evidently.report import Report
    from evidently.metric_preset import DataDriftPreset, DataQualityPreset
    EVIDENTLY_AVAILABLE = True
except ImportError:
    EVIDENTLY_AVAILABLE = False


def check_drift(
    reference_data: pd.DataFrame,
    current_data: pd.DataFrame,
    output_dir: Path,
    numerical_features: list = None,
    categorical_features: list = None,
) -> dict:
    """
    Ejecuta anÃ¡lisis de drift entre datos de referencia y actuales.
    
    Returns
    -------
    dict
        MÃ©tricas de drift incluyendo:
        - dataset_drift: bool (True si hay drift significativo)
        - drift_share: float (% de features con drift)
        - drifted_features: list (features con drift detectado)
    """
    
    if not EVIDENTLY_AVAILABLE:
        return {"error": "Evidently no instalado", "dataset_drift": None}
    
    # Column mapping
    column_mapping = ColumnMapping()
    if numerical_features:
        column_mapping.numerical_features = numerical_features
    if categorical_features:
        column_mapping.categorical_features = categorical_features
    
    # Crear reporte
    report = Report(metrics=[
        DataDriftPreset(),
        DataQualityPreset(),
    ])
    
    report.run(
        reference_data=reference_data,
        current_data=current_data,
        column_mapping=column_mapping
    )
    
    # Guardar HTML
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    html_path = output_dir / f"drift_report_{timestamp}.html"
    report.save_html(str(html_path))
    
    # Extraer mÃ©tricas
    results = report.as_dict()
    
    drift_metrics = {
        "timestamp": timestamp,
        "reference_rows": len(reference_data),
        "current_rows": len(current_data),
        "dataset_drift": False,
        "drift_share": 0.0,
        "drifted_features": [],
        "report_path": str(html_path),
    }
    
    # Parsear resultados de Evidently
    for metric in results.get("metrics", []):
        if "DataDriftTable" in str(metric.get("metric", "")):
            result = metric.get("result", {})
            drift_metrics["dataset_drift"] = result.get("dataset_drift", False)
            drift_metrics["drift_share"] = result.get("drift_share", 0.0)
            
            # Features con drift
            drift_by_columns = result.get("drift_by_columns", {})
            for col, col_data in drift_by_columns.items():
                if col_data.get("drift_detected", False):
                    drift_metrics["drifted_features"].append(col)
    
    # Guardar mÃ©tricas JSON
    json_path = output_dir / f"drift_metrics_{timestamp}.json"
    with open(json_path, "w") as f:
        json.dump(drift_metrics, f, indent=2)
    
    return drift_metrics


def main():
    parser = argparse.ArgumentParser(description="Check data drift")
    parser.add_argument("--reference", required=True, help="Path to reference data CSV")
    parser.add_argument("--current", required=True, help="Path to current data CSV")
    parser.add_argument("--output", default="artifacts", help="Output directory")
    args = parser.parse_args()
    
    reference = pd.read_csv(args.reference)
    current = pd.read_csv(args.current)
    
    metrics = check_drift(reference, current, Path(args.output))
    
    print(json.dumps(metrics, indent=2))
    
    # Exit code basado en drift
    if metrics.get("dataset_drift"):
        print("âš ï¸ DRIFT DETECTADO")
        exit(1)
    else:
        print("âœ… No hay drift significativo")
        exit(0)


if __name__ == "__main__":
    main()
```

### GitHub Action para Drift Scheduled

```yaml
# .github/workflows/drift-detection.yml

name: Drift Detection

on:
  schedule:
    - cron: '0 2 * * *'  # Diario a las 2am UTC
  workflow_dispatch:

jobs:
  check-drift:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install pandas evidently
      
      - name: Run drift check
        run: |
          python monitoring/check_drift.py \
            --reference data/reference/train.csv \
            --current data/recent/latest.csv \
            --output artifacts/drift
      
      - name: Upload report
        uses: actions/upload-artifact@v4
        with:
          name: drift-report
          path: artifacts/drift/
      
      - name: Create issue if drift detected
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'âš ï¸ Data Drift Detected',
              body: 'Drift detection workflow failed. Check the artifacts.',
              labels: ['drift', 'monitoring']
            })
```

---

<a id="165-runbooks-de-alertas"></a>

## 16.5 Runbooks de Alertas â­ NUEVO

Un **runbook** documenta los pasos exactos para responder a una alerta. Sin runbooks, las alertas son ruido; con runbooks, son acciÃ³n.

### 16.5.1 Estructura de un Runbook

```markdown
# ğŸš¨ Runbook: HighPredictionLatency

## Resumen
| Campo | Valor |
|-------|-------|
| Alerta | `HighPredictionLatency` |
| Severidad | Warning â†’ Critical si persiste >15m |
| Impacto | UX degradada, timeouts en clientes |
| On-call | @ml-platform-team |

## DiagnÃ³stico RÃ¡pido (< 2 min)

1. **Verificar si es puntual o sostenido**
   ```bash
   # Ver P99 de Ãºltimos 15 min
   curl -s "http://prometheus:9090/api/v1/query?query=histogram_quantile(0.99,rate(prediction_latency_seconds_bucket[15m]))" | jq
   ```

2. **Verificar recursos del servicio**
   ```bash
   docker stats bankchurn-api --no-stream
   # CPU > 80%? Memory > 85%?
   ```

3. **Verificar logs recientes**
   ```bash
   docker logs bankchurn-api --tail 100 --since 5m | grep -i error
   ```

## Causas Comunes y Soluciones

### Causa 1: Modelo demasiado grande en memoria
**SÃ­ntomas**: Memory alta, swap activo
**SoluciÃ³n**:
```bash
# Escalar horizontalmente
kubectl scale deployment bankchurn --replicas=3

# O reiniciar para liberar memoria
docker restart bankchurn-api
```

### Causa 2: Spike de trÃ¡fico
**SÃ­ntomas**: Request rate 3x+ del baseline
**SoluciÃ³n**:
```bash
# Verificar rate actual
curl -s "http://prometheus:9090/api/v1/query?query=rate(http_requests_total[5m])"

# Escalar si es necesario
kubectl autoscale deployment bankchurn --min=2 --max=10 --cpu-percent=70
```

### Causa 3: Datos de entrada anÃ³malos
**SÃ­ntomas**: Latencia solo en algunos requests
**SoluciÃ³n**:
```bash
# Revisar payloads problemÃ¡ticos en logs
docker logs bankchurn-api | grep "latency_ms.*[0-9]{4}" | tail 20

# AÃ±adir validaciÃ³n de input mÃ¡s estricta
```

## EscalaciÃ³n
- Si no se resuelve en **15 min** â†’ Escalar a @senior-ml-engineer
- Si impacto en revenue â†’ Escalar a @on-call-manager
- Si es recurrente (3+ veces/semana) â†’ Crear ticket para investigaciÃ³n root cause

## Post-mortem
DespuÃ©s de resolver, documentar:
- [ ] Timeline del incidente
- [ ] Root cause
- [ ] Acciones para prevenir recurrencia
```

### 16.5.2 Runbook: Data Drift Detectado

```markdown
# ğŸš¨ Runbook: DataDriftDetected

## Resumen
| Campo | Valor |
|-------|-------|
| Alerta | `DataDriftDetected` |
| Severidad | Warning |
| Impacto | Predicciones potencialmente degradadas |
| Urgencia | 24-48h para investigar |

## DiagnÃ³stico

1. **Revisar reporte de drift**
   ```bash
   # Ver Ãºltimo reporte
   ls -la artifacts/drift/
   # Abrir HTML en browser para anÃ¡lisis visual
   open artifacts/drift/drift_report_*.html
   ```

2. **Identificar features con drift**
   ```bash
   cat artifacts/drift/drift_metrics_*.json | jq '.drifted_features'
   ```

3. **Verificar si hay cambio en fuente de datos**
   - Â¿CambiÃ³ el proveedor de datos?
   - Â¿Hay un nuevo segmento de usuarios?
   - Â¿Hay un bug en el pipeline de datos?

## Ãrbol de DecisiÃ³n

```
Â¿Drift > 30% de features?
â”œâ”€â”€ SÃ â†’ Probable cambio en fuente de datos
â”‚        â†’ Investigar pipeline de ingesta
â”‚        â†’ Considerar retrain urgente
â”‚
â””â”€â”€ NO â†’ Drift localizado
         â”œâ”€â”€ Â¿Features crÃ­ticas?
         â”‚   â”œâ”€â”€ SÃ â†’ Retrain en 1-2 dÃ­as
         â”‚   â””â”€â”€ NO â†’ Monitorear 1 semana
         â”‚
         â””â”€â”€ Â¿Drift estacional esperado?
             â”œâ”€â”€ SÃ â†’ Documentar, no acciÃ³n
             â””â”€â”€ NO â†’ Investigar causa
```

## Acciones segÃºn severidad

| Drift Share | AcciÃ³n |
|-------------|--------|
| < 10% | Monitorear, no acciÃ³n inmediata |
| 10-30% | Investigar en 48h, considerar retrain |
| > 30% | Retrain urgente, posible rollback a modelo anterior |

## Comandos de Retrain

```bash
# 1. Verificar datos disponibles
ls -la data/recent/

# 2. Disparar retrain
python main.py --mode train --experiment-name "retrain-$(date +%Y%m%d)"

# 3. Comparar mÃ©tricas
python scripts/compare_models.py --baseline production --candidate new

# 4. Si mejora, promover
python scripts/promote_model.py --model-name bankchurn --stage Production
```
```

### 16.5.3 Runbook: Service Down

```markdown
# ğŸš¨ Runbook: ServiceDown

## Resumen
| Campo | Valor |
|-------|-------|
| Alerta | `ServiceDown` |
| Severidad | CRITICAL |
| Impacto | Servicio completamente inaccesible |
| SLA | Responder < 5 min |

## DiagnÃ³stico Inmediato (< 1 min)

```bash
# 1. Verificar estado de contenedores
docker ps -a | grep -E "(bankchurn|carvision|telecom)"

# 2. Ver Ãºltimo log
docker logs --tail 50 <container_name>

# 3. Health check manual
curl -v http://localhost:8001/health
```

## RecuperaciÃ³n RÃ¡pida

### OpciÃ³n A: Reiniciar servicio
```bash
docker restart bankchurn-api
# Esperar 30s y verificar
curl http://localhost:8001/health
```

### OpciÃ³n B: Recrear contenedor
```bash
docker compose -f docker-compose.demo.yml up -d bankchurn
```

### OpciÃ³n C: Rollback a versiÃ³n anterior
```bash
# Si el problema es por deploy reciente
docker pull ghcr.io/duqueom/bankchurn:previous-tag
docker compose up -d
```

## Causas Comunes

| Causa | DiagnÃ³stico | SoluciÃ³n |
|-------|-------------|----------|
| OOM Kill | `docker logs` muestra `Killed` | Aumentar memory limit |
| Puerto ocupado | `netstat -tlnp | grep 8001` | Matar proceso conflictivo |
| Modelo no encontrado | Log: `FileNotFoundError` | Verificar volumen montado |
| Crash en startup | Exit code 1 | Ver logs completos |

## ComunicaciÃ³n
- Notificar en #incidents-ml dentro de 5 min
- Si > 15 min: Actualizar status page
- Si > 30 min: Escalar a management
```

### 16.5.4 Template de Runbook

```markdown
# ğŸš¨ Runbook: [NOMBRE_ALERTA]

## Resumen
| Campo | Valor |
|-------|-------|
| Alerta | `[nombre]` |
| Severidad | [Warning/Critical] |
| Impacto | [DescripciÃ³n del impacto en usuarios/negocio] |
| SLA | [Tiempo mÃ¡ximo de respuesta] |
| Owner | [@team o @persona] |

## DiagnÃ³stico
1. [Paso 1 con comando]
2. [Paso 2 con comando]
3. [Paso 3]

## Causas Comunes
| Causa | SÃ­ntomas | SoluciÃ³n |
|-------|----------|----------|
| [Causa 1] | [CÃ³mo identificarla] | [Comandos/pasos] |
| [Causa 2] | [CÃ³mo identificarla] | [Comandos/pasos] |

## EscalaciÃ³n
- [CuÃ¡ndo escalar]
- [A quiÃ©n escalar]

## Referencias
- [Links a dashboards relevantes]
- [Links a documentaciÃ³n]
```

### 16.5.5 OrganizaciÃ³n de Runbooks en el Repo

```
docs/runbooks/
â”œâ”€â”€ README.md                    # Ãndice de runbooks
â”œâ”€â”€ high-latency.md             # Latencia alta
â”œâ”€â”€ high-error-rate.md          # Tasa de error
â”œâ”€â”€ service-down.md             # Servicio caÃ­do
â”œâ”€â”€ data-drift.md               # Drift detectado
â”œâ”€â”€ model-degradation.md        # DegradaciÃ³n de mÃ©tricas
â””â”€â”€ disk-full.md                # Disco lleno (artifacts/logs)
```

---

<a id="errores-habituales"></a>
 
## ğŸ§¨ Errores habituales y cÃ³mo depurarlos en Observabilidad ML

En observabilidad ML es habitual tener dashboards bonitos pero poca seÃ±al Ãºtil, o scripts de drift que fallan en silencio.

Si alguno de estos errores te tomÃ³ **>15 minutos**, regÃ­stralo en el **[Diario de Errores](study_tools/DIARIO_ERRORES.md)** y aplica el flujo de **rescate cognitivo** de **[Protocolo E](study_tools/PROTOCOLO_E.md)**.

### 1) MÃ©tricas que no aparecen en Prometheus/Grafana

**SÃ­ntomas tÃ­picos**

- En Grafana, los paneles muestran `No data`.
- En Prometheus, la mÃ©trica `predictions_total` no existe o tiene solo ceros.

**CÃ³mo identificarlo**

- Verifica que el endpoint `/metrics` responde localmente (`curl http://localhost:8000/metrics`).
- Revisa `prometheus-config.yaml`:
  - Â¿El `job_name` y `targets` apuntan al host/puerto correctos?
  - Â¿`metrics_path` es `/metrics`?

**CÃ³mo corregirlo**

- Asegura que el API exponga `/metrics` y que el contenedor estÃ© accesible desde Prometheus (mismo docker network).
- Usa nombres de servicio (`bankchurn:8000`) coherentes con `docker-compose`.

---

### 2) Alertas demasiado ruidosas (alert fatigue)

**SÃ­ntomas tÃ­picos**

- Canal de Slack/Email lleno de alertas constantes que el equipo ignora.

**CÃ³mo identificarlo**

- Revisa las reglas de alerta: thresholds demasiado agresivos (por ejemplo, alertar por cualquier spike puntual).

**CÃ³mo corregirlo**

- Usa ventanas de tiempo y reglas de severidad (warning vs critical).
- Define claramente mÃ©tricas **crÃ­ticas** (latencia P99, error rate, dataset_drift) y otras solo informativas.

---

### 3) Logs JSON imposibles de parsear

**SÃ­ntomas tÃ­picos**

- La herramienta de logs (ELK, Loki, etc.) no reconoce campos como `request_id` o `prediction`.
- Aparecen lÃ­neas mezcladas de formatos distintos.

**CÃ³mo identificarlo**

- Revisa `setup_logging`: Â¿todos los handlers usan `JSONFormatter` en producciÃ³n?
- Busca logs que usen `print` en vez de `logger.info`.

**CÃ³mo corregirlo**

- Centraliza la configuraciÃ³n de logging y evita crear loggers adicionales con otros formatos.
- Usa siempre `extra={...}` en los logs de negocio en vez de concatenar strings.

---

### 4) Script de drift que falla en CI o nunca encuentra drift

**SÃ­ntomas tÃ­picos**

- El workflow `drift-detection.yml` falla por `ImportError: evidently` o rutas incorrectas.
- El script siempre devuelve "âœ… No hay drift" aunque sabes que los datos cambiaron.

**CÃ³mo identificarlo**

- Revisa los paths `--reference` y `--current` usados en el workflow.
- Comprueba que `EVIDENTLY_AVAILABLE` es `True` y que las columnas de referencia/actual coinciden.

**CÃ³mo corregirlo**

- Alinea las rutas de datos de referencia y actuales con la estructura de tu repo.
- AsegÃºrate de instalar `evidently` en el job de CI (`pip install evidently`).
- Revisa el JSON de mÃ©tricas generado para validar que `drift_share` y `drifted_features` tienen sentido.

---

### 5) PatrÃ³n general de debugging en observabilidad ML

1. Empieza por el **flujo de datos**: API â†’ `/metrics` â†’ Prometheus â†’ Grafana.
2. Verifica que logs y mÃ©tricas contengan campos de negocio (no solo tÃ©cnica bÃ¡sica).
3. Revisa periÃ³dicamente los umbrales de alerta segÃºn el comportamiento real del sistema.
4. Usa los reports de drift como insumo para decisiones, no como verdad absoluta: combÃ­nalos con mÃ©tricas de negocio.

Con esta mentalidad, la observabilidad deja de ser un "extra" y se convierte en tu principal herramienta para operar modelos en producciÃ³n.

---

## ğŸ’¼ Consejos Profesionales

> **Recomendaciones para destacar en entrevistas y proyectos reales**

### Para Entrevistas

1. **Observability vs Monitoring**: Monitoring = mÃ©tricas predefinidas, Observability = entender comportamiento inesperado.

2. **Three Pillars**: Logs, Metrics, Traces. Explica cada uno.

3. **ML Monitoring**: Model drift, data drift, concept drift.

### Para Proyectos Reales

| SituaciÃ³n | Consejo |
|-----------|---------|
| Alertas | Evita alert fatigue: alerta solo lo accionable |
| Dashboards | Un dashboard por audiencia (ops, ML, negocio) |
| On-call | Documenta runbooks para cada alerta |
| Drift detection | Monitorea distribuciones de features y predictions |

### MÃ©tricas Clave para ML

- **Serving**: Latency p50/p95/p99, error rate, throughput
- **Model**: Prediction distribution, confidence scores
- **Data**: Missing values, schema changes, drift
- **Business**: Conversion, revenue impact


---

## ğŸ“º Recursos Externos del MÃ³dulo

> ğŸ·ï¸ Sistema: ğŸ”´ Obligatorio | ğŸŸ¡ Recomendado | ğŸŸ¢ Complementario

### ğŸ¬ Videos

| ğŸ·ï¸ | TÃ­tulo | Canal | DuraciÃ³n | Link |
|:--:|:-------|:------|:--------:|:-----|
| ğŸ”´ | **Prometheus + Grafana Tutorial** | TechWorld with Nana | 50 min | [YouTube](https://www.youtube.com/watch?v=7gW5pSM6dlU) |
| ğŸŸ¡ | **ML Model Monitoring with Evidently** | Evidently AI | 30 min | [YouTube](https://www.youtube.com/watch?v=L4Pv6ExBQPM) |
| ğŸŸ¢ | **Drift Detection Explained** | NannyML | 25 min | [YouTube](https://www.youtube.com/watch?v=82Sb8n3wN24) |

### ğŸ“š Cursos

| ğŸ·ï¸ | TÃ­tulo | Plataforma | DuraciÃ³n | Link |
|:--:|:-------|:-----------|:--------:|:-----|
| ğŸŸ¡ | ML Monitoring | Made With ML | 3h | [MadeWithML](https://madewithml.com/courses/mlops/monitoring/) |

### ğŸ“„ DocumentaciÃ³n

| ğŸ·ï¸ | Recurso | DescripciÃ³n |
|:--:|:--------|:------------|
| ğŸ”´ | [Evidently Docs](https://docs.evidentlyai.com/) | DocumentaciÃ³n oficial |
| ğŸŸ¡ | [Prometheus Docs](https://prometheus.io/docs/) | DocumentaciÃ³n de Prometheus |
| ğŸŸ¢ | [Grafana Dashboards](https://grafana.com/docs/grafana/latest/dashboards/) | CreaciÃ³n de dashboards |

---

## âš–ï¸ DecisiÃ³n TÃ©cnica: ADR-011 Prometheus + Grafana

**Contexto**: Necesitamos monitorear modelos en producciÃ³n y detectar drift.

**DecisiÃ³n**: Usar Prometheus para mÃ©tricas y Grafana para dashboards.

**Alternativas Consideradas**:
- **Datadog**: Excelente pero costoso
- **New Relic**: Similar a Datadog
- **CloudWatch/Stackdriver**: Vendor lock-in

**Consecuencias**:
- âœ… Open source, sin costo
- âœ… EstÃ¡ndar de la industria
- âœ… Alertas configurables
- âœ… IntegraciÃ³n con K8s nativa
- âŒ MÃ¡s setup que SaaS

---

## ğŸ”§ Ejercicios del MÃ³dulo

### Ejercicio 16.1: Logging Estructurado
**Objetivo**: Implementar logging profesional en JSON.
**Dificultad**: â­â­

```python
# TU TAREA: Configurar logging estructurado

import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    """Formatter que produce logs en JSON."""
    
    def format(self, record):
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
            # TODO: AÃ±adir mÃ¡s campos Ãºtiles
        }
        return json.dumps(log_data)

# TODO: Configurar logger con este formatter
```

<details>
<summary>ğŸ’¡ Ver soluciÃ³n</summary>

```python
import logging
import json
import sys
from datetime import datetime
from typing import Any

class JSONFormatter(logging.Formatter):
    """Formatter que produce logs en JSON estructurado."""
    
    def format(self, record: logging.LogRecord) -> str:
        log_data: dict[str, Any] = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        
        # AÃ±adir campos extra si existen
        if hasattr(record, "customer_id"):
            log_data["customer_id"] = record.customer_id
        if hasattr(record, "prediction"):
            log_data["prediction"] = record.prediction
        if hasattr(record, "latency_ms"):
            log_data["latency_ms"] = record.latency_ms
            
        # AÃ±adir exception si existe
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)
            
        return json.dumps(log_data)


def setup_logger(name: str = "ml_api") -> logging.Logger:
    """Configura logger con formato JSON."""
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    
    # Handler para stdout
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(JSONFormatter())
    logger.addHandler(handler)
    
    return logger


# Uso:
logger = setup_logger()
logger.info("Prediction made", extra={"customer_id": 123, "prediction": 1, "latency_ms": 45})
# Output: {"timestamp": "2024-01-15T10:30:00Z", "level": "INFO", "message": "Prediction made", ...}
```
</details>

---

## ğŸ”— Glosario del MÃ³dulo

| TÃ©rmino | DefiniciÃ³n |
|---------|------------|
| **Data Drift** | Cambio en la distribuciÃ³n de features entre training y producciÃ³n |
| **Concept Drift** | Cambio en la relaciÃ³n P(Y\|X) entre features y target |
| **PSI** | Population Stability Index - mÃ©trica para detectar drift |
| **Prometheus** | Sistema open source de monitoreo y alertas basado en mÃ©tricas |
| **Grafana** | Plataforma de visualizaciÃ³n para dashboards de mÃ©tricas |

---

## ğŸ CHECKPOINT FASE 3: MLOps Core Completado

> ğŸ¯ **Â¡Has completado los mÃ³dulos 11-16!**
>
> Ahora dominas las prÃ¡cticas que distinguen un proyecto ML profesional:
> - âœ… Testing para ML con 80%+ coverage
> - âœ… CI/CD con GitHub Actions
> - âœ… Docker multi-stage y docker-compose
> - âœ… APIs de producciÃ³n con FastAPI
> - âœ… Dashboards interactivos con Streamlit
> - âœ… Observabilidad con Prometheus/Grafana y drift detection

---

### ğŸ“‹ ExÃ¡menes de Hito: Testing y Deployment

#### Examen 3: Testing (Extracto)

**CÃ³digo a Revisar:**
```python
# tests/test_model.py

def test_model():
    model = load_model()
    data = pd.read_csv("data/test.csv")
    predictions = model.predict(data)
    assert len(predictions) == len(data)
```

**Problemas a identificar:** Â¿QuÃ© falta en este test?

<details>
<summary>ğŸ“ Ver SoluciÃ³n</summary>

**Errores:**
1. No usa fixtures (hardcoded paths)
2. No verifica tipos de predicciones
3. No valida rangos vÃ¡lidos
4. No es reproducible (depende de archivo externo)

**Test Corregido:**
```python
import pytest
import numpy as np

@pytest.fixture
def sample_data():
    return pd.DataFrame({
        "feature1": [1.0, 2.0, 3.0],
        "feature2": [0.5, 1.5, 2.5],
    })

@pytest.fixture
def trained_model(sample_data):
    # Modelo entrenado en fixture
    return train_model(sample_data, labels=[0, 1, 0])

def test_predictions_shape(trained_model, sample_data):
    predictions = trained_model.predict(sample_data)
    assert len(predictions) == len(sample_data)
    
def test_predictions_valid_range(trained_model, sample_data):
    predictions = trained_model.predict(sample_data)
    assert all(p in [0, 1] for p in predictions)
    
def test_predictions_type(trained_model, sample_data):
    predictions = trained_model.predict(sample_data)
    assert isinstance(predictions, np.ndarray)
```
</details>

---

### ğŸ¤ Simulacro de Entrevista: Nivel Mid

> **60 preguntas** para validar MLOps Core (MÃ³dulos 07-16)
> **Tiempo**: 90 minutos
> **Objetivo**: PreparaciÃ³n para posiciones Mid ML Engineer

#### Preguntas de Muestra

**Testing ML (10 preguntas)**
1. Â¿QuÃ© es la pirÃ¡mide de testing y cÃ³mo aplica a ML?
2. Â¿CÃ³mo testeas que un modelo no tiene data leakage?
3. Â¿QuÃ© fixtures usarÃ­as en `conftest.py` para tests ML?

**CI/CD (10 preguntas)**
4. Â¿CÃ³mo configurarÃ­as matrix testing en GitHub Actions?
5. Â¿QuÃ© es un coverage gate y por quÃ© es importante?
6. Â¿CÃ³mo integras security scanning en tu pipeline?

**Docker (10 preguntas)**
7. Â¿Por quÃ© usar multi-stage builds para ML?
8. Â¿CÃ³mo optimizas el tamaÃ±o de imagen Docker para ML?
9. Â¿QuÃ© es un usuario non-root y por quÃ© usarlo?

**APIs (10 preguntas)**
10. Â¿CÃ³mo manejas errores en FastAPI para ML?
11. Â¿QuÃ© endpoints de health check implementarÃ­as?
12. Â¿CÃ³mo validas inputs con Pydantic en APIs ML?

**Observabilidad (10 preguntas)**
13. Â¿QuÃ© mÃ©tricas capturarÃ­as para un modelo en producciÃ³n?
14. Â¿CÃ³mo detectas data drift en producciÃ³n?
15. Â¿Diferencia entre logging estructurado y tradicional?

<details>
<summary>ğŸ’¡ Ver Respuestas de Muestra</summary>

**1. PirÃ¡mide de testing en ML:**
> Base: unit tests (funciones individuales), Medio: integration tests (pipeline completo), Top: E2E tests (API funcionando). En ML agregamos: tests de datos (schema, rangos), tests de modelo (reproducibilidad, mÃ©tricas mÃ­nimas).

**7. Multi-stage builds:**
> Separamos build (instalar dependencias, compilar) de runtime (solo lo necesario para ejecutar). Reduce imagen de ~2GB a ~500MB. Stage 1: instala todo, Stage 2: copia solo wheels y cÃ³digo.

**14. Detectar data drift:**
> PSI (Population Stability Index) para features categÃ³ricas, KS-test para numÃ©ricas. Umbral tÃ­pico: PSI > 0.2 = drift significativo. Herramientas: Evidently, Alibi Detect, Great Expectations.
</details>

---

[Ver simulacro completo â†’](simulacros/SIMULACRO_ENTREVISTA_MID.md)

---

## âœ… Checkpoint del MÃ³dulo

- [ ] Tienes endpoint `/metrics` en tu API
- [ ] Logs en formato JSON estructurado
- [ ] Script de drift detection funcional
- [ ] Alertas configuradas para mÃ©tricas crÃ­ticas

---

## ğŸ”œ Siguiente Fase: ProducciÃ³n

Con MLOps Core completado, es hora de aprender **estrategias de despliegue e infraestructura**.

**[Comenzar Fase 4 â†’ MÃ³dulo 17: Despliegue](17_DESPLIEGUE.md)**

---

<div align="center">

[â† Streamlit Dashboards](15_STREAMLIT.md) | [Siguiente: Despliegue â†’](17_DESPLIEGUE.md)

</div>
