# 07. sklearn Pipelines: El CorazÃ³n de MLOps

## ğŸ¯ Objetivo del MÃ³dulo

Dominar el patrÃ³n mÃ¡s importante de ML profesional: **pipelines unificados** que garantizan reproducibilidad desde entrenamiento hasta producciÃ³n.

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘  ğŸš¨ EL ERROR #1 EN PRODUCCIÃ“N ML:                                            â•‘
â•‘                                                                              â•‘
â•‘  Entrenar con una transformaciÃ³n, servir con otra.                           â•‘
â•‘                                                                              â•‘
â•‘  Ejemplo real:                                                               â•‘
â•‘  â€¢ Training: StandardScaler fitted en train set (mean=45000, std=20000)      â•‘
â•‘  â€¢ Production: StandardScaler fitted en cada request (mean=???, std=???)     â•‘
â•‘  â€¢ Resultado: Predicciones COMPLETAMENTE diferentes                          â•‘
â•‘                                                                              â•‘
â•‘  ğŸ›¡ï¸ LA SOLUCIÃ“N: Pipeline unificado que guarda TODO junto                    â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<a id="00-prerrequisitos"></a>

## 0.0 Prerrequisitos

- Haber completado **[01_PYTHON_MODERNO](01_PYTHON_MODERNO.md)** y entender el motivo del `src/` layout.
- Tener un proyecto del portafolio a mano (ideal: BankChurn) para ubicar el `pipeline.pkl` real.
- Entender el problema de *training-serving skew* (al menos a nivel conceptual).

---

<a id="01-protocolo-e-como-estudiar-este-modulo"></a>

## 0.1 ğŸ§  Protocolo E: CÃ³mo estudiar este mÃ³dulo

- **Antes de codificar**: abre **[Protocolo E](study_tools/PROTOCOLO_E.md)** y define tu *output mÃ­nimo* (ej: â€œpipeline serializable + tests bÃ¡sicosâ€).
- **Mientras debuggeas**: si te atoras >15 min (ColumnTransformer, columnas, dtypes, `fit/transform`), registra el bloqueo en **[Diario de Errores](study_tools/DIARIO_ERRORES.md)**.
- **Al cerrar la semana**: usa **[Cierre Semanal](study_tools/CIERRE_SEMANAL.md)** para decidir quÃ© mejorar (reproducibilidad, tests, DX).

---

<a id="02-entregables-verificables-minimo-viable"></a>

## 0.2 âœ… Entregables verificables (mÃ­nimo viable)

Al terminar este mÃ³dulo, deberÃ­as poder mostrar (en al menos 1 proyecto del portafolio):

- [ ] **1 pipeline unificado** serializado (`pipeline.pkl`) que incluya preprocesamiento + modelo.
- [ ] **Inferencia consistente**: `pipeline.predict(X_new)` sin re-fit de transformadores.
- [ ] **Checklist de verificaciÃ³n** pasando (secciÃ³n â€œCheckpointâ€).

---

<a id="03-puente-teoria-codigo-portafolio"></a>

## 0.3 ğŸ§© Puente teorÃ­a â†” cÃ³digo (Portafolio)

Para que esto cuente como progreso real, fuerza este mapeo:

- **Concepto**: Pipeline/ColumnTransformer/custom transformers
- **Archivo**: `src/<paquete>/training.py`, `src/<paquete>/features.py`, `models/pipeline.pkl`
- **Prueba**: entrenar una vez, guardar `pipeline.pkl`, cargarlo y predecir con datos nuevos.

---

## ğŸ“‹ Contenido

- **0.0** [Prerrequisitos](#00-prerrequisitos)
- **0.1** [Protocolo E: CÃ³mo estudiar este mÃ³dulo](#01-protocolo-e-como-estudiar-este-modulo)
- **0.2** [Entregables verificables (mÃ­nimo viable)](#02-entregables-verificables-minimo-viable)
- **0.3** [Puente teorÃ­a â†” cÃ³digo (Portafolio)](#03-puente-teoria-codigo-portafolio)
1. [Â¿Por QuÃ© Pipelines?](#71-por-que-pipelines)
2. [ColumnTransformer: Transformaciones Paralelas](#72-columntransformer-transformaciones-paralelas)
3. [Custom Transformers](#73-custom-transformers-tu-superpoder)
4. [Pipeline Completo: CÃ³digo Real](#74-pipeline-completo-codigo-real)
5. [Ejercicios PrÃ¡cticos](#75-ejercicios-practicos)
- [Errores habituales](#errores-habituales)
- [âœ… Checkpoint](#checkpoint)

---

<a id="71-por-que-pipelines"></a>

## 7.1 Â¿Por QuÃ© Pipelines?

### La AnalogÃ­a de la LÃ­nea de Ensamblaje

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ­ IMAGINA UNA FÃBRICA DE AUTOS:                                         â•‘
â•‘                                                                           â•‘
â•‘  SIN LÃNEA DE ENSAMBLAJE (cÃ³digo suelto):                                 â•‘
â•‘  â€¢ Trabajador 1 pone ruedas, pero a veces se le olvida                    â•‘
â•‘  â€¢ Trabajador 2 pinta, pero usa colores diferentes cada dÃ­a               â•‘
â•‘  â€¢ Trabajador 3 instala motor, pero a veces del modelo equivocado         â•‘
â•‘  â€¢ Resultado: Cada auto es diferente, imposible de mantener               â•‘
â•‘                                                                           â•‘
â•‘  CON LÃNEA DE ENSAMBLAJE (Pipeline):                                      â•‘
â•‘  â€¢ Paso 1: Chasis â†’ Paso 2: Motor â†’ Paso 3: Pintura â†’ Paso 4: Ruedas      â•‘
â•‘  â€¢ Cada paso estÃ¡ definido y es SIEMPRE igual                             â•‘
â•‘  â€¢ El proceso completo es una sola unidad                                 â•‘
â•‘  â€¢ Resultado: Todos los autos son consistentes                            â•‘
â•‘                                                                           â•‘
â•‘  sklearn Pipeline = LÃ­nea de ensamblaje para ML                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### El Problema Real: Training-Serving Skew

```python
# âŒ CÃ“DIGO PROBLEMÃTICO (muy comÃºn en notebooks convertidos a producciÃ³n)

# === ENTRENAMIENTO ===
from sklearn.preprocessing import StandardScaler, OneHotEncoder  # Importa transformadores de sklearn.

# Ajustar scaler en datos de entrenamiento
scaler = StandardScaler()                                  # Crea instancia: aÃºn no tiene parÃ¡metros learned.
X_train_scaled = scaler.fit_transform(X_train[num_cols])   # fit_transform: calcula mean/std Y transforma.

encoder = OneHotEncoder()                                  # Crea encoder para convertir categorÃ­as a binario.
X_train_encoded = encoder.fit_transform(X_train[cat_cols]) # fit: aprende categorÃ­as Ãºnicas; transform: aplica.

# Entrenar modelo
model = RandomForestClassifier()                           # Crea el modelo de clasificaciÃ³n.
model.fit(X_train_processed, y_train)                      # Entrena con datos ya transformados.

# Guardar modelo... pero Â¿y el scaler? Â¿y el encoder?
joblib.dump(model, "model.pkl")  # â† Â¡ERROR! Solo guarda el modelo, NO los transformadores.

# === PRODUCCIÃ“N (meses despuÃ©s, otro desarrollador) ===
model = joblib.load("model.pkl")                           # Carga solo el modelo.

# Â¿CÃ³mo transformo los datos nuevos?
# ğŸ¤· No tengo el scaler ni el encoder fitted             # Los transformadores se perdieron.
# ğŸ¤· Incluso si los tuviera, Â¿cÃ³mo sÃ© quÃ© columnas usar? # No hay documentaciÃ³n de las columnas.
# ğŸ¤· Â¿Era StandardScaler o MinMaxScaler?                 # Imposible saber quÃ© se usÃ³.

# "SoluciÃ³n" del desarrollador desesperado:
scaler = StandardScaler()                                  # Crea NUEVO scaler (sin los parÃ¡metros originales).
X_new_scaled = scaler.fit_transform(X_new[num_cols])       # fit en datos NUEVOS: mean/std DIFERENTES.
# âš ï¸ Ahora mean y std son DIFERENTES a los de entrenamiento â†’ training-serving skew.
# âš ï¸ Las predicciones son BASURA porque la escala es inconsistente.

# ============================================================================
# âœ… SOLUCIÃ“N: Pipeline Unificado
# ============================================================================

# === ENTRENAMIENTO ===
from sklearn.pipeline import Pipeline              # Pipeline: encadena pasos secuenciales.
from sklearn.compose import ColumnTransformer      # ColumnTransformer: aplica transformaciones por grupo de columnas.

# Definir pipeline completo
pipeline = Pipeline([                              # Lista de tuplas (nombre, objeto).
    ('preprocessor', ColumnTransformer([           # Primer paso: preprocesamiento por columnas.
        ('num', StandardScaler(), num_cols),       # Escala numÃ©ricas (aprende mean/std de train).
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)  # One-hot categorÃ­cas; ignore evita crash.
    ])),
    ('model', RandomForestClassifier())            # Segundo paso: el modelo.
])

# Un solo fit entrena TODO
pipeline.fit(X_train, y_train)                     # fit() propaga por todos los pasos: transforma Y entrena.

# Guardar TODO junto
joblib.dump(pipeline, "pipeline.pkl")              # Serializa Scaler + Encoder + Model en UN archivo.

# === PRODUCCIÃ“N ===
pipeline = joblib.load("pipeline.pkl")             # Carga todo: transformadores YA fitted + modelo.

# Una sola llamada hace TODO (con los parÃ¡metros de entrenamiento)
predictions = pipeline.predict(X_new)              # predict() internamente transforma X_new y luego predice.

# âœ… El scaler usa mean/std del entrenamiento      â†’ Consistencia garantizada.
# âœ… El encoder conoce las categorÃ­as del entrenamiento â†’ No crash por categorÃ­as nuevas.
# âœ… Las predicciones son consistentes             â†’ Sin training-serving skew.
```

---

<a id="72-columntransformer-transformaciones-paralelas"></a>

## 7.2 ColumnTransformer: Transformaciones Paralelas

### El Problema: Diferentes Columnas, Diferentes Tratamientos

```
Datos de un banco:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CreditScore â”‚ Geography â”‚ Gender  â”‚   Age   â”‚ Balanceâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     619     â”‚  France   â”‚  Female â”‚    42   â”‚  10000 â”‚
â”‚     608     â”‚   Spain   â”‚  Female â”‚    41   â”‚  83808 â”‚
â”‚     502     â”‚  France   â”‚  Female â”‚    42   â”‚      0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Columnas numÃ©ricas (CreditScore, Age, Balance):
â†’ StandardScaler: normalizar a mean=0, std=1

Columnas categÃ³ricas (Geography, Gender):
â†’ OneHotEncoder: convertir a columnas binarias
```

### ColumnTransformer: La SoluciÃ³n Elegante

```python
from sklearn.compose import ColumnTransformer        # Enruta transformaciones por grupos de columnas.
from sklearn.preprocessing import StandardScaler, OneHotEncoder  # Transformadores estÃ¡ndar.
from sklearn.impute import SimpleImputer             # Imputa valores faltantes (NaN).
from sklearn.pipeline import Pipeline                # Encadena pasos secuenciales.

# Definir quÃ© columnas son de cada tipo
num_cols = ["CreditScore", "Age", "Tenure", "Balance", "NumOfProducts", "EstimatedSalary"]  # NumÃ©ricas.
cat_cols = ["Geography", "Gender"]                   # CategÃ³ricas: valores discretos/textuales.

# Pipeline para numÃ©ricas: Imputar NaN â†’ Escalar
num_pipeline = Pipeline([                            # Pipeline DENTRO de ColumnTransformer.
    ('imputer', SimpleImputer(strategy='median')),   # Rellena NaN con mediana (robusto a outliers).
    ('scaler', StandardScaler())                     # Normaliza a mean=0, std=1.
])

# Pipeline para categÃ³ricas: Imputar NaN â†’ One-Hot
cat_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),  # NaN â†’ string "Unknown".
    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # ignore: categorÃ­as nuevas â†’ vector de ceros.
])

# ColumnTransformer: Aplica cada pipeline a sus columnas
preprocessor = ColumnTransformer(
    transformers=[                                   # Lista de transformadores.
        ('num', num_pipeline, num_cols),             # (nombre, transformer, columnas_a_transformar).
        ('cat', cat_pipeline, cat_cols)              # Cada grupo se procesa independientemente.
    ],
    remainder='drop'                                 # 'drop': elimina columnas no listadas. 'passthrough': las deja.
)

# Resultado: Un solo objeto que sabe transformar todo
X_processed = preprocessor.fit_transform(X_train)   # fit: aprende parÃ¡metros; transform: aplica.
```

### VisualizaciÃ³n del Flujo

```
                        ColumnTransformer
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚               â”‚               â”‚
              â–¼               â–¼               â–¼
        num_pipeline    cat_pipeline    remainder
              â”‚               â”‚               â”‚
              â”‚               â”‚               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚               â”‚
    â”‚                   â”‚     â”‚               â”‚
    â–¼                   â–¼     â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   drop
â”‚Imputer  â”‚       â”‚ Scaler  â”‚ â”‚ Imputer â”‚
â”‚(median) â”‚       â”‚         â”‚ â”‚(Unknown)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ OneHot  â”‚
                              â”‚ Encoder â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                    â”‚                    â”‚
              â–¼                    â–¼                    â–¼
    [6 columnas numÃ©ricas]  [3 Geography cols]  [2 Gender cols]
         escaladas            (France, Spain,     (Female, Male)
                               Germany)

    Output: 11 columnas totales (6 + 3 + 2)
```

---

<a id="73-custom-transformers-tu-superpoder"></a>

## 7.3 Custom Transformers: Tu Superpoder

### Â¿CuÃ¡ndo Crear un Custom Transformer?

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Crea un Custom Transformer cuando:                                       â•‘
â•‘                                                                           â•‘
â•‘  âœ… Necesitas feature engineering especÃ­fico del dominio                  â•‘
â•‘  âœ… La transformaciÃ³n debe aplicarse igual en train y producciÃ³n          â•‘
â•‘  âœ… sklearn no tiene un transformer que haga lo que necesitas             â•‘
â•‘                                                                           â•‘
â•‘  Ejemplos del portafolio:                                                 â•‘
â•‘  â€¢ CarVision: Calcular vehicle_age desde model_year                       â•‘
â•‘  â€¢ CarVision: Extraer brand desde model                                   â•‘
â•‘  â€¢ BankChurn: Resampling para clases desbalanceadas                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Ejemplo 1: FeatureEngineer (CarVision)

```python
# src/carvision/features.py - CÃ³digo REAL del portafolio

from __future__ import annotations

from typing import Optional

import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin


class FeatureEngineer(BaseEstimator, TransformerMixin):
    """
    Centralized feature engineering to ensure consistency across
    Training, Inference, and Analysis.
    
    Este transformer garantiza que las mismas transformaciones
    se apliquen en:
    1. Entrenamiento (training.py)
    2. Inferencia API (fastapi_app.py)
    3. Dashboard (streamlit_app.py)
    
    Attributes
    ----------
    current_year : int, optional
        AÃ±o para calcular vehicle_age. Si None, usa aÃ±o actual.
    
    Examples
    --------
    >>> fe = FeatureEngineer(current_year=2024)
    >>> df_transformed = fe.fit_transform(df)
    >>> print(df_transformed.columns)
    # Incluye: vehicle_age, brand (derivadas de model_year y model)
    """

    def __init__(self, current_year: Optional[int] = None):
        self.current_year = current_year

    def fit(self, X: pd.DataFrame, y: pd.DataFrame = None) -> "FeatureEngineer":
        """Fit no hace nada (stateless transformer)."""
        # Este transformer es stateless: no aprende nada de los datos
        # Solo necesita fit() para ser compatible con Pipeline
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """Aplica feature engineering.
        
        Features creadas:
        - vehicle_age: current_year - model_year
        - brand: primera palabra de model
        - price_per_mile: price / odometer (solo si price existe)
        """
        X = X.copy()  # â† Nunca modificar el input original

        # Usar aÃ±o configurado o aÃ±o actual
        year = self.current_year or pd.Timestamp.now().year

        # Feature: Edad del vehÃ­culo
        if "model_year" in X.columns:
            X["vehicle_age"] = year - X["model_year"]

        # Feature: Marca (primera palabra del modelo)
        if "model" in X.columns:
            X["brand"] = X["model"].astype(str).str.split().str[0]

        # Features derivadas (solo en training, no en inferencia)
        # Porque price no estÃ¡ disponible en inferencia
        if "odometer" in X.columns and "price" in X.columns:
            X["price_per_mile"] = X["price"] / (X["odometer"] + 1)

        return X
    
    # MÃ©todos opcionales para mejor introspecciÃ³n
    def get_feature_names_out(self, input_features=None):
        """Retorna nombres de features de salida."""
        base = list(input_features) if input_features else []
        return base + ["vehicle_age", "brand"]
```

### Ejemplo 2: ResampleClassifier (BankChurn)

```python
# src/bankchurn/models.py - CÃ³digo REAL del portafolio

from __future__ import annotations

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils.validation import check_is_fitted


class ResampleClassifier(BaseEstimator, ClassifierMixin):
    """Custom classifier with resampling for imbalanced datasets.
    
    Implementa oversampling (SMOTE), undersampling, y class weighting
    para mejorar performance en clasificaciÃ³n desbalanceada.
    
    Este wrapper permite:
    1. Probar diferentes estrategias de resampling fÃ¡cilmente
    2. Mantener la interfaz sklearn estÃ¡ndar (fit/predict)
    3. Ser parte de un Pipeline (incluyendo GridSearchCV)
    
    Parameters
    ----------
    estimator : estimator object, optional
        Clasificador base. Si None, usa LogisticRegression.
    strategy : {"none", "oversample", "undersample", "class_weight"}
        Estrategia de resampling:
        - "none": Sin resampling
        - "oversample": SMOTE oversampling de clase minoritaria
        - "undersample": Undersampling de clase mayoritaria
        - "class_weight": Balanceo automÃ¡tico de pesos
    random_state : int, default=42
        Semilla para reproducibilidad.
    
    Examples
    --------
    >>> clf = ResampleClassifier(
    ...     estimator=RandomForestClassifier(),
    ...     strategy="oversample",
    ...     random_state=42
    ... )
    >>> clf.fit(X_train, y_train)
    >>> predictions = clf.predict(X_test)
    """

    def __init__(
        self,
        estimator: BaseEstimator | None = None,
        strategy: str = "none",
        random_state: int = 42,
    ) -> None:
        self.estimator = estimator
        self.strategy = strategy
        self.random_state = random_state

    def fit(self, X: np.ndarray, y: np.ndarray) -> "ResampleClassifier":
        """Entrena el clasificador con resampling opcional."""
        from sklearn.linear_model import LogisticRegression

        # Inicializar estimador si no se proporcionÃ³
        if self.estimator is None:
            self.estimator_ = LogisticRegression(random_state=self.random_state)
        else:
            # Clonar para no modificar el original
            from sklearn.base import clone
            self.estimator_ = clone(self.estimator)

        # Guardar clases (requerido por sklearn)
        self.classes_ = np.unique(y)

        # Aplicar estrategia de resampling
        X_resampled, y_resampled = self._apply_resampling(X, y)

        # Entrenar estimador base
        self.estimator_.fit(X_resampled, y_resampled)

        return self

    def _apply_resampling(
        self, X: np.ndarray, y: np.ndarray
    ) -> tuple[np.ndarray, np.ndarray]:
        """Aplica la estrategia de resampling."""
        if self.strategy == "none":
            return X, y
        
        elif self.strategy == "oversample":
            try:
                from imblearn.over_sampling import SMOTE
                smote = SMOTE(random_state=self.random_state)
                return smote.fit_resample(X, y)
            except ImportError:
                # Si imblearn no estÃ¡ instalado, ignorar
                return X, y
        
        elif self.strategy == "undersample":
            try:
                from imblearn.under_sampling import RandomUnderSampler
                rus = RandomUnderSampler(random_state=self.random_state)
                return rus.fit_resample(X, y)
            except ImportError:
                return X, y
        
        elif self.strategy == "class_weight":
            # No modifica datos, el estimador maneja los pesos
            if hasattr(self.estimator_, 'class_weight'):
                self.estimator_.set_params(class_weight='balanced')
            return X, y
        
        else:
            raise ValueError(f"Unknown strategy: {self.strategy}")

    def predict(self, X: np.ndarray) -> np.ndarray:
        """Predice clases."""
        check_is_fitted(self, ['estimator_', 'classes_'])
        return self.estimator_.predict(X)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """Predice probabilidades."""
        check_is_fitted(self, ['estimator_', 'classes_'])
        return self.estimator_.predict_proba(X)
```

### La Plantilla: Crea Tu Propio Transformer

```python
from sklearn.base import BaseEstimator, TransformerMixin

class MiTransformer(BaseEstimator, TransformerMixin):
    """
    Plantilla para crear transformers custom.
    
    REGLAS IMPORTANTES:
    1. __init__ solo guarda parÃ¡metros (no computa nada)
    2. fit() aprende de los datos (puede ser no-op)
    3. transform() aplica la transformaciÃ³n
    4. Nunca modificar input, siempre X.copy()
    """
    
    def __init__(self, param1: str = "default", param2: int = 10):
        # Solo guardar parÃ¡metros, NO computar nada
        self.param1 = param1
        self.param2 = param2
    
    def fit(self, X, y=None):
        """Aprende de los datos (opcional).
        
        Ejemplos de quÃ© aprender:
        - Media/std para normalizaciÃ³n
        - Vocabulario para encoding
        - Umbrales para binning
        """
        # Si el transformer es stateless, solo retorna self
        # Si aprende algo:
        # self.learned_param_ = compute_something(X)
        return self
    
    def transform(self, X):
        """Aplica la transformaciÃ³n."""
        X = X.copy()  # â† Siempre copiar
        # ... tu lÃ³gica de transformaciÃ³n ...
        return X
```

---

<a id="74-pipeline-completo-codigo-real"></a>

## 7.4 Pipeline Completo: CÃ³digo Real

### CarVision: El Pipeline de 3 Etapas

```python
# src/carvision/training.py - Pipeline REAL del portafolio

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor

from src.carvision.features import FeatureEngineer

def build_pipeline(cfg: dict) -> Pipeline:
    """Construye el pipeline completo de CarVision.
    
    Estructura: Features â†’ Preprocessing â†’ Model
    
    Esta arquitectura de 3 etapas garantiza:
    1. Feature engineering consistente (FeatureEngineer)
    2. Preprocesamiento apropiado por tipo de columna (ColumnTransformer)
    3. Modelo entrenado con datos correctamente transformados
    """
    # ParÃ¡metros de configuraciÃ³n
    num_cols = cfg["preprocessing"]["numeric_features"]
    cat_cols = cfg["preprocessing"]["categorical_features"]
    dataset_year = cfg.get("dataset_year", 2024)
    rf_params = cfg["training"].get("random_forest_params", {})
    
    # Etapa 1: Feature Engineering
    feature_engineer = FeatureEngineer(current_year=dataset_year)
    
    # Etapa 2: Preprocessing (despuÃ©s de feature engineering)
    # Nota: Las columnas aquÃ­ son las que EXISTEN despuÃ©s del FeatureEngineer
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', Pipeline([
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler())
            ]), num_cols),
            ('cat', Pipeline([
                ('imputer', SimpleImputer(strategy='most_frequent')),
                ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
            ]), cat_cols)
        ],
        remainder='drop'
    )
    
    # Etapa 3: Modelo
    model = RandomForestRegressor(**rf_params)
    
    # Pipeline completo: Una sola unidad entrenable/guardable
    pipeline = Pipeline([
        ('features', feature_engineer),    # Crea vehicle_age, brand
        ('pre', preprocessor),              # Escala y encoda
        ('model', model)                    # Predice
    ])
    
    return pipeline


# === USO ===
# Entrenamiento
pipeline = build_pipeline(config)
pipeline.fit(X_train, y_train)

# Guardar TODO junto
joblib.dump(pipeline, "artifacts/model.joblib")

# ProducciÃ³n
pipeline = joblib.load("artifacts/model.joblib")
price = pipeline.predict(X_new)  # Una llamada hace TODO
```

### BankChurn: Pipeline con Ensemble

```python
# src/bankchurn/training.py - Pipeline REAL del portafolio

def build_pipeline(self) -> Pipeline:
    """Construye el pipeline de BankChurn.
    
    Estructura:
    - Preprocessing: ColumnTransformer (num + cat)
    - Model: VotingClassifier o ResampleClassifier
    """
    # Columnas desde config
    num_cols = self.config.data.numerical_features
    cat_cols = self.config.data.categorical_features
    
    # Preprocessing
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', Pipeline([
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler())
            ]), num_cols),
            ('cat', Pipeline([
                ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
                ('encoder', OneHotEncoder(handle_unknown='ignore'))
            ]), cat_cols)
        ]
    )
    
    # Modelo: Ensemble o single model
    if self.config.model.type == "ensemble":
        model = VotingClassifier(
            estimators=[
                ('lr', LogisticRegression(
                    **self.config.model.logistic_regression.dict()
                )),
                ('rf', RandomForestClassifier(
                    **self.config.model.random_forest.dict()
                ))
            ],
            voting=self.config.model.ensemble.voting,
            weights=self.config.model.ensemble.weights
        )
    else:
        # Con wrapper de resampling
        model = ResampleClassifier(
            estimator=RandomForestClassifier(
                **self.config.model.random_forest.dict()
            ),
            strategy=self.config.model.resampling_strategy,
            random_state=self.random_state
        )
    
    # Pipeline final
    return Pipeline([
        ('preprocessor', preprocessor),
        ('model', model)
    ])
```

---

<a id="75-ejercicios-practicos"></a>

## 7.5 Ejercicios PrÃ¡cticos

### Ejercicio 1: Construir un ColumnTransformer

```python
# Datos de telecom:
# - calls: float (numÃ©rico)
# - minutes: float (numÃ©rico)
# - messages: int (numÃ©rico)
# - mb_used: float (numÃ©rico)
# - plan_type: str (categÃ³rico) - "basic", "premium"
# - region: str (categÃ³rico) - "north", "south", "east", "west"

# Tu tarea: Crea un ColumnTransformer que:
# 1. Escale las columnas numÃ©ricas con StandardScaler
# 2. Encode las columnas categÃ³ricas con OneHotEncoder
# 3. Maneje valores faltantes apropiadamente

num_cols = ["calls", "minutes", "messages", "mb_used"]
cat_cols = ["plan_type", "region"]

# Escribe tu cÃ³digo aquÃ­:
preprocessor = ColumnTransformer(
    # ...
)
```

<details>
<summary>ğŸ“ Ver SoluciÃ³n</summary>

```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

num_cols = ["calls", "minutes", "messages", "mb_used"]
cat_cols = ["plan_type", "region"]

preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), num_cols),
        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ]), cat_cols)
    ],
    remainder='drop'
)

# Verificar
print(f"Transformers: {[t[0] for t in preprocessor.transformers]}")
# Output: ['num', 'cat']
```

</details>

---

### Ejercicio 2: Crear un Custom Transformer

```python
# Tu tarea: Crea un transformer que calcule ratios de uso de telecom
# 
# Features a crear:
# - minutes_per_call = minutes / (calls + 1)
# - mb_per_message = mb_used / (messages + 1)
# - total_usage = calls + messages + (mb_used / 1000)
#
# Requisitos:
# - Debe heredar de BaseEstimator y TransformerMixin
# - fit() debe retornar self
# - transform() debe retornar DataFrame con nuevas columnas

from sklearn.base import BaseEstimator, TransformerMixin

class TelecomFeatureEngineer(BaseEstimator, TransformerMixin):
    # Tu cÃ³digo aquÃ­
    pass
```

<details>
<summary>ğŸ“ Ver SoluciÃ³n</summary>

```python
from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd


class TelecomFeatureEngineer(BaseEstimator, TransformerMixin):
    """Feature engineering para datos de telecom."""
    
    def __init__(self):
        pass
    
    def fit(self, X: pd.DataFrame, y=None):
        """No aprende nada (stateless)."""
        return self
    
    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """Crea features derivadas."""
        X = X.copy()
        
        # Minutos por llamada
        if "minutes" in X.columns and "calls" in X.columns:
            X["minutes_per_call"] = X["minutes"] / (X["calls"] + 1)
        
        # MB por mensaje
        if "mb_used" in X.columns and "messages" in X.columns:
            X["mb_per_message"] = X["mb_used"] / (X["messages"] + 1)
        
        # Uso total normalizado
        if all(col in X.columns for col in ["calls", "messages", "mb_used"]):
            X["total_usage"] = X["calls"] + X["messages"] + (X["mb_used"] / 1000)
        
        return X
    
    def get_feature_names_out(self, input_features=None):
        """Retorna nombres de features creadas."""
        return ["minutes_per_call", "mb_per_message", "total_usage"]


# Verificar
import pandas as pd

df = pd.DataFrame({
    "calls": [50, 100],
    "minutes": [200, 500],
    "messages": [100, 50],
    "mb_used": [5000, 10000]
})

fe = TelecomFeatureEngineer()
df_transformed = fe.fit_transform(df)
print(df_transformed.columns.tolist())
# Output incluye: minutes_per_call, mb_per_message, total_usage
```

</details>

---

### Ejercicio 3: Pipeline Completo para TelecomAI

```python
# Tu tarea: Construye un pipeline completo para TelecomAI
# 
# Estructura:
# 1. TelecomFeatureEngineer (del ejercicio anterior)
# 2. ColumnTransformer para preprocessing
# 3. LogisticRegression como modelo
#
# El pipeline debe ser guardable con joblib

# Tu cÃ³digo aquÃ­:
def build_telecom_pipeline():
    pass
```

<details>
<summary>ğŸ“ Ver SoluciÃ³n</summary>

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
import joblib


def build_telecom_pipeline(config: dict = None) -> Pipeline:
    """Construye pipeline completo para TelecomAI."""
    
    # ConfiguraciÃ³n por defecto
    if config is None:
        config = {
            "num_cols": ["calls", "minutes", "messages", "mb_used", 
                        "minutes_per_call", "mb_per_message", "total_usage"],
            "cat_cols": [],
            "random_state": 42
        }
    
    num_cols = config["num_cols"]
    cat_cols = config.get("cat_cols", [])
    
    # Etapa 1: Feature Engineering
    feature_engineer = TelecomFeatureEngineer()
    
    # Etapa 2: Preprocessing
    transformers = [
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), num_cols)
    ]
    
    if cat_cols:
        transformers.append(
            ('cat', Pipeline([
                ('imputer', SimpleImputer(strategy='most_frequent')),
                ('encoder', OneHotEncoder(handle_unknown='ignore'))
            ]), cat_cols)
        )
    
    preprocessor = ColumnTransformer(
        transformers=transformers,
        remainder='drop'
    )
    
    # Etapa 3: Modelo
    model = LogisticRegression(
        random_state=config.get("random_state", 42),
        max_iter=1000
    )
    
    # Pipeline completo
    pipeline = Pipeline([
        ('features', feature_engineer),
        ('preprocessor', preprocessor),
        ('model', model)
    ])
    
    return pipeline


# Uso
pipeline = build_telecom_pipeline()
# pipeline.fit(X_train, y_train)
# joblib.dump(pipeline, "artifacts/model.joblib")
```

</details>

---

<a id="errores-habituales"></a>

## ğŸ§¨ Errores habituales y cÃ³mo depurarlos en sklearn Pipelines

Los errores en este mÃ³dulo rara vez son â€œfallos exÃ³ticosâ€ del algoritmo; casi siempre son **desalineaciones** entre datos, columnas, transformers y cÃ³mo guardas/cargas el pipeline.

Si alguno de estos errores te tomÃ³ **>15 minutos**, regÃ­stralo en el **[Diario de Errores](study_tools/DIARIO_ERRORES.md)** y aplica el flujo de **rescate cognitivo** de **[Protocolo E](study_tools/PROTOCOLO_E.md)**.

### 1) `ValueError: number of features does not match` (mismatch entre train e inference)

**SÃ­ntomas tÃ­picos**

- En entrenamiento todo bien, pero al predecir obtienes:
  ```text
  ValueError: X has 15 features, but StandardScaler is expecting 12 features as input.
  ```
- O bien errores de Ã­ndice similares en `OneHotEncoder`.

**CÃ³mo identificarlo**

- Verifica que usas **el mismo pipeline serializado** en training e inference:
  - Â¿Guardas y cargas `pipeline.pkl`/`model.joblib`, o solo el modelo suelto?
- Comprueba que las columnas de entrada en producciÃ³n tienen el mismo orden y nombres que en entrenamiento.

**CÃ³mo corregirlo**

- En el portafolio, **siempre** serializa el pipeline completo:
  ```python
  joblib.dump(pipeline, "artifacts/model.joblib")
  pipeline = joblib.load("artifacts/model.joblib")
  ```
- AsegÃºrate de que el orden y nombres de columnas que construyes en la API/Streamlit coincidan con las listas `num_cols` y `cat_cols` del pipeline.

---

### 2) Data leakage por features que usan el target (especialmente en CarVision)

**SÃ­ntomas tÃ­picos**

- MÃ©tricas en training/validation son **sospechosamente altas**, pero en producciÃ³n caen.
- Features como `price_per_mile` o `price_category` dependen de la variable objetivo (`price`).

**CÃ³mo identificarlo**

- Examina tu `FeatureEngineer` y lista de columnas que entran al modelo:
  - Â¿EstÃ¡s incluyendo columnas derivadas del target en el `ColumnTransformer`?
- Revisa tu config (`cfg["preprocessing"]["numeric_features"]`, etc.) y confirma que solo incluyes features vÃ¡lidos.

**CÃ³mo corregirlo**

- AsegÃºrate de que features que dependen del target **no** se usen como input del modelo.
- En CarVision, por ejemplo, `price_per_mile` y `price_category` se calculan solo para anÃ¡lisis, pero se excluyen de `num_cols` para el pipeline.

---

### 3) Custom transformers que modifican el input in-place o no respetan la API sklearn

**SÃ­ntomas tÃ­picos**

- Errores del tipo:
  ```text
  TypeError: __init__() takes 1 positional argument but 2 were given
  ```
  o
  ```text
  AttributeError: 'MiTransformer' object has no attribute 'fit'
  ```
- Comportamientos raros donde un transformer â€œensuciaâ€ los datos para otros steps.

**CÃ³mo identificarlo**

- Revisa que tu transformer:
  - Herede de `BaseEstimator` y `TransformerMixin`.
  - Tenga `__init__`, `fit`, `transform` con las firmas estÃ¡ndar.
  - Use `X = X.copy()` dentro de `transform`.

**CÃ³mo corregirlo**

- Usa la plantilla de este mÃ³dulo (`MiTransformer`) como referencia.
- Evita lÃ³gica pesada en `__init__`; ahÃ­ solo se guardan parÃ¡metros.
- AÃ±ade tests unitarios simples (`fit_transform` sobre un `DataFrame` pequeÃ±o) para validar que mantiene columnas esperadas.

---

### 4) Pipelines diferentes en training y en la API

**SÃ­ntomas tÃ­picos**

- El pipeline usado en `training.py` no coincide con el que se monta en `fastapi_app.py` o `streamlit_app.py`.
- Bugs donde la API aplica transformaciones manuales **ademÃ¡s** del pipeline.

**CÃ³mo identificarlo**

- Busca en el proyecto si estÃ¡s construyendo pipelines duplicados:
  - En CarVision, la Ãºnica fuente de verdad debe ser `build_pipeline` en `src/carvision/training.py`.
  - La API y Streamlit solo deberÃ­an **cargar** el pipeline serializado, no recrearlo a mano.

**CÃ³mo corregirlo**

- Centraliza la construcciÃ³n del pipeline en una funciÃ³n (`build_pipeline` / `build_telecom_pipeline`).
- En la API/Streamlit, no replicar lÃ³gicas de preprocesado; limitarse a cargar y usar el pipeline.

---

### 5) PatrÃ³n general de debugging para pipelines

1. **Reproduce el error** con un input mÃ­nimo (1â€“2 filas de `DataFrame`).
2. **Inspecciona shapes y columnas** tras cada etapa:
   - Usa `pipeline.named_steps["pre"].transform(X_sample)` o similares.
3. **Verifica la serializaciÃ³n**: guarda, vuelve a cargar, y compara predicciones en un mismo batch.
4. **Conecta el problema** con el concepto del mÃ³dulo:
   - Training-serving skew â†’ pipeline parcial o mal serializado.
   - Mismatch de columnas â†’ listas `num_cols`/`cat_cols` desincronizadas.
   - Transformers rotos â†’ no respetan `fit`/`transform`.

Con este enfoque, los pipelines dejan de ser una â€œcaja negra mÃ¡gicaâ€ y se convierten en una lÃ­nea de ensamblaje transparente y depurable.

----

<a id="checkpoint"></a>

## âœ… Checkpoint: Â¿Completaste el MÃ³dulo?

### Checklist
- [ ] Sabes usar ColumnTransformer para diferentes tipos de columnas
- [ ] Puedes crear un Custom Transformer con fit/transform
- [ ] Has construido un pipeline de 3 etapas (features â†’ preprocessing â†’ model)
- [ ] Puedes guardar y cargar un pipeline completo con joblib

---

## ğŸ”— ADR: Decisiones de Arquitectura

### ADR-007: Pipeline Unificado Obligatorio

**Contexto**: Transformaciones separadas causan inconsistencias en producciÃ³n.

**DecisiÃ³n**: Todo el flujo (features â†’ preprocessing â†’ model) debe estar en un solo Pipeline.

**Consecuencias**:
- âœ… Una sola serializaciÃ³n guarda todo
- âœ… Imposible olvidar una transformaciÃ³n
- âœ… Reproducibilidad garantizada
- âŒ MÃ¡s complejo de debuggear (caja negra)
- âŒ Requiere entender sklearn profundamente

### ADR-008: Custom Transformers para Feature Engineering

**Contexto**: sklearn no tiene transformers para lÃ³gica de negocio especÃ­fica.

**DecisiÃ³n**: Crear FeatureEngineer como TransformerMixin.

**Consecuencias**:
- âœ… Reutilizable en train, API, y dashboard
- âœ… Testeable unitariamente
- âœ… DocumentaciÃ³n clara de features derivadas
- âŒ MÃ¡s cÃ³digo que escribir
- âŒ Requiere entender BaseEstimator/TransformerMixin

---

## ğŸ“¦ CÃ³mo se UsÃ³ en el Portafolio

Los pipelines sklearn son el corazÃ³n de los 3 proyectos del portafolio:

### Pipeline Unificado de BankChurn

```python
# BankChurn-Predictor/src/bankchurn/pipeline.py (estructura real)
def build_pipeline(config: BankChurnConfig) -> Pipeline:
    """Pipeline completo de 3 etapas."""
    return Pipeline([
        ('preprocessor', ColumnTransformer([
            ('num', Pipeline([
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler())
            ]), config.data.numerical_features),
            ('cat', Pipeline([
                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
                ('encoder', OneHotEncoder(handle_unknown='ignore'))
            ]), config.data.categorical_features)
        ])),
        ('model', get_model(config))
    ])
```

### FeatureEngineer de CarVision

```python
# CarVision-Market-Intelligence/src/carvision/features.py
class FeatureEngineer(BaseEstimator, TransformerMixin):
    """Custom transformer para features de autos."""
    
    def __init__(self, current_year: int = None):
        self.current_year = current_year
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X = X.copy()
        # vehicle_age, brand, mileage_category, etc.
        return X
```

### Archivos Clave por Proyecto

| Proyecto | Pipeline | Features | Artefacto |
|----------|----------|----------|-----------|
| BankChurn | `src/bankchurn/pipeline.py` | En preprocessor | `artifacts/pipeline.joblib` |
| CarVision | `src/carvision/pipeline.py` | `src/carvision/features.py` | `artifacts/pipeline.joblib` |
| TelecomAI | `src/telecomai/training.py` | En pipeline | `artifacts/model.joblib` |

### ğŸ”§ Ejercicio: Explora los Pipelines Reales

```bash
# 1. Ve a BankChurn y carga el pipeline
cd BankChurn-Predictor
python -c "
import joblib
pipe = joblib.load('artifacts/pipeline.joblib')
print('Steps:', [name for name, _ in pipe.steps])
print('Preprocessor:', pipe.named_steps['preprocessor'])
"

# 2. Inspecciona el FeatureEngineer de CarVision
cat CarVision-Market-Intelligence/src/carvision/features.py
```

---

## ğŸ’¼ Consejos Profesionales

> **Recomendaciones para destacar en entrevistas y proyectos reales**

### Para Entrevistas

1. **Â¿Por quÃ© Pipelines?**: Evitan data leakage, garantizan reproducibilidad, simplifican deployment.

2. **Custom Transformers**: Demuestra que puedes crear transformadores con `fit()` y `transform()`.

3. **ColumnTransformer**: Explica cÃ³mo aplicar diferentes transformaciones a diferentes columnas.

### Para Proyectos Reales

| SituaciÃ³n | Consejo |
|-----------|---------|
| Features nuevas | AÃ±ade transformadores al pipeline, no cÃ³digo suelto |
| Debugging | Usa `pipeline.named_steps` para inspeccionar etapas |
| ProducciÃ³n | Serializa el pipeline completo, no solo el modelo |
| Testing | Testea cada transformador individualmente |

### Patrones Avanzados

- **FeatureUnion**: Combinar features de diferentes fuentes
- **Pipeline dentro de Pipeline**: Para transformaciones complejas
- **make_pipeline**: Sintaxis simplificada sin nombres
- **clone**: Para cross-validation sin modificar original


---

## ğŸ“º Recursos Externos del MÃ³dulo

> ğŸ·ï¸ Sistema: ğŸ”´ Obligatorio | ğŸŸ¡ Recomendado | ğŸŸ¢ Complementario

### ğŸ¬ Videos

| ğŸ·ï¸ | TÃ­tulo | Canal | DuraciÃ³n | Link |
|:--:|:-------|:------|:--------:|:-----|
| ğŸ”´ | **Sklearn Pipeline Tutorial** | Data School | 28 min | [YouTube](https://www.youtube.com/watch?v=irHhDMbw3xo) |
| ğŸ”´ | **ColumnTransformer Explained** | Data School | 35 min | [YouTube](https://www.youtube.com/watch?v=NGq8wnH5VSo) |
| ğŸŸ¡ | **Custom Transformers in Sklearn** | PyData | 32 min | [YouTube](https://www.youtube.com/watch?v=BFaadIqWlAg) |
| ğŸŸ¢ | **Sklearn Pipeline Best Practices** | PyData Berlin | 45 min | [YouTube](https://www.youtube.com/watch?v=0UWXCAYn8rk) |

### ğŸ“š Cursos

| ğŸ·ï¸ | TÃ­tulo | Plataforma | DuraciÃ³n | Link |
|:--:|:-------|:-----------|:--------:|:-----|
| ğŸ”´ | ML Pipelines with scikit-learn | DataCamp | 4h | [DataCamp](https://www.datacamp.com/courses/machine-learning-with-scikit-learn) |
| ğŸŸ¡ | Feature Engineering for ML | Coursera (Google) | 5 weeks | [Coursera](https://www.coursera.org/learn/feature-engineering) |

### ğŸ“„ DocumentaciÃ³n

| ğŸ·ï¸ | Recurso | DescripciÃ³n |
|:--:|:--------|:------------|
| ğŸ”´ | [sklearn Pipeline User Guide](https://scikit-learn.org/stable/modules/compose.html) | GuÃ­a oficial de pipelines |
| ğŸŸ¡ | [Custom Transformers](https://scikit-learn.org/stable/developers/develop.html) | CÃ³mo crear transformers custom |

---

## âš–ï¸ DecisiÃ³n TÃ©cnica: ADR-002 scikit-learn

**Contexto**: Necesitamos un framework ML para clasificaciÃ³n/regresiÃ³n tabular.

**DecisiÃ³n**: Usar scikit-learn como framework principal.

**Alternativas Consideradas**:
- **XGBoost/LightGBM**: MÃ¡s performance, menos integraciÃ³n con pipelines
- **PyTorch**: Overkill para datos tabulares

**Consecuencias**:
- âœ… Pipelines unificados con `Pipeline` y `ColumnTransformer`
- âœ… FÃ¡cil de testear y serializar
- âœ… DocumentaciÃ³n excelente
- âŒ Menos performance que gradient boosting dedicado

---

## ğŸ”§ Ejercicios del MÃ³dulo

### Ejercicio 7.1: Pipeline BÃ¡sico
**Objetivo**: Crear un pipeline con preprocesamiento.
**Dificultad**: â­â­

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

# TU TAREA: Crear pipeline con:
# 1. StandardScaler para features numÃ©ricas
# 2. RandomForestClassifier

pipe = Pipeline([
    # TU CÃ“DIGO
])
```

<details>
<summary>ğŸ’¡ Ver soluciÃ³n</summary>

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(
        n_estimators=100,
        random_state=42
    ))
])

# Uso:
pipe.fit(X_train, y_train)
predictions = pipe.predict(X_test)

# SerializaciÃ³n:
import joblib
joblib.dump(pipe, 'artifacts/pipeline.joblib')
```
</details>

---

### Ejercicio 7.2: ColumnTransformer
**Objetivo**: Procesar columnas numÃ©ricas y categÃ³ricas por separado.
**Dificultad**: â­â­â­

```python
# Dado un DataFrame con:
# - numeric_cols = ['age', 'balance', 'salary']
# - categorical_cols = ['geography', 'gender']

# TU TAREA: Crear ColumnTransformer que:
# - Aplique StandardScaler a numÃ©ricas
# - Aplique OneHotEncoder a categÃ³ricas

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

preprocessor = ColumnTransformer([
    # TU CÃ“DIGO
])
```

<details>
<summary>ğŸ’¡ Ver soluciÃ³n</summary>

```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

numeric_cols = ['age', 'balance', 'salary']
categorical_cols = ['geography', 'gender']

# Pipelines individuales para cada tipo
numeric_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

# ColumnTransformer combina ambos
preprocessor = ColumnTransformer([
    ('num', numeric_transformer, numeric_cols),
    ('cat', categorical_transformer, categorical_cols)
])

# Pipeline completo con modelo
full_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])
```
</details>

---

### Ejercicio 7.3: Custom Transformer
**Objetivo**: Crear un transformer personalizado.
**Dificultad**: â­â­â­

```python
from sklearn.base import BaseEstimator, TransformerMixin

# TU TAREA: Crear AgeGroupTransformer que:
# - AÃ±ada columna 'age_group' basada en rangos de edad
# - 0-30: 'young', 31-50: 'middle', 51+: 'senior'

class AgeGroupTransformer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        # TU CÃ“DIGO
        return self
    
    def transform(self, X):
        # TU CÃ“DIGO
        pass
```

<details>
<summary>ğŸ’¡ Ver soluciÃ³n</summary>

```python
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin

class AgeGroupTransformer(BaseEstimator, TransformerMixin):
    """Transformer que aÃ±ade categorÃ­a de edad."""
    
    def __init__(self, age_column: str = 'age'):
        self.age_column = age_column
    
    def fit(self, X, y=None):
        # No hay nada que aprender
        return self
    
    def transform(self, X):
        X = X.copy()
        
        # Crear bins de edad
        bins = [0, 30, 50, np.inf]
        labels = ['young', 'middle', 'senior']
        
        X['age_group'] = pd.cut(
            X[self.age_column],
            bins=bins,
            labels=labels
        )
        return X
    
    def get_feature_names_out(self, input_features=None):
        """Para compatibilidad con sklearn >= 1.0"""
        return list(input_features) + ['age_group']


# Uso en pipeline:
pipeline = Pipeline([
    ('age_groups', AgeGroupTransformer(age_column='age')),
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier())
])
```
</details>

---

## ğŸ”— Glosario del MÃ³dulo

| TÃ©rmino | DefiniciÃ³n |
|---------|------------|
| **Pipeline** | Cadena de transformaciones + modelo que se serializa como unidad |
| **ColumnTransformer** | Aplica diferentes transformaciones a diferentes columnas en paralelo |
| **Data Leakage** | FiltraciÃ³n de informaciÃ³n del target al training, causando mÃ©tricas infladas |
| **fit_transform** | MÃ©todo que aprende parÃ¡metros y transforma en un solo paso |

---

<div align="center">

**Siguiente mÃ³dulo** â†’ [08. IngenierÃ­a de Features](08_INGENIERIA_FEATURES.md)

---

[â† Volver al Ãndice](00_INDICE.md)

</div>
