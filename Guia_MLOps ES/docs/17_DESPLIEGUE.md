# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MÃ“DULO 17: SERVERLESS VS CONTENEDORES
# CuÃ¡ndo Usar Lambda, ECS o Kubernetes
# GuÃ­a MLOps v5.0: Senior Edition | DuqueOM | Noviembre 2025
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

<div align="center">

# ğŸŒ MÃ“DULO 17: Serverless vs Contenedores

### La DecisiÃ³n que Define tu Arquitectura

*"No hay soluciÃ³n universal. Hay trade-offs que debes entender."*

| DuraciÃ³n             | TeorÃ­a               | PrÃ¡ctica             |
| :------------------: | :------------------: | :------------------: |
| **4-5 horas**        | 40%                  | 60%                  |

</div>

---

## ğŸ“‹ Contenido

- **0.0** [Prerrequisitos](#00-prerrequisitos)
- **0.1** [Protocolo E: CÃ³mo estudiar este mÃ³dulo](#01-protocolo-e-como-estudiar-este-modulo)
- **0.2** [Entregables verificables (mÃ­nimo viable)](#02-entregables-verificables-minimo-viable)
- **0.3** [Puente teorÃ­a â†” cÃ³digo (Portafolio)](#03-puente-teoria-codigo-portafolio)
- **17.1** [Matriz de DecisiÃ³n](#171-matriz-de-decision)
- **17.2** [OpciÃ³n 1: Serverless (AWS Lambda)](#172-opcion-1-serverless-aws-lambda)
- **17.3** [OpciÃ³n 2: Contenedores Managed](#173-opcion-2-contenedores-managed-aws-ecs-gcp-cloud-run)
- **17.4** [OpciÃ³n 3: Kubernetes](#174-opcion-3-kubernetes)
- **17.5** [AnÃ¡lisis de Costos (FinOps)](#175-analisis-de-costos-finops)
- **17.6** [DecisiÃ³n para BankChurn](#176-decision-para-bankchurn)
- **17.7** [ğŸ”¬ IngenierÃ­a Inversa: K8s Ingress Real](#177-ingenieria-inversa-k8s) â­ NUEVO
- [Errores habituales](#errores-habituales)
- [âœ… Ejercicio](#ejercicio)
- [âœ… Checkpoint](#checkpoint)

<a id="00-prerrequisitos"></a>

## 0.0 Prerrequisitos

- Haber completado el mÃ³dulo 13 (Docker) para entender imÃ¡genes, redes y puertos.
- Haber completado el mÃ³dulo 14 (FastAPI) y contar con un endpoint `/health`.
- Conocer los conceptos de latencia, throughput y costo (FinOps bÃ¡sico).

---

<a id="01-protocolo-e-como-estudiar-este-modulo"></a>

## 0.1 ğŸ§  Protocolo E: CÃ³mo estudiar este mÃ³dulo

- **Antes de elegir**: define tu escenario (trÃ¡fico, latencia, costo y equipo Ops).
- **Durante el estudio**: convierte la teorÃ­a en una decisiÃ³n explÃ­cita (ADR) y un deploy mÃ­nimo (Cloud Run/ECS o Lambda).
- **Si te atoras >15 min** con puertos, healthchecks, cold starts o tamaÃ±o de imagen, registra el caso en el **[Diario de Errores](study_tools/DIARIO_ERRORES.md)** y aplica el flujo de **[Protocolo E](study_tools/PROTOCOLO_E.md)**.

---

<a id="02-entregables-verificables-minimo-viable"></a>

## 0.2 âœ… Entregables verificables (mÃ­nimo viable)

- [ ] ADR (decisiÃ³n y trade-offs) para tu caso (por ejemplo: MVP en Cloud Run).
- [ ] Deploy funcional (Cloud Run/ECS o Lambda) con `/health` y un endpoint de predicciÃ³n.
- [ ] Healthcheck verificado en plataforma (readiness/liveness o equivalente).
- [ ] Plan de rollback (documentado y probado al menos una vez).

---

<a id="03-puente-teoria-codigo-portafolio"></a>

## 0.3 ğŸ§© Puente teorÃ­a â†” cÃ³digo (Portafolio)

- **TeorÃ­a**: serverless vs contenedores vs Kubernetes
- **PrÃ¡ctica**: Dockerfile + deploy en Cloud Run/ECS + runbooks
- **Prueba**: `curl /health` en el endpoint desplegado + revisiÃ³n de logs/healthchecks

---

<a id="171-matriz-de-decision"></a>

## 17.1 Matriz de DecisiÃ³n

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    MATRIZ DE DECISIÃ“N DE DESPLIEGUE                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘   Factor              â”‚ Lambda/Serverless â”‚ ECS/Cloud Run â”‚ Kubernetes        â•‘
â•‘   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•‘
â•‘   TrÃ¡fico             â”‚ < 1M req/mes      â”‚ 1M-100M       â”‚ > 100M            â•‘
â•‘   Latencia            â”‚ Variable (cold)   â”‚ Consistente   â”‚ Consistente       â•‘
â•‘   Costo bajo trÃ¡fico  â”‚ ğŸ’° Muy bajo       â”‚ ğŸ’°ğŸ’° Medio    â”‚ ğŸ’°ğŸ’°ğŸ’° Alto      â•‘
â•‘   Costo alto trÃ¡fico  â”‚ ğŸ’°ğŸ’°ğŸ’° Caro       â”‚ ğŸ’°ğŸ’° Medio    â”‚ ğŸ’° Barato        â•‘
â•‘   Complejidad Ops     â”‚ â­ Baja           â”‚ â­â­ Media   â”‚ â­â­â­â­ Alta  â•‘
â•‘   Equipo necesario    â”‚ 1 persona         â”‚ 2-3 personas  â”‚ 5+ personas       â•‘
â•‘   GPU Support         â”‚ âŒ                â”‚ âœ…           â”‚ âœ…               â•‘
â•‘   Max memoria         â”‚ 10GB              â”‚ 120GB+        â”‚ Ilimitado         â•‘
â•‘   Max timeout         â”‚ 15 min            â”‚ Ilimitado     â”‚ Ilimitado         â•‘
â•‘   Modelo size lÃ­mite  â”‚ ~250MB pkg        â”‚ Sin lÃ­mite    â”‚ Sin lÃ­mite        â•‘
â•‘   Auto-scaling        â”‚ AutomÃ¡tico        â”‚ AutomÃ¡tico    â”‚ Configurable      â•‘
â•‘   Vendor lock-in      â”‚ Alto              â”‚ Medio         â”‚ Bajo              â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<a id="172-opcion-1-serverless-aws-lambda"></a>

## 17.2 OpciÃ³n 1: Serverless (AWS Lambda)

### CuÃ¡ndo Usar

```
âœ… USA LAMBDA SI:
â€¢ TrÃ¡fico bajo o esporÃ¡dico (< 1M requests/mes)
â€¢ Modelo pequeÃ±o (< 250MB empaquetado)
â€¢ Latencia variable es aceptable
â€¢ No tienes equipo de DevOps
â€¢ Quieres minimizar costos en bajo trÃ¡fico

âŒ NO USES LAMBDA SI:
â€¢ Necesitas GPU
â€¢ Modelo > 250MB
â€¢ Cold starts son inaceptables (< 100ms requerido)
â€¢ TrÃ¡fico constante y alto
```

### Estructura para Lambda

```
lambda_function/
â”œâ”€â”€ handler.py          # Entry point
â”œâ”€â”€ model/
â”‚   â””â”€â”€ pipeline.pkl    # Modelo (< 250MB)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ inference.py    # LÃ³gica
â””â”€â”€ requirements.txt
```

### handler.py

```python
# handler.py - AWS Lambda Handler
import json                              # Parse/serialize JSON.
import joblib                            # Cargar modelo sklearn.
import pandas as pd                      # DataFrame para predicciÃ³n.
from pathlib import Path                 # Rutas multiplataforma.

# Cargar modelo al inicio (fuera del handler para reutilizar)
MODEL_PATH = Path(__file__).parent / "model" / "pipeline.pkl"  # Ruta relativa al handler.
model = joblib.load(MODEL_PATH)          # Se carga UNA vez (warm start reutiliza).

def lambda_handler(event, context):      # Punto de entrada de Lambda. context: metadata del runtime.
    """AWS Lambda handler."""
    try:
        # Parse input - Lambda puede recibir body como string o dict
        if isinstance(event.get("body"), str):  # API Gateway envÃ­a body como string.
            body = json.loads(event["body"])    # Deserializa JSON.
        else:
            body = event.get("body", event)     # InvocaciÃ³n directa: body es dict.
        
        # Crear DataFrame
        df = pd.DataFrame([body])               # Lista con 1 elemento â†’ 1 fila.
        
        # Predecir
        proba = model.predict_proba(df)[0, 1]   # [0,1]: fila 0, clase positiva.
        prediction = "churn" if proba >= 0.5 else "no_churn"  # Umbral 0.5.
        
        return {                                # Response format para API Gateway.
            "statusCode": 200,                  # HTTP 200 OK.
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps({                # body DEBE ser string JSON.
                "churn_probability": round(proba, 4),
                "prediction": prediction,
            })
        }
    except Exception as e:
        return {
            "statusCode": 500,                  # 500: Internal Server Error.
            "body": json.dumps({"error": str(e)})
        }
```

### serverless.yml (Serverless Framework)

```yaml
# serverless.yml
service: bankchurn-predictor             # Nombre del servicio (prefijo de recursos).

provider:
  name: aws                              # Cloud provider.
  runtime: python3.11                    # VersiÃ³n de Python.
  region: us-east-1                      # RegiÃ³n de AWS.
  memorySize: 1024                       # MB de RAM (mÃ¡s RAM = mÃ¡s CPU proporcional).
  timeout: 30                            # Timeout en segundos (mÃ¡x 15 min).

functions:
  predict:                               # Nombre de la funciÃ³n Lambda.
    handler: handler.lambda_handler      # mÃ³dulo.funciÃ³n a ejecutar.
    events:                              # Triggers que invocan la funciÃ³n.
      - http:                            # API Gateway HTTP trigger.
          path: predict                  # Ruta: /predict
          method: post                   # MÃ©todo HTTP.
          cors: true                     # Habilita CORS automÃ¡ticamente.

plugins:
  - serverless-python-requirements      # Plugin para empaquetar deps de Python.

custom:
  pythonRequirements:
    dockerizePip: true                   # Compila deps en Docker (para binarios nativos).
    slim: true                           # Elimina archivos innecesarios (reduce tamaÃ±o).
```

---

<a id="173-opcion-2-contenedores-managed-aws-ecs-gcp-cloud-run"></a>

## 17.3 OpciÃ³n 2: Contenedores Managed (AWS ECS / GCP Cloud Run)

### CuÃ¡ndo Usar

```
âœ… USA ECS/CLOUD RUN SI:
â€¢ TrÃ¡fico medio-alto (1M-100M requests/mes)
â€¢ Necesitas latencia consistente
â€¢ Modelo de cualquier tamaÃ±o
â€¢ Quieres balance entre control y simplicidad
â€¢ Equipo pequeÃ±o de DevOps (2-3 personas)

âŒ NO USES SI:
â€¢ Necesitas control granular de networking
â€¢ Multi-cloud es requisito
â€¢ TrÃ¡fico extremadamente alto (> 100M)
```

### AWS ECS Task Definition

```json
{
  "family": "bankchurn-api",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "512",
  "memory": "1024",
  "containerDefinitions": [
    {
      "name": "api",
      "image": "123456789.dkr.ecr.us-east-1.amazonaws.com/bankchurn:latest",
      "portMappings": [
        {
          "containerPort": 8000,
          "protocol": "tcp"
        }
      ],
      "environment": [
        {"name": "LOG_LEVEL", "value": "INFO"}
      ],
      "healthCheck": {
        "command": ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"],
        "interval": 30,
        "timeout": 5,
        "retries": 3
      },
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/bankchurn",
          "awslogs-region": "us-east-1",
          "awslogs-stream-prefix": "api"
        }
      }
    }
  ]
}
```

### GCP Cloud Run (mÃ¡s simple)

```bash
# Deploy a Cloud Run
gcloud run deploy bankchurn-api \
  --image gcr.io/my-project/bankchurn:latest \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --memory 1Gi \
  --cpu 1 \
  --min-instances 0 \
  --max-instances 10 \
  --port 8000
```

---

<a id="174-opcion-3-kubernetes"></a>

## 17.4 OpciÃ³n 3: Kubernetes

### CuÃ¡ndo Usar

```
âœ… USA KUBERNETES SI:
â€¢ TrÃ¡fico muy alto (> 100M requests/mes)
â€¢ MÃºltiples servicios ML que escalan diferente
â€¢ Necesitas GPU para inferencia
â€¢ Multi-cloud o hybrid cloud
â€¢ Equipo de Ops experimentado (5+ personas)
â€¢ Ya tienes inversiÃ³n en K8s

âŒ NO USES SI:
â€¢ Un solo modelo simple
â€¢ Equipo pequeÃ±o sin experiencia K8s
â€¢ Presupuesto limitado para Ops
```

### Manifiestos BÃ¡sicos

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bankchurn-api
  labels:
    app: bankchurn-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bankchurn-api
  template:
    metadata:
      labels:
        app: bankchurn-api
    spec:
      containers:
      - name: api
        image: ghcr.io/username/bankchurn:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 15
          periodSeconds: 10
        env:
        - name: LOG_LEVEL
          value: "INFO"
---
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: bankchurn-api
spec:
  selector:
    app: bankchurn-api
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP
---
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: bankchurn-api
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: bankchurn-api
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

---

<a id="175-analisis-de-costos-finops"></a>

## 17.5 AnÃ¡lisis de Costos (FinOps)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ANÃLISIS DE COSTOS MENSUAL                                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘   ESCENARIO: 1M requests/mes, ~1 req/seg promedio                             â•‘
â•‘                                                                               â•‘
â•‘   AWS Lambda:                                                                 â•‘
â•‘   â€¢ 1M requests Ã— $0.20/1M = $0.20                                            â•‘
â•‘   â€¢ 1M Ã— 200ms Ã— 1GB = 200K GB-s Ã— $0.0000166 = $3.32                         â•‘
â•‘   â€¢ Total: ~$4/mes âœ… (bajo trÃ¡fico es barato)                                â•‘
â•‘                                                                               â•‘
â•‘   ECS Fargate:                                                                â•‘
â•‘   â€¢ 0.5 vCPU Ã— 730h Ã— $0.04 = $14.60                                          â•‘
â•‘   â€¢ 1GB RAM Ã— 730h Ã— $0.004 = $2.92                                           â•‘
â•‘   â€¢ Total: ~$18/mes (consistente)                                             â•‘
â•‘                                                                               â•‘
â•‘   EKS (3 nodos t3.small):                                                     â•‘
â•‘   â€¢ 3 Ã— $15/mes (EC2) = $45                                                   â•‘
â•‘   â€¢ EKS fee: $72/mes                                                          â•‘
â•‘   â€¢ Total: ~$120/mes (overkill para este volumen)                             â•‘
â•‘                                                                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘   ESCENARIO: 100M requests/mes, ~40 req/seg promedio                          â•‘
â•‘                                                                               â•‘
â•‘   AWS Lambda:                                                                 â•‘
â•‘   â€¢ 100M Ã— $0.20/1M = $20                                                     â•‘
â•‘   â€¢ 100M Ã— 200ms Ã— 1GB = 20M GB-s Ã— $0.0000166 = $332                         â•‘
â•‘   â€¢ Total: ~$350/mes (ya no tan barato)                                       â•‘
â•‘                                                                               â•‘
â•‘   ECS Fargate (auto-scaling):                                                 â•‘
â•‘   â€¢ ~5 tareas promedio                                                        â•‘
â•‘   â€¢ Total: ~$90/mes âœ…                                                        â•‘
â•‘                                                                               â•‘
â•‘   EKS (auto-scaling):                                                         â•‘
â•‘   â€¢ 5 nodos t3.medium promedio                                                â•‘
â•‘   â€¢ Total: ~$200/mes                                                          â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<a id="176-decision-para-bankchurn"></a>

## 17.6 DecisiÃ³n para BankChurn

### RecomendaciÃ³n por Fase

| Fase | Plataforma | RazÃ³n |
| :--- | :--------- | :---- |
| **MVP/Desarrollo** | Cloud Run o Lambda | Simplicidad, bajo costo inicial |
| **ProducciÃ³n inicial** | ECS/Cloud Run | Balance costo-control |
| **Escala enterprise** | Kubernetes | Control total, multi-service |

### ADR para BankChurn

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ADR-009: Despliegue de BankChurn en Cloud Run                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘  DECISIÃ“N: Usar Google Cloud Run para el MVP                                  â•‘
â•‘                                                                               â•‘
â•‘  RAZONES:                                                                     â•‘
â•‘  â€¢ Escala a cero cuando no hay trÃ¡fico (costo mÃ­nimo)                         â•‘
â•‘  â€¢ Sin gestiÃ³n de infraestructura                                             â•‘
â•‘  â€¢ Latencia consistente (mejor que Lambda para ML)                            â•‘
â•‘  â€¢ Soporta contenedores Docker estÃ¡ndar                                       â•‘
â•‘  â€¢ FÃ¡cil migraciÃ³n a GKE si necesario                                         â•‘
â•‘                                                                               â•‘
â•‘  TRADE-OFFS ACEPTADOS:                                                        â•‘
â•‘  â€¢ Vendor lock-in medio (GCP)                                                 â•‘
â•‘  â€¢ Menos control que K8s                                                      â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<a id="177-ingenieria-inversa-k8s"></a>

## 17.7 ğŸ”¬ IngenierÃ­a Inversa PedagÃ³gica: Kubernetes Ingress Real

> **Objetivo**: Entender CADA decisiÃ³n detrÃ¡s del Ingress del portafolio que expone los 3 proyectos ML.

### 17.7.1 ğŸ¯ El "Por QuÃ©" ArquitectÃ³nico

Â¿Por quÃ© el portafolio usa Ingress con subdominios en lugar de un solo LoadBalancer por servicio?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISIONES ARQUITECTÃ“NICAS DEL PORTAFOLIO                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  PROBLEMA 1: Â¿CÃ³mo expongo 3 APIs ML al internet sin 3 LoadBalancers?           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  RIESGO: $15-20/mes por LoadBalancer Ã— 3 = $45-60/mes solo en networking        â”‚
â”‚  DECISIÃ“N: Un solo Ingress con routing por host/path                            â”‚
â”‚  RESULTADO: Un LoadBalancer, 3 servicios accesibles, ~$15/mes                   â”‚
â”‚  REFERENCIA: ingress.yaml spec.rules (lÃ­neas 24-78)                             â”‚
â”‚                                                                                 â”‚
â”‚  PROBLEMA 2: Â¿CÃ³mo protejo las APIs con HTTPS sin gestionar certificados?       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  RIESGO: HTTP en producciÃ³n = credenciales expuestas, penalizaciÃ³n SEO          â”‚
â”‚  DECISIÃ“N: cert-manager + Let's Encrypt (renovaciÃ³n automÃ¡tica)                 â”‚
â”‚  RESULTADO: TLS gratis, automÃ¡tico, sin intervenciÃ³n manual                     â”‚
â”‚  REFERENCIA: ingress.yaml annotations cert-manager.io (lÃ­nea 8)                 â”‚
â”‚                                                                                 â”‚
â”‚  PROBLEMA 3: Â¿CÃ³mo evito que un atacante sature las APIs con requests?          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  RIESGO: DDoS, costos inflados, degradaciÃ³n para usuarios legÃ­timos             â”‚
â”‚  DECISIÃ“N: Rate limiting vÃ­a annotations nginx (100 req/s, 10 rps por IP)       â”‚
â”‚  RESULTADO: ProtecciÃ³n bÃ¡sica sin WAF externo                                   â”‚
â”‚  REFERENCIA: ingress.yaml annotations rate-limit (lÃ­neas 10-11)                 â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 17.7.2 ğŸ” AnatomÃ­a de `ingress.yaml`

**Archivo**: `ML-MLOps-Portfolio/k8s/ingress.yaml`

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ml-portfolio-ingress
  namespace: ml-portfolio               # Todos los recursos en un namespace.
  annotations:
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BLOQUE 1: ConfiguraciÃ³n del Ingress Controller
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    kubernetes.io/ingress.class: nginx  # Usa NGINX Ingress Controller.
    # Â¿Por quÃ© nginx? Es el estÃ¡ndar, bien documentado, muchas features.
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BLOQUE 2: TLS AutomÃ¡tico con Let's Encrypt
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    cert-manager.io/cluster-issuer: letsencrypt-prod
    # Â¿CÃ³mo funciona?
    # 1. cert-manager detecta esta annotation.
    # 2. Solicita certificado a Let's Encrypt vÃ­a ACME challenge.
    # 3. Almacena el certificado en el Secret indicado en spec.tls.
    # 4. Renueva automÃ¡ticamente antes de expirar (cada 90 dÃ­as).
    
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    # Fuerza HTTPS: cualquier request HTTP â†’ 301 a HTTPS.
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BLOQUE 3: Rate Limiting (ProtecciÃ³n DDoS bÃ¡sica)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    nginx.ingress.kubernetes.io/rate-limit: "100"       # 100 req/s globales.
    nginx.ingress.kubernetes.io/limit-rps: "10"         # 10 req/s por IP.
    # Â¿Por quÃ© 10 rps por IP?
    # - Un usuario legÃ­timo no hace 10 predicciones por segundo.
    # - Un scraper/bot sÃ­, y esto lo bloquea.
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BLOQUE 4: Timeouts para ML (inferencia puede ser lenta)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"  # Max body 10MB (imÃ¡genes).
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "60"
    # Â¿Por quÃ© 60s?
    # - Inferencia de modelos grandes (CarVision con imÃ¡genes) puede tardar.
    # - Default de NGINX es 60s, pero lo hacemos explÃ­cito.

spec:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # BLOQUE 5: Certificados TLS
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  tls:
  - hosts:
    - ml.duqueom.com                    # Dominio principal.
    - bankchurn.ml.duqueom.com          # Subdominio por proyecto.
    - telecom.ml.duqueom.com
    - carvision.ml.duqueom.com
    secretName: ml-portfolio-tls        # Donde cert-manager guarda el cert.
  # Â¿Por quÃ© un solo Secret para 4 dominios?
  # - Let's Encrypt soporta certificados multi-dominio (SAN).
  # - Un cert = menos gestiÃ³n que 4 certs separados.
  
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # BLOQUE 6: Routing por Subdominio (PatrÃ³n preferido)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  rules:
  - host: bankchurn.ml.duqueom.com      # Subdominio dedicado.
    http:
      paths:
      - path: /                          # Todo el trÃ¡fico va al servicio.
        pathType: Prefix
        backend:
          service:
            name: bankchurn-service
            port:
              number: 80
  # Â¿Por quÃ© subdominios vs paths?
  # - Aislamiento: cada proyecto tiene su propio "namespace" de URLs.
  # - Cookies: no se mezclan entre servicios.
  # - Escalado: puedes mover un subdominio a otro cluster sin afectar otros.
  
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # BLOQUE 7: Routing por Path (Alternativa para API Gateway)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  - host: ml.duqueom.com
    http:
      paths:
      - path: /bankchurn                 # /bankchurn/* â†’ bankchurn-service
        pathType: Prefix
        backend:
          service:
            name: bankchurn-service
            port:
              number: 80
      - path: /telecom                   # /telecom/* â†’ telecom-service
        pathType: Prefix
        backend:
          service:
            name: telecom-service
            port:
              number: 80
  # Â¿CuÃ¡ndo usar paths?
  # - Cuando necesitas un "API Gateway" con un solo dominio.
  # - Para frontends que consumen mÃºltiples APIs.
```

### 17.7.3 ğŸ§ª Laboratorio de ReplicaciÃ³n

**Tu misiÃ³n**: Crear un Ingress para tu proyecto BankChurn.

1. **Instala NGINX Ingress Controller** (si no lo tienes):
   ```bash
   kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml
   ```

2. **Instala cert-manager** para TLS automÃ¡tico:
   ```bash
   kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.2/cert-manager.yaml
   ```

3. **Crea tu ClusterIssuer**:
   ```yaml
   # clusterissuer.yaml
   apiVersion: cert-manager.io/v1
   kind: ClusterIssuer
   metadata:
     name: letsencrypt-prod
   spec:
     acme:
       server: https://acme-v02.api.letsencrypt.org/directory
       email: tu-email@example.com
       privateKeySecretRef:
         name: letsencrypt-prod
       solvers:
       - http01:
           ingress:
             class: nginx
   ```

4. **Crea tu Ingress bÃ¡sico**:
   ```yaml
   # mi-ingress.yaml
   apiVersion: networking.k8s.io/v1
   kind: Ingress
   metadata:
     name: bankchurn-ingress
     annotations:
       cert-manager.io/cluster-issuer: letsencrypt-prod
       nginx.ingress.kubernetes.io/ssl-redirect: "true"
   spec:
     tls:
     - hosts:
       - tu-dominio.com
       secretName: bankchurn-tls
     rules:
     - host: tu-dominio.com
       http:
         paths:
         - path: /
           pathType: Prefix
           backend:
             service:
               name: bankchurn-service
               port:
                 number: 80
   ```

5. **Verifica**:
   ```bash
   kubectl apply -f mi-ingress.yaml
   kubectl get certificate  # Espera a que estÃ© "Ready"
   curl https://tu-dominio.com/health
   ```

### 17.7.4 ğŸš¨ Troubleshooting Preventivo

| SÃ­ntoma | Causa Probable | SoluciÃ³n |
|---------|----------------|----------|
| **404 en el Ingress** | Servicio no existe o puerto incorrecto | `kubectl get svc` y verifica nombre/puerto. |
| **502 Bad Gateway** | Pods no estÃ¡n ready o healthcheck falla | `kubectl get pods` y revisa logs del pod. |
| **Certificate no se genera** | DNS no apunta al Ingress IP o ClusterIssuer mal | `kubectl describe certificate` para ver eventos. |
| **HTTP funciona pero HTTPS no** | Secret TLS no existe o estÃ¡ vacÃ­o | `kubectl get secret bankchurn-tls -o yaml`. |
| **Rate limit bloquea usuarios legÃ­timos** | Umbral muy bajo | Incrementa `limit-rps` o usa whitelist por IP. |

---

<a id="errores-habituales"></a>

## ğŸ§¨ Errores habituales y cÃ³mo depurarlos en despliegue ML

En despliegue ML es muy fÃ¡cil elegir mal la plataforma o romper detalles como puertos, healthchecks o tamaÃ±os de imagen.

Si alguno de estos errores te tomÃ³ **>15 minutos**, regÃ­stralo en el **[Diario de Errores](study_tools/DIARIO_ERRORES.md)** y aplica el flujo de **rescate cognitivo** de **[Protocolo E](study_tools/PROTOCOLO_E.md)**.

### 1) Elegir la plataforma equivocada (costos o latencia inesperados)

**SÃ­ntomas tÃ­picos**

- Con Lambda: facturas altas al subir el trÃ¡fico o latencias variables por cold starts.
- Con K8s: infraestructura sobredimensionada para un solo modelo simple.

**CÃ³mo identificarlo**

- Compara tu caso con la **matriz de decisiÃ³n** del mÃ³dulo (trÃ¡fico, latencia, equipo Ops, presupuesto).

**CÃ³mo corregirlo**

- Para MVPs y trÃ¡fico moderado, prefiere **Cloud Run/ECS** en lugar de K8s.
- Reserva K8s para escenarios enterprise con mÃºltiples servicios y trÃ¡fico muy alto.

---

### 2) Lambdas que no despliegan o fallan al importar el modelo

**SÃ­ntomas tÃ­picos**

- Errores como `Unable to import module 'handler'`.
- Deployment fallido por paquete demasiado grande (> 250MB).

**CÃ³mo identificarlo**

- Revisa el tamaÃ±o del zip y la estructura de `lambda_function/`.

**CÃ³mo corregirlo**

- Empaqueta solo lo necesario (`model/`, `src/`, `handler.py`, `requirements.txt`).
- Usa capas o reduce dependencias pesadas si es posible.

---

### 3) Contenedores que arrancan pero nunca pasan el healthcheck

**SÃ­ntomas tÃ­picos**

- En ECS/Cloud Run/K8s el servicio queda en estado `UNHEALTHY` o se reinicia en bucle.

**CÃ³mo identificarlo**

- Compara el `healthCheck`/`readinessProbe` con los endpoints reales (`/health`, puerto 8000). 

**CÃ³mo corregirlo**

- Asegura que tu API expone exactamente el endpoint y puerto que la plataforma espera.
- Ajusta `initialDelaySeconds`/`timeout` si el modelo tarda en cargar.

---

### 4) Puertos y rutas inconsistentes entre Docker y la plataforma

**SÃ­ntomas tÃ­picos**

- Funciona en `docker run -p 8000:8000` pero falla al desplegar en Cloud Run/ECS.

**CÃ³mo identificarlo**

- Verifica que el `EXPOSE` del Dockerfile, el puerto del servidor (uvicorn) y el puerto configurado en la plataforma coincidan.

**CÃ³mo corregirlo**

- Usa un puerto estÃ¡ndar (8000) y mantÃ©n el mismo valor en Dockerfile y manifiestos.

---

### 5) PatrÃ³n general de debugging en despliegue ML

1. Verifica primero que la imagen Docker funciona **en local** (`docker run` + `curl /health`).
2. Revisa logs de la plataforma (Lambda logs, Cloud Run logs, ECS/K8s events) para ver errores reales.
3. Comprueba healthchecks, puertos y variables de entorno.
4. Ajusta la plataforma elegida si tus patrones de trÃ¡fico o equipo no encajan con la decisiÃ³n inicial.

Con esta disciplina, pasar de local a producciÃ³n se vuelve un proceso repetible y menos doloroso.

---

## 17.6.1 Kubernetes Ingress con TLS y Rate Limiting

> **Referencia del portafolio**: `k8s/ingress.yaml`

### Ingress con cert-manager (TLS automÃ¡tico)

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ml-portfolio-ingress
  annotations:
    # TLS con cert-manager
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    # Rate limiting con nginx-ingress
    nginx.ingress.kubernetes.io/limit-rps: "10"
    nginx.ingress.kubernetes.io/limit-connections: "5"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - ml-api.example.com
    secretName: ml-api-tls
  rules:
  - host: ml-api.example.com
    http:
      paths:
      - path: /bankchurn
        pathType: Prefix
        backend:
          service:
            name: bankchurn-service
            port:
              number: 80
      - path: /carvision
        pathType: Prefix
        backend:
          service:
            name: carvision-service
            port:
              number: 80
```

### ClusterIssuer para Let's Encrypt

```yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: tu-email@example.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx
```

### VerificaciÃ³n

```bash
# Verificar ingress
kubectl get ingress ml-portfolio-ingress

# Verificar certificado TLS
kubectl get certificate ml-api-tls

# Test con curl
curl -v https://ml-api.example.com/bankchurn/health
```

---

<a id="ejercicio"></a>

## 17.7 Ejercicio: Deploy a Cloud Run

```bash
# 1. Build imagen
docker build -t gcr.io/my-project/bankchurn:v1 .

# 2. Push a GCR
docker push gcr.io/my-project/bankchurn:v1

# 3. Deploy
gcloud run deploy bankchurn \
  --image gcr.io/my-project/bankchurn:v1 \
  --platform managed \
  --region us-central1 \
  --memory 1Gi \
  --allow-unauthenticated

# 4. Test
curl -X POST https://bankchurn-xxx.run.app/api/v1/predict \
  -H "Content-Type: application/json" \
  -d '{"credit_score": 650, "age": 35, ...}'
```

---

## âœ… Ejercicios

Ver [EJERCICIOS.md](EJERCICIOS.md) - MÃ³dulo 17:
- **17.1**: Dockerfile multi-stage
- **17.2**: Docker Compose para stack ML

---

<a id="checkpoint"></a>

## âœ… Checkpoint

- [ ] Puedes explicar (en 60s) por quÃ© tu caso usa Lambda vs Cloud Run/ECS vs Kubernetes.
- [ ] Tu servicio desplegado responde `/health` en la plataforma elegida.
- [ ] El healthcheck/readiness/liveness estÃ¡ configurado y pasa en producciÃ³n.
- [ ] Tienes un plan de rollback (y sabes ejecutarlo).
- [ ] Registraste en runbook quÃ© hacer ante latencia alta y error rate alto.

---

## ğŸ”œ Siguiente Paso

Con la plataforma elegida, es hora de gestionar **infraestructura como cÃ³digo**.

**[Ir a MÃ³dulo 18: Infraestructura como CÃ³digo â†’](18_INFRAESTRUCTURA.md)**

---

## ğŸ“º Recursos Externos del MÃ³dulo

> ğŸ·ï¸ Sistema: ğŸ”´ Obligatorio | ğŸŸ¡ Recomendado | ğŸŸ¢ Complementario

### ğŸ¬ Videos

| ğŸ·ï¸ | TÃ­tulo | Canal | DuraciÃ³n | Link |
|:--:|:-------|:------|:--------:|:-----|
| ğŸ”´ | **Cloud Run Tutorial** | Google Cloud | 25 min | [YouTube](https://www.youtube.com/watch?v=3OP-q55hOUI) |
| ğŸŸ¡ | **AWS Lambda for ML** | AWS | 30 min | [YouTube](https://www.youtube.com/watch?v=eOBq__h4OJ4) |
| ğŸŸ¢ | **Blue-Green Deployments** | DevOps Toolkit | 20 min | [YouTube](https://www.youtube.com/watch?v=gfQRuL8Gj_A) |

### ğŸ“„ DocumentaciÃ³n

| ğŸ·ï¸ | Recurso | DescripciÃ³n |
|:--:|:--------|:------------|
| ğŸ”´ | [Cloud Run Docs](https://cloud.google.com/run/docs) | GuÃ­a oficial GCP |
| ğŸŸ¡ | [AWS Lambda](https://docs.aws.amazon.com/lambda/) | Serverless AWS |

---

## âš–ï¸ DecisiÃ³n TÃ©cnica: ADR-007 Plataforma de Deployment

**Contexto**: Necesitamos elegir dÃ³nde desplegar APIs ML.

**DecisiÃ³n**: Cloud Run para APIs de inferencia (default), K8s para casos complejos.

**Alternativas Consideradas**:
- **AWS Lambda**: Cold starts problemÃ¡ticos para ML
- **EC2/GCE**: MÃ¡s control pero mÃ¡s gestiÃ³n
- **Kubernetes**: MÃ¡s complejo pero mÃ¡s flexible

**Consecuencias**:
- âœ… Escalado automÃ¡tico (0 a N)
- âœ… Pay-per-use, sin servidores ociosos
- âœ… CI/CD simple con Cloud Build
- âŒ Cold starts (mitigable con min-instances)

---

## ğŸ”§ Ejercicios del MÃ³dulo

### Ejercicio 17.1: AnÃ¡lisis de Costos
**Objetivo**: Comparar costos entre plataformas.
**Dificultad**: â­â­

```
Escenario:
- API con 10,000 requests/dÃ­a
- Latencia promedio 200ms
- Imagen Docker 500MB
- 1GB RAM por instancia

TU TAREA: Calcular costo mensual aproximado en:
- Cloud Run
- AWS Lambda
- EC2 t3.small
```

<details>
<summary>ğŸ’¡ Ver soluciÃ³n</summary>

```
CLOUD RUN (GCP):
- Requests: 10,000/dÃ­a Ã— 30 = 300,000/mes
- CPU: 300,000 Ã— 0.2s = 60,000 CPU-seconds = 16.7 CPU-hours
- Memory: 16.7 hours Ã— 1GB = 16.7 GB-hours
- Costo: ~$5-10/mes (con free tier)

AWS LAMBDA:
- Requests: 300,000/mes (1M free)
- Duration: 300,000 Ã— 200ms = 60,000 GB-seconds
- Costo: ~$1-5/mes (pero cold starts!)

EC2 t3.small (always on):
- $0.0208/hour Ã— 720h = ~$15/mes
- + Load Balancer: ~$20/mes
- Total: ~$35/mes

RECOMENDACIÃ“N:
- < 100K req/mes: Cloud Run (escala a 0)
- 100K-1M req/mes: Cloud Run con min-instances
- > 1M req/mes: Kubernetes o EC2 dedicado
```
</details>

---

## ğŸ”— Glosario del MÃ³dulo

| TÃ©rmino | DefiniciÃ³n |
|---------|------------|
| **Blue-Green** | Estrategia de deployment con dos ambientes idÃ©nticos |
| **Canary** | Despliegue gradual a un % de trÃ¡fico |
| **Cold Start** | Tiempo de inicio cuando no hay instancias activas |
| **Serverless** | Modelo donde el proveedor gestiona la infraestructura |

---

<div align="center">

**Siguiente mÃ³dulo** â†’ [18. Infraestructura](18_INFRAESTRUCTURA.md)

---

[â† Volver al Ãndice](00_INDICE.md)

</div>
