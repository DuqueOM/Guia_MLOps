# 20. Proyecto Integrador
 
 <a id="00-prerrequisitos"></a>
 
 ## 0.0 Prerrequisitos
 
 - Haber completado los mÃ³dulos 01â€“19 (en particular: Testing, CI/CD, APIs, Observabilidad y DocumentaciÃ³n).
 - Tener listo un repositorio â€œvacÃ­o pero bien estructuradoâ€ (o estar dispuesto a crearlo primero) antes de entrenar cualquier modelo.
 - Aceptar el enfoque de este mÃ³dulo: *integraciÃ³n por capas* (estructura â†’ pipeline â†’ tests â†’ API â†’ Docker â†’ CI/CD â†’ docs).
 
 ---
 
 <a id="01-protocolo-e-como-estudiar-este-modulo"></a>
 
 ## 0.1 ğŸ§  Protocolo E: CÃ³mo estudiar este mÃ³dulo
 
 - **Antes de empezar**: define un â€œalcance seniorâ€ realista (quÃ© vas a construir y quÃ© NO).
 - **Durante**: trabaja con commits pequeÃ±os, y valida cada capa (instalaciÃ³n, tests, API) antes de pasar a la siguiente.
 - **Si te atoras >15 min** (tests rotos, CI fallando, configs duplicadas), regÃ­stralo en el **[Diario de Errores](study_tools/DIARIO_ERRORES.md)** y aplica el flujo de **[Protocolo E](study_tools/PROTOCOLO_E.md)**.
 
 ---
 
 <a id="02-entregables-verificables-minimo-viable"></a>
 
 ## 0.2 âœ… Entregables verificables (mÃ­nimo viable)
 
 - [ ] El repo instala con `pip install -e .` (sin pasos manuales ocultos).
 - [ ] `make test` pasa en local con coverage objetivo.
 - [ ] `make train` produce artefactos reproducibles (y el pipeline se puede re-ejecutar).
 - [ ] La API expone `/health` y `/predict` y tiene tests mÃ­nimos.
 - [ ] Hay documentaciÃ³n mÃ­nima (README + Model/Data card).
 
 ---
 
 <a id="03-puente-teoria-codigo-portafolio"></a>
 
 ## 0.3 ğŸ§© Puente teorÃ­a â†” cÃ³digo (Portafolio)
 
 - Este mÃ³dulo es tu â€œ**producto final**â€: demostrar que puedes ensamblar un sistema ML completo, no solo un modelo.
 - Reutiliza patrones del portafolio (estructura `src/`, config, tests, CI) pero justificando adaptaciones.
 - Tu objetivo es que un revisor pueda clonar tu repo, ejecutar 2â€“3 comandos y ver el sistema funcionando.
 
 ---
 
 ## ğŸ“‹ Contenido
 
 - **0.0** [Prerrequisitos](#00-prerrequisitos)
 - **0.1** [Protocolo E: CÃ³mo estudiar este mÃ³dulo](#01-protocolo-e-como-estudiar-este-modulo)
 - **0.2** [Entregables verificables (mÃ­nimo viable)](#02-entregables-verificables-minimo-viable)
 - **0.3** [Puente teorÃ­a â†” cÃ³digo (Portafolio)](#03-puente-teoria-codigo-portafolio)
 - **20.1** [Objetivo](#201-objetivo)
 - **20.2** [El Proyecto: Sistema de RecomendaciÃ³n de Planes](#202-el-proyecto-sistema-de-recomendacion-de-planes)
 - **20.3** [Checklist de Entrega (100 puntos)](#203-checklist-de-entrega-100-puntos)
 - **20.4** [Plantilla de README](#204-plantilla-de-readme)
 - **20.5** [RÃºbrica de EvaluaciÃ³n](#205-rubrica-de-evaluacion)
 - [Errores habituales](#errores-habituales)
 - **20.6** [Tips para Ã‰xito](#206-tips-para-exito)
 - **20.7** [Consejos Profesionales](#207-consejos-profesionales)
 - **20.8** [Recursos Externos Recomendados](#208-recursos-externos-recomendados)
 - **20.9** [Referencias del Glosario](#209-referencias-del-glosario)
- [âœ… Ejercicio](#ejercicio)
- **20.10** [Entrega](#2010-entrega)
- **23.11** [ğŸ”¬ IngenierÃ­a Inversa: Arquitectura Monorepo](#2011-monorepo) â­ NUEVO
- [âœ… Checkpoint](#checkpoint)
 
 ---
 
 <a id="201-objetivo"></a>
 
 ## ğŸ¯ Objetivo
 
 Construir un proyecto ML completo desde cero, aplicando TODO lo aprendido.
 
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘  ğŸ† EL RETO FINAL                                                            â•‘
â•‘                                                                              â•‘
â•‘  Has aprendido los conceptos. Has estudiado el cÃ³digo del portafolio.        â•‘
â•‘  Ahora es momento de DEMOSTRAR que puedes construirlo desde cero.            â•‘
â•‘                                                                              â•‘
â•‘  TIEMPO: 1-2 semanas                                                         â•‘
â•‘  RESULTADO: Un 4to proyecto digno del portafolio                             â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“‹ <a id="202-el-proyecto-sistema-de-recomendacion-de-planes"></a> El Proyecto: Sistema de RecomendaciÃ³n de Planes

**Contexto**: Una empresa de telecomunicaciones quiere recomendar planes mÃ³viles basÃ¡ndose en el comportamiento del usuario.

**Dataset sugerido**: [Telecom Users Dataset](https://www.kaggle.com/datasets) o similar.

---

## âœ… <a id="203-checklist-de-entrega-100-puntos"></a> Checklist de Entrega (100 puntos)

### Fase 1: Estructura y ConfiguraciÃ³n (20 puntos)

| Requisito | Puntos | Archivo |
|-----------|:------:|---------|
| Estructura src/ layout | 3 | `src/planrec/` |
| pyproject.toml completo | 3 | `pyproject.toml` |
| Makefile con comandos bÃ¡sicos | 2 | `Makefile` |
| Config Pydantic con validaciÃ³n | 4 | `src/planrec/config.py` |
| Config YAML externo | 2 | `configs/config.yaml` |
| .gitignore apropiado | 2 | `.gitignore` |
| README profesional | 4 | `README.md` |

### Fase 2: Pipeline ML (25 puntos)

| Requisito | Puntos | Archivo |
|-----------|:------:|---------|
| Carga y validaciÃ³n de datos | 3 | `src/planrec/data.py` |
| Feature Engineering como Transformer | 5 | `src/planrec/features.py` |
| sklearn Pipeline unificado | 5 | `src/planrec/training.py` |
| Cross-validation estratificada | 3 | `src/planrec/training.py` |
| MÃ©tricas apropiadas (F1, AUC) | 3 | `src/planrec/evaluation.py` |
| Guardado de artefactos | 3 | `artifacts/` |
| PrevenciÃ³n de data leakage | 3 | `drop_columns` en config |

### Fase 3: Testing (20 puntos)

| Requisito | Puntos | Archivo |
|-----------|:------:|---------|
| conftest.py con fixtures | 4 | `tests/conftest.py` |
| Tests unitarios (features) | 4 | `tests/test_features.py` |
| Tests de datos | 3 | `tests/test_data.py` |
| Tests de modelo | 3 | `tests/test_model.py` |
| Tests de integraciÃ³n | 3 | `tests/test_training.py` |
| Coverage â‰¥ 80% | 3 | `pytest --cov` |

### Fase 4: API y Serving (15 puntos)

| Requisito | Puntos | Archivo |
|-----------|:------:|---------|
| FastAPI con Pydantic schemas | 4 | `app/fastapi_app.py` |
| Endpoint /health | 2 | |
| Endpoint /predict | 4 | |
| Dockerfile multi-stage | 3 | `Dockerfile` |
| Non-root user | 2 | |

### Fase 5: CI/CD y Calidad (15 puntos)

| Requisito | Puntos | Archivo |
|-----------|:------:|---------|
| GitHub Actions workflow | 5 | `.github/workflows/ci.yml` |
| Tests automÃ¡ticos | 3 | |
| Coverage enforcement | 3 | |
| Linting (ruff/black) | 2 | |
| Pre-commit hooks | 2 | `.pre-commit-config.yaml` |

### Fase 6: DocumentaciÃ³n (5 puntos)

| Requisito | Puntos | Archivo |
|-----------|:------:|---------|
| Model Card | 3 | `docs/model_card.md` |
| Data Card | 2 | `docs/data_card.md` |

---

## ğŸ“ <a id="204-plantilla-de-readme"></a> Plantilla de README

```markdown
# ğŸ“± PlanRec: Mobile Plan Recommender

[![CI](https://github.com/USER/planrec/actions/workflows/ci.yml/badge.svg)](...)
[![Coverage](https://img.shields.io/badge/Coverage-85%25-brightgreen)](...)
[![Python](https://img.shields.io/badge/Python-3.11-blue)](...)

> Sistema de recomendaciÃ³n de planes mÃ³viles basado en comportamiento de usuarios.

## ğŸ¯ Resumen del Proyecto

| MÃ©trica | Valor |
|---------|-------|
| **Accuracy** | 85% |
| **F1-Score** | 0.82 |
| **Coverage** | 85% |

## ğŸš€ Quick Start

\`\`\`bash
# Instalar
pip install -e ".[dev]"

# Entrenar
make train

# Servir API
make serve

# Tests
make test
\`\`\`

## ğŸ“ Estructura

\`\`\`
planrec/
â”œâ”€â”€ src/planrec/       # CÃ³digo fuente
â”œâ”€â”€ app/               # FastAPI
â”œâ”€â”€ tests/             # Tests
â”œâ”€â”€ configs/           # ConfiguraciÃ³n
â””â”€â”€ artifacts/         # Modelos (gitignored)
\`\`\`

## ğŸ“Š Arquitectura

[Diagrama de arquitectura]

## ğŸ› ï¸ Stack TecnolÃ³gico

- **ML**: scikit-learn, pandas, numpy
- **API**: FastAPI, uvicorn
- **Config**: Pydantic, PyYAML
- **Testing**: pytest, pytest-cov
- **CI/CD**: GitHub Actions
- **Container**: Docker

## ğŸ“– DocumentaciÃ³n

- [Model Card](docs/model_card.md)
- [Data Card](docs/data_card.md)
```

---

## ğŸ¯ <a id="205-rubrica-de-evaluacion"></a> RÃºbrica de EvaluaciÃ³n

### Nivel Junior (50-69 puntos)
- Funciona pero con estructura bÃ¡sica
- Tests mÃ­nimos
- Sin CI/CD

### Nivel Mid (70-84 puntos)
- Estructura correcta
- Tests con coverage > 70%
- CI bÃ¡sico

### Nivel Senior (85-94 puntos)
- Custom Transformer funcionando
- Coverage > 80%
- CI/CD completo
- DocumentaciÃ³n profesional

### Nivel Staff (95-100 puntos)
- Todo lo anterior
- Drift detection
- MLflow integration
- Model Card completo
- Code review pasable en FAANG

---

## ğŸ§¨ <a id="errores-habituales"></a> Errores habituales y cÃ³mo depurarlos en el Proyecto Integrador

En el proyecto integrador el mayor reto no es una tecnologÃ­a concreta, sino **coordinar todas las piezas** sin romper nada en el camino.

Si alguno de estos errores te tomÃ³ **>15 minutos**, regÃ­stralo en el **[Diario de Errores](study_tools/DIARIO_ERRORES.md)** y aplica el flujo de **rescate cognitivo** de **[Protocolo E](study_tools/PROTOCOLO_E.md)**.

### 1) Empezar por el modelo y olvidar la estructura

**SÃ­ntomas tÃ­picos**

- Tienes notebooks y scripts sueltos, pero no un paquete `src/planrec` ni `pyproject.toml` claros.
- Es difÃ­cil correr el proyecto en otra mÃ¡quina o en CI.

**CÃ³mo identificarlo**

- PregÃºntate: Â¿puedo ejecutar `pip install -e .` y luego `python -m planrec.cli` o similar?

**CÃ³mo corregirlo**

- Copia la estructura de BankChurn/CarVision: `src/`, `configs/`, `app/`, `tests/`, `artifacts/`.
- Define desde el inicio `pyproject.toml`, `Makefile` y `.gitignore`.

---

### 2) Config dispersa o duplicada

**SÃ­ntomas tÃ­picos**

- Rutas de datos, thresholds o hiperparÃ¡metros hardcodeados en varios archivos.
- Cambias algo en un sitio y se rompe otra parte.

**CÃ³mo identificarlo**

- Busca valores repetidos (por ejemplo, paths o columnas) en mÃºltiples mÃ³dulos.

**CÃ³mo corregirlo**

- Centraliza configuraciÃ³n en `configs/config.yaml` y una clase Pydantic (`Config`) que valide todo.
- Haz que training, API y scripts lean SIEMPRE desde esa fuente de verdad.

---

### 3) Tests que no cubren el flujo completo

**SÃ­ntomas tÃ­picos**

- Coverage aceptable, pero sin tests de integraciÃ³n ni de API.
- El pipeline entero falla cuando intentas ejecutar `make train` o el endpoint `/predict`.

**CÃ³mo identificarlo**

- Revisa si tienes al menos:
  - Tests de features (`test_features.py`).
  - Tests de datos (`test_data.py`).
  - Tests de entrenamiento/integraciÃ³n (`test_training.py`).

**CÃ³mo corregirlo**

- AÃ±ade al menos un test que recorra el flujo E2E con datos pequeÃ±os, similar a los de CarVision.
- Usa fixtures y `tmp_path` para no depender de rutas reales.

---

### 4) CI/CD que solo corre en local

**SÃ­ntomas tÃ­picos**

- Tienes un archivo `.github/workflows/ci.yml` pero los jobs fallan siempre en GitHub.

**CÃ³mo identificarlo**

- Compara el workflow con el del portafolio: Â¿coinciden `working-directory`, versiones de Python y comandos?

**CÃ³mo corregirlo**

- Simplifica primero: un job que haga `pip install -e .` y `pytest`.
- AÃ±ade coverage y linting cuando el flujo bÃ¡sico sea estable.

---

### 5) PatrÃ³n general de debugging del proyecto integrador

1. Valida la **base**: estructura, instalaciÃ³n (`pip install -e .`), `make test`.
2. AsegÃºrate de que el **pipeline de training** funciona de principio a fin con datos pequeÃ±os.
3. Solo entonces aÃ±ade API, Docker y CI/CD, verificando cada capa con su propio conjunto de tests.

Con este enfoque, reduces la frustraciÃ³n y aumentas la probabilidad de tener un **4Âº proyecto sÃ³lido de portafolio**.

---

## ğŸ’¡ <a id="206-tips-para-exito"></a> Tips para Ã‰xito

1. **Empieza por la estructura** - No escribas cÃ³digo sin tener pyproject.toml y Makefile
2. **Tests primero** - TDD te ahorra tiempo a largo plazo
3. **Commits pequeÃ±os** - Un commit por feature, mensajes claros
4. **README actualizado** - ActualÃ­zalo mientras avanzas, no al final
5. **Copia patrones** - Usa el cÃ³digo de BankChurn/CarVision como referencia

---

## ğŸ’¼ <a id="207-consejos-profesionales"></a> Consejos Profesionales

> **Recomendaciones para destacar en entrevistas y proyectos reales**

### Para Entrevistas

1. **Cuenta una historia**: Tu portafolio debe mostrar progresiÃ³n y aprendizaje.

2. **Explica decisiones**: "Â¿Por quÃ© elegiste X?" es la pregunta mÃ¡s comÃºn.

3. **Muestra mÃ©tricas**: Impacto cuantificable impresiona mÃ¡s que features.

### Para tu Portafolio

| Elemento | Por quÃ© Importa |
|----------|-----------------|
| README profesional | Primera impresiÃ³n, 30 segundos para captar atenciÃ³n |
| Demo en vivo | Muestra que funciona, no solo que existe |
| CÃ³digo limpio | Los revisores leen tu cÃ³digo |
| DocumentaciÃ³n | Demuestra comunicaciÃ³n tÃ©cnica |

### Checklist Final del Portafolio

- [ ] Cada proyecto tiene problema claro y soluciÃ³n
- [ ] MÃ©tricas de performance documentadas
- [ ] CI/CD funcionando con badges
- [ ] Docker para reproducibilidad
- [ ] README con GIFs o screenshots
- [ ] Deployed y accesible (demo link)


---

## ğŸ“º Recursos Externos del MÃ³dulo

> ğŸ·ï¸ Sistema: ğŸ”´ Obligatorio | ğŸŸ¡ Recomendado | ğŸŸ¢ Complementario

### ğŸ¬ Videos

| ğŸ·ï¸ | TÃ­tulo | Canal | DuraciÃ³n | Link |
|:--:|:-------|:------|:--------:|:-----|
| ğŸ”´ | **End-to-End ML Project** | Krish Naik | 2h | [YouTube](https://www.youtube.com/watch?v=S_F_c9e2bz4) |
| ğŸŸ¡ | **MLOps Best Practices** | Google Cloud | 45 min | [YouTube](https://www.youtube.com/watch?v=4W_NfOeQKEU) |

### ğŸ“š Cursos Integrales

| ğŸ·ï¸ | TÃ­tulo | Plataforma | Cubre | Link |
|:--:|:-------|:-----------|:------|:-----|
| ğŸ”´ | **MLOps Zoomcamp** | DataTalksClub | Todo el stack | [GitHub](https://github.com/DataTalksClub/mlops-zoomcamp) |
| ğŸ”´ | **Made With ML** | MadeWithML | ML + MLOps | [MadeWithML](https://madewithml.com/) |
| ğŸŸ¡ | **Full Stack Deep Learning** | FSDL | ProducciÃ³n ML | [FSDL](https://fullstackdeeplearning.com/) |

---

## ğŸ”§ Ejercicios del MÃ³dulo

### Ejercicio 23.1: Script E2E Completo
**Objetivo**: Crear script que ejecute pipeline completo.
**Dificultad**: â­â­â­

```python
# scripts/run_e2e.py
# TU TAREA: Script que ejecute todo el pipeline

def run_e2e_pipeline():
    """Ejecuta pipeline completo de ML."""
    # 1. Verificar datos existen
    # 2. Entrenar modelo
    # 3. Verificar artefactos
    # 4. Levantar API
    # 5. Test de integraciÃ³n
    # 6. Cleanup
    pass
```

<details>
<summary>ğŸ’¡ Ver soluciÃ³n</summary>

```python
#!/usr/bin/env python3
"""Script E2E para validar pipeline completo."""

import subprocess
import sys
import time
from pathlib import Path

import requests


def run_e2e_pipeline() -> bool:
    """Ejecuta pipeline completo de ML."""
    print("ğŸš€ Iniciando pipeline E2E...")
    
    # 1. Verificar datos
    data_path = Path("data/raw/dataset.csv")
    if not data_path.exists():
        print(f"âŒ Dataset no encontrado: {data_path}")
        return False
    print("âœ… Dataset encontrado")
    
    # 2. Entrenar modelo
    print("ğŸ”„ Entrenando modelo...")
    result = subprocess.run(
        ["python", "-m", "bankchurn.training"],
        capture_output=True,
        text=True
    )
    if result.returncode != 0:
        print(f"âŒ Error en training: {result.stderr}")
        return False
    print("âœ… Modelo entrenado")
    
    # 3. Verificar artefactos
    model_path = Path("artifacts/model.joblib")
    if not model_path.exists():
        print(f"âŒ Modelo no encontrado: {model_path}")
        return False
    print("âœ… Artefactos generados")
    
    # 4. Levantar API
    print("ğŸ”„ Iniciando API...")
    api_process = subprocess.Popen(
        ["uvicorn", "app.fastapi_app:app", "--port", "8000"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    time.sleep(5)  # Esperar startup
    
    try:
        # 5. Test de integraciÃ³n
        print("ğŸ”„ Ejecutando tests de integraciÃ³n...")
        
        # Health check
        response = requests.get("http://localhost:8000/health")
        assert response.status_code == 200
        print("âœ… Health check OK")
        
        # Prediction
        test_data = {"feature1": 1.0, "feature2": 0.5}
        response = requests.post(
            "http://localhost:8000/predict",
            json=test_data
        )
        assert response.status_code == 200
        assert "prediction" in response.json()
        print("âœ… Prediction endpoint OK")
        
        print("ğŸ‰ Pipeline E2E completado exitosamente!")
        return True
        
    finally:
        # 6. Cleanup
        api_process.terminate()
        api_process.wait()
        print("âœ… Cleanup completado")


if __name__ == "__main__":
    success = run_e2e_pipeline()
    sys.exit(0 if success else 1)
```
</details>

---

## ğŸ”— Glosario del MÃ³dulo

| TÃ©rmino | DefiniciÃ³n |
|---------|------------|
| **E2E Pipeline** | Flujo completo desde datos raw hasta predicciÃ³n en producciÃ³n |
| **Integration Test** | Tests que verifican mÃºltiples componentes trabajando juntos |
| **Smoke Test** | Test rÃ¡pido que verifica funcionalidad bÃ¡sica estÃ¡ operativa |
| **Self-assessment** | AutoevaluaciÃ³n usando rÃºbrica de criterios |

---

## ğŸ Entrega Final
 
1. Repositorio pÃºblico en GitHub
2. CI pasando (verde)
3. README con badges actualizados
4. Self-assessment del checklist completado

---

<a id="2011-monorepo"></a>

## 23.11 ğŸ”¬ IngenierÃ­a Inversa PedagÃ³gica: Arquitectura Monorepo

> **Objetivo**: Entender cÃ³mo escalar de 1 proyecto a 3 proyectos compartiendo cÃ³digo.

### 23.11.1 ğŸ¯ El "Por QuÃ©" ArquitectÃ³nico

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EVOLUCIÃ“N DEL PORTAFOLIO: 1 â†’ 3 PROYECTOS                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PROBLEMA 1: Â¿CÃ³mo comparto cÃ³digo entre BankChurn, CarVision y TelecomAI?      â”‚
â”‚  DECISIÃ“N: common_utils/ como librerÃ­a interna instalable                       â”‚
â”‚  RESULTADO: DRY a nivel de portafolio, logger y seeds consistentes              â”‚
â”‚                                                                                 â”‚
â”‚  PROBLEMA 2: Â¿CÃ³mo mantengo CI/CD para 3 proyectos sin duplicar workflows?      â”‚
â”‚  DECISIÃ“N: Matriz de GitHub Actions con strategy.matrix.project                 â”‚
â”‚  RESULTADO: Un workflow, 3 proyectos testeados en paralelo                      â”‚
â”‚                                                                                 â”‚
â”‚  PROBLEMA 3: Â¿CÃ³mo evito que cambios en un proyecto rompan otros?               â”‚
â”‚  DECISIÃ“N: Cada proyecto tiene su propio pyproject.toml y tests aislados        â”‚
â”‚  RESULTADO: Independencia con cÃ³digo compartido opcional                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 23.11.2 ğŸ” AnatomÃ­a del Monorepo

**Estructura**: `ML-MLOps-Portfolio/`

```
ML-MLOps-Portfolio/
â”‚
â”œâ”€â”€ common_utils/                    # â† LIBRERÃA COMPARTIDA
â”‚   â”œâ”€â”€ __init__.py                  # Exports: setup_logging, set_seed
â”‚   â”œâ”€â”€ logger.py                    # Logging consistente para todos
â”‚   â””â”€â”€ seed.py                      # Reproducibilidad centralizada
â”‚
â”œâ”€â”€ BankChurn-Predictor/             # â† PROYECTO 1 (independiente)
â”‚   â”œâ”€â”€ src/bankchurn/
â”‚   â”‚   â””â”€â”€ training.py              # from common_utils import setup_logging
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ pyproject.toml               # Dependencias propias
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ CarVision-Market-Intelligence/   # â† PROYECTO 2 (independiente)
â”‚   â”œâ”€â”€ src/carvision/
â”‚   â”‚   â””â”€â”€ training.py              # from common_utils import set_seed
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ TelecomAI-Customer-Intelligence/ # â† PROYECTO 3 (independiente)
â”‚   â”œâ”€â”€ src/telecom/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci-mlops.yml                 # â† UN workflow para los 3 proyectos
â”‚
â”œâ”€â”€ infra/                           # Docker Compose, Prometheus, etc.
â””â”€â”€ Makefile                         # Comandos raÃ­z delegando a sub-proyectos
```

### 23.11.3 ğŸ“¦ common_utils: CÃ³digo Compartido

```python
# common_utils/__init__.py
"""
API pÃºblica de utilidades compartidas.

Todas las funciones aquÃ­ son usadas por BankChurn, CarVision y TelecomAI
para garantizar consistencia en logging y reproducibilidad.
"""

from common_utils.logger import setup_logging    # Logging consistente.
from common_utils.seed import set_seed           # Seeds para reproducibilidad.

__version__ = "1.0.0"                            # VersiÃ³n de la librerÃ­a.
__all__ = ["setup_logging", "set_seed"]          # Exports explÃ­citos.
```

```python
# common_utils/seed.py
"""Reproducibilidad centralizada para todos los proyectos."""

import os                                        # Variables de entorno.
import random                                    # Random de Python.
import numpy as np                               # NumPy random.

DEFAULT_SEED = 42                                # Valor por defecto.


def set_seed(seed: int = DEFAULT_SEED) -> int:
    """
    Configura seeds globales para reproducibilidad.
    
    Esta funciÃ³n setea el seed para Python, NumPy, y opcionalmente
    PyTorch/TensorFlow si estÃ¡n instalados.
    """
    os.environ["PYTHONHASHSEED"] = str(seed)     # Hash determinÃ­stico.
    random.seed(seed)                            # Random de Python.
    np.random.seed(seed)                         # NumPy.
    
    # PyTorch (opcional, si estÃ¡ instalado).
    try:
        import torch
        torch.manual_seed(seed)                  # CPU.
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)     # GPU.
    except ImportError:
        pass  # PyTorch no instalado.
    
    return seed
```

### 23.11.4 ğŸ”„ CI/CD con Matriz de Proyectos

```yaml
# .github/workflows/ci-mlops.yml
name: CI/CD MLOps Portfolio

on:
  push:
    branches: [main, develop]

jobs:
  tests:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false                           # No cancelar otros si uno falla.
      matrix:
        python-version: ['3.11', '3.12']         # Probar mÃºltiples versiones.
        project:                                 # â† LOS 3 PROYECTOS
          - BankChurn-Predictor
          - CarVision-Market-Intelligence
          - TelecomAI-Customer-Intelligence
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Install dependencies
        working-directory: ${{ matrix.project }} # â† Cambia a cada proyecto.
        run: pip install -e ".[dev]"
      
      - name: Run tests
        working-directory: ${{ matrix.project }}
        run: pytest --cov --cov-fail-under=80
```

### 23.11.5 ğŸ§ª Laboratorio de ReplicaciÃ³n

```bash
# Paso 1: Crear estructura monorepo desde cero
mkdir mi-portfolio-ml && cd mi-portfolio-ml

# Paso 2: Crear common_utils
mkdir -p common_utils
cat > common_utils/__init__.py << 'EOF'
from common_utils.logger import setup_logging
from common_utils.seed import set_seed
__all__ = ["setup_logging", "set_seed"]
EOF

# Paso 3: Crear primer proyecto usando common_utils
mkdir -p proyecto1/src/proyecto1
cat > proyecto1/src/proyecto1/training.py << 'EOF'
import sys
sys.path.insert(0, "../..")  # Para desarrollo local.
from common_utils import setup_logging, set_seed

logger = setup_logging(__name__)
set_seed(42)

def train():
    logger.info("Training con seed reproducible")
EOF

# Paso 4: Verificar que funciona
cd proyecto1 && python -c "from src.proyecto1.training import train; train()"
```

### 23.11.6 ğŸš¨ Troubleshooting Monorepo

| SÃ­ntoma | Causa | SoluciÃ³n |
|---------|-------|----------|
| **"ModuleNotFoundError: common_utils"** | PYTHONPATH no incluye raÃ­z | `pip install -e ../common_utils` o `sys.path.insert` |
| **CI falla solo en un proyecto** | Dependencias diferentes | Verificar `pyproject.toml` de ese proyecto |
| **Cambio en common_utils rompe proyecto** | Sin tests de integraciÃ³n | AÃ±adir tests que importen desde common_utils |

---

## ğŸ† CHECKPOINT FINAL: GuÃ­a Completa

> ğŸ¯ **Â¡Felicidades! Has completado los mÃ³dulos 01-23**
>
> Ahora tienes el conocimiento de un Senior/Staff MLOps Engineer:
> - âœ… Python profesional con type hints y Pydantic
> - âœ… Pipelines ML reproducibles con sklearn y DVC
> - âœ… Testing completo con 80%+ coverage
> - âœ… CI/CD profesional con GitHub Actions
> - âœ… Docker, APIs y dashboards de producciÃ³n
> - âœ… Observabilidad con drift detection
> - âœ… Infraestructura como cÃ³digo
> - âœ… 3 proyectos production-ready en tu portafolio

---

### ğŸ“‹ Examen Final de IntegraciÃ³n

> **Formato**: Self-Correction System Design  
> **DuraciÃ³n**: 90-120 minutos  
> **Puntaje mÃ­nimo**: 70/100

#### Ejercicio: DiseÃ±a un Sistema ML

**Escenario**: Una empresa de e-commerce quiere predecir quÃ© productos comprarÃ¡n los usuarios. Tienes:
- 10M usuarios activos
- 1M productos
- 100M interacciones/mes
- Latencia requerida: <100ms
- Budget: Moderado (no FAANG)

**Tu tarea**: DiseÃ±a la arquitectura completa.

<details>
<summary>ğŸ“ Ver SoluciÃ³n</summary>

**Arquitectura Propuesta:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA LAYER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  S3 (raw) â†’ Spark (ETL) â†’ Feature Store (Redis) â†’ DVC      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRAINING LAYER                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MLflow Tracking â†’ Kubernetes Jobs â†’ Model Registry         â”‚
â”‚  - Batch training diario                                    â”‚
â”‚  - A/B testing de modelos                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVING LAYER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Load Balancer â†’ FastAPI (K8s HPA) â†’ Redis Cache            â”‚
â”‚  - 3 replicas mÃ­nimo                                        â”‚
â”‚  - Auto-scale hasta 10                                      â”‚
â”‚  - Cache de predicciones frecuentes                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MONITORING LAYER                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Prometheus â†’ Grafana â†’ Alerting â†’ Evidently (drift)        â”‚
â”‚  - Latency p99 < 100ms                                      â”‚
â”‚  - Error rate < 0.1%                                        â”‚
â”‚  - Drift check diario                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Decisiones clave:**
- **Feature Store (Redis)**: Pre-computa features para latencia <100ms
- **K8s HPA**: Auto-scaling para 10M usuarios
- **Cache**: Productos populares tienen predicciones cacheadas
- **Batch + Real-time**: Training batch, serving real-time
</details>

---

### ğŸ¤ Simulacro de Entrevista: Nivel Senior/Staff

> **115+ preguntas** divididas en 2 partes
> **Tiempo**: 90 min cada parte
> **Objetivo**: PreparaciÃ³n para posiciones Senior/Staff ML Engineer

#### Parte 1: TÃ©cnico Avanzado (50+ preguntas)

**System Design (15 preguntas)**
1. Â¿CÃ³mo diseÃ±arÃ­as un sistema de recomendaciones para 100M usuarios?
2. Â¿Trade-offs entre batch y real-time serving?
3. Â¿CÃ³mo manejarÃ­as cold start en recomendaciones?

**Arquitectura ML (15 preguntas)**
4. Â¿CuÃ¡ndo usarÃ­as feature store vs computar on-the-fly?
5. Â¿CÃ³mo implementarÃ­as A/B testing para modelos?
6. Â¿Estrategia de rollback si un modelo degrada?

**Infraestructura (10 preguntas)**
7. Â¿CÃ³mo optimizarÃ­as costos en un pipeline ML en AWS?
8. Â¿CuÃ¡ndo usar Spot vs On-Demand para training?
9. Â¿CÃ³mo escalarÃ­as a mÃºltiples regiones?

<details>
<summary>ğŸ’¡ Ver Respuestas de Muestra</summary>

**1. Sistema de recomendaciones 100M usuarios:**
> Arquitectura en capas: (1) Candidate generation con ANN (approximate nearest neighbors), (2) Ranking con modelo mÃ¡s complejo, (3) Re-ranking con reglas de negocio. Feature store para latencia. Sharding por user_id. Cache para usuarios frecuentes.

**5. A/B testing de modelos:**
> Traffic splitting a nivel de load balancer. MÃ©tricas: latency, prediction distribution, business metrics (CTR, conversiÃ³n). DuraciÃ³n mÃ­nima para significance estadÃ­stica. Rollback automÃ¡tico si degradaciÃ³n >X%.

**7. Optimizar costos AWS:**
> (1) Spot instances para training (70% ahorro), (2) Right-sizing de instancias, (3) S3 lifecycle policies, (4) Reserved capacity para serving baseline, (5) Auto-scaling agresivo en off-peak.
</details>

---

#### Parte 2: Liderazgo y Trade-offs (65 preguntas)

**Liderazgo TÃ©cnico (20 preguntas)**
1. Â¿CÃ³mo priorizas deuda tÃ©cnica vs nuevas features?
2. Â¿CÃ³mo convences a stakeholders de invertir en MLOps?
3. Â¿CÃ³mo mentorizas a juniors en ML?

**Trade-offs (20 preguntas)**
4. Â¿CuÃ¡ndo sacrificarÃ­as accuracy por latency?
5. Â¿Build vs buy para herramientas MLOps?
6. Â¿Monolito vs microservicios para ML?

**Casos PrÃ¡cticos (25 preguntas)**
7. Tu modelo tiene bias racial. Â¿QuÃ© haces?
8. El CEO quiere ML en todo. Â¿CÃ³mo priorizas?
9. ProducciÃ³n falla a las 3am. Â¿Tu proceso?

<details>
<summary>ğŸ’¡ Ver Respuestas de Muestra</summary>

**2. Convencer stakeholders de MLOps:**
> Mostrar mÃ©tricas de impacto: "Sin CI/CD, bugs llegan a producciÃ³n 3x mÃ¡s. Con testing, reducimos incidentes 60%." Hablar en tÃ©rminos de negocio: tiempo de desarrollo, confiabilidad, velocidad de iteraciÃ³n.

**5. Build vs buy:**
> Build si: core competency, requirements muy especÃ­ficos, control total necesario. Buy si: commodity, time-to-market crÃ­tico, equipo pequeÃ±o. MLflow: open source gratuito. Datadog: costoso pero ahorra tiempo.

**7. Bias en modelo:**
> (1) No silenciar, escalar inmediatamente. (2) Cuantificar: Â¿quÃ© grupos afectados? (3) Rollback si impacto significativo. (4) Root cause: datos histÃ³ricos, features proxy. (5) MitigaciÃ³n: resampling, fairness constraints, auditorÃ­a continua.
</details>

---

[Ver simulacro completo Parte 1 â†’](simulacros/SIMULACRO_ENTREVISTA_SENIOR_PARTE1.md)
[Ver simulacro completo Parte 2 â†’](simulacros/SIMULACRO_ENTREVISTA_SENIOR_PARTE2.md)

---

### ğŸ¯ PreparaciÃ³n de Entrevistas

#### Speech de Portafolio (5-7 minutos)

**Estructura recomendada:**

```
1. INTRO (30 seg)
   "He construido un portafolio de 3 proyectos ML production-ready..."

2. PROYECTO DESTACADO (2-3 min)
   - Problema de negocio
   - Decisiones tÃ©cnicas clave
   - MÃ©tricas de impacto

3. STACK TÃ‰CNICO (1-2 min)
   - Por quÃ© sklearn pipelines
   - Por quÃ© FastAPI + Docker
   - Observabilidad con Prometheus

4. DIFERENCIADORES (1 min)
   - 80%+ coverage en todos los proyectos
   - CI/CD completo
   - DocumentaciÃ³n profesional

5. CIERRE (30 seg)
   "Estoy buscando oportunidades donde pueda..."
```

#### Talking Points Clave

| Pregunta ComÃºn | Respuesta Concisa |
|----------------|-------------------|
| "Â¿Por quÃ© sklearn?" | "Pipelines serializables, reproducibilidad, integraciÃ³n con MLflow" |
| "Â¿Por quÃ© FastAPI?" | "Async, validaciÃ³n Pydantic, docs automÃ¡ticas, rendimiento" |
| "Â¿CÃ³mo garantizas calidad?" | "80%+ coverage, pre-commit hooks, CI gates" |
| "Â¿Tu mayor desafÃ­o?" | "Prevenir data leakage en pipelines complejos" |

---

[Ver Speech completo â†’](entrevistas/APENDICE_A_SPEECH_PORTAFOLIO.md)
[Ver Talking Points â†’](entrevistas/APENDICE_B_TALKING_POINTS.md)

---

## âœ… Checklist Final del Portafolio

- [ ] `pip install -e .` funciona en un entorno limpio
- [ ] `make test` pasa con coverage objetivo
- [ ] `make train` produce artefactos reproducibles
- [ ] API expone `/health` y `/predict`
- [ ] CI estÃ¡ en verde con badges en README
- [ ] 3 proyectos con diferentes problemas ML
- [ ] Model Cards y documentaciÃ³n completa
- [ ] Demo accesible (local o deployed)

---

<div align="center">

## ğŸ‰ Â¡Felicidades!

Has completado la **GuÃ­a MLOps â€” Portfolio Edition**

Ahora tienes las habilidades de un **Senior/Staff MLOps Engineer**

---

**[â† IaC Empresarial](22_IAC_EMPRESARIAL.md)** | **[Volver al Ãndice â†’](00_INDICE.md)**

</div>

**Â¡Ã‰xito en tu proyecto! ğŸš€**

[â† DocumentaciÃ³n](19_DOCUMENTACION.md) | [Siguiente: Glosario â†’](21_GLOSARIO.md)

</div>
