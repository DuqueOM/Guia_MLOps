# 10. Experiment Tracking con MLflow
 
 <a id="00-prerrequisitos"></a>
 
 ## 0.0 Prerrequisitos
 
 - Tener al menos 1 proyecto del portafolio con:
   - `artifacts/training_results.json` (o equivalente)
   - Un modelo serializado (`model.pkl`, `model.joblib`, etc.)
 - Poder instalar MLflow en tu entorno del proyecto.
 
 ---
 
 <a id="01-protocolo-e-como-estudiar-este-modulo"></a>
 
 ## 0.1 ğŸ§  Protocolo E: CÃ³mo estudiar este mÃ³dulo
 
 - **Antes de empezar**: abre **[Protocolo E](study_tools/PROTOCOLO_E.md)** y define tu *output mÃ­nimo* (1 run visible y comparable).
 - **Durante el debugging**: si te atoras >15 min (tracking_uri, artifacts, registry), registra el caso en **[Diario de Errores](study_tools/DIARIO_ERRORES.md)**.
 - **Al cierre de semana**: usa **[Cierre Semanal](study_tools/CIERRE_SEMANAL.md)** para auditar trazabilidad (runs, params, mÃ©tricas, artifacts).
 
 ---
 
 <a id="02-entregables-verificables-minimo-viable"></a>
 
 ## 0.2 âœ… Entregables verificables (mÃ­nimo viable)
 
 - [ ] Un experimento MLflow con:
   - params (ej. hiperparÃ¡metros o `run_type`)
   - mÃ©tricas (ej. `test_f1`, `test_auc`)
   - artifacts (ej. `training_results.json`, `config.yaml`)
 - [ ] Evidencia (UI o `mlflow ui`) de poder comparar 2 runs.
 - [ ] 1 entrada en **[Diario de Errores](study_tools/DIARIO_ERRORES.md)** si hubo bloqueo real.
 
 ---
 
 <a id="03-puente-teoria-codigo-portafolio"></a>
 
 ## 0.3 ğŸ§© Puente teorÃ­a â†” cÃ³digo (Portafolio)
 
 - **Concepto**: experiment tracking (params/metrics/artifacts + comparaciÃ³n + registry)
 - **Archivo**: `scripts/run_mlflow.py`, `docker-compose.mlflow.yml`, `configs/config.yaml`
 - **Prueba**: entrenar â†’ loguear â†’ abrir UI â†’ comparar runs
 
 ## ğŸ¯ Objetivo del MÃ³dulo
 
 Implementar tracking de experimentos como lo hace el portafolio con `run_mlflow.py`.

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘  SIN MLFLOW:                           CON MLFLOW:                           â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â•‘
â•‘  "Â¿QuÃ© hiperparÃ¡metros usÃ© hace        "MLflow run abc123: RF con            â•‘
â•‘   2 semanas cuando obtuve F1=0.85?"    n_estimators=200, F1=0.85"            â•‘
â•‘                                                                              â•‘
â•‘  "Â¿DÃ³nde guardÃ© ese modelo bueno?"     "Artifacts en run abc123/model.pkl"   â•‘
â•‘                                                                              â•‘
â•‘  "Â¿Por quÃ© este modelo es peor?"       "Comparar runs en UI: diff params"    â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### ğŸ§© CÃ³mo se aplica en este portafolio

- En **BankChurn-Predictor** ya tienes:
  - `scripts/run_mlflow.py` como script de logging posterior al entrenamiento.
  - ConfiguraciÃ³n de MLflow en `configs/config.yaml` y `src/bankchurn/config.py`.
 - El archivo `docker-compose.mlflow.yml` en la raÃ­z del repo levanta un servidor MLflow
   real que puedes usar para practicar este mÃ³dulo.
 - El mismo patrÃ³n de logging puedes aplicarlo a **CarVision** y **TelecomAI**, usando
   sus `artifacts/` y modelos entrenados como fuente de mÃ©tricas y artifacts.

---

## ğŸ“‹ Contenido

- **0.0** [Prerrequisitos](#00-prerrequisitos)
- **0.1** [Protocolo E: CÃ³mo estudiar este mÃ³dulo](#01-protocolo-e-como-estudiar-este-modulo)
- **0.2** [Entregables verificables (mÃ­nimo viable)](#02-entregables-verificables-minimo-viable)
- **0.3** [Puente teorÃ­a â†” cÃ³digo (Portafolio)](#03-puente-teoria-codigo-portafolio)
1. [Conceptos de MLflow](#101-conceptos-de-mlflow)
2. [Setup y ConfiguraciÃ³n](#102-setup-y-configuracion)
3. [Logging de Experimentos](#103-logging-de-experimentos)
4. [Model Registry](#104-model-registry)
5. [CÃ³digo Real del Portafolio](#105-codigo-real-del-portafolio)
6. [ğŸ”¬ IngenierÃ­a Inversa PedagÃ³gica: MLflow ProducciÃ³n](#106-ingenieria-inversa-mlflow) â­ NUEVO
- [Errores habituales](#errores-habituales)
- [âœ… Ejercicio](#ejercicio)
- [âœ… Checkpoint](#checkpoint)

---

<a id="101-conceptos-de-mlflow"></a>

## 10.1 Conceptos de MLflow

### Los 4 Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          MLFLOW COMPONENTS                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. TRACKING                    2. PROJECTS                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚  â€¢ Log params, metrics          â€¢ Empaquetar cÃ³digo                         â”‚
â”‚  â€¢ Guardar artifacts            â€¢ MLproject file                            â”‚
â”‚  â€¢ Comparar runs                â€¢ Reproducibilidad                          â”‚
â”‚                                                                             â”‚
â”‚  3. MODELS                      4. REGISTRY                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚
â”‚  â€¢ Formato estÃ¡ndar             â€¢ Versionado de modelos                     â”‚
â”‚  â€¢ Flavors (sklearn, pytorch)   â€¢ Staging â†’ Production                      â”‚
â”‚  â€¢ Serving                      â€¢ Aprobaciones                              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

EN ESTE PORTAFOLIO USAMOS: Tracking + Registry
```

### JerarquÃ­a de MLflow

```
MLflow Server
â””â”€â”€ Experiment: "BankChurn"
    â”œâ”€â”€ Run: abc123 (2024-01-15)
    â”‚   â”œâ”€â”€ Parameters: {n_estimators: 200, max_depth: 10}
    â”‚   â”œâ”€â”€ Metrics: {f1: 0.65, auc: 0.88}
    â”‚   â””â”€â”€ Artifacts: [model.pkl, config.yaml]
    â”‚
    â”œâ”€â”€ Run: def456 (2024-01-16)
    â”‚   â”œâ”€â”€ Parameters: {n_estimators: 200, max_depth: 15}
    â”‚   â”œâ”€â”€ Metrics: {f1: 0.62, auc: 0.86}
    â”‚   â””â”€â”€ Artifacts: [model.pkl, config.yaml]
    â”‚
    â””â”€â”€ Run: ghi789 (2024-01-17) â† MEJOR
        â”œâ”€â”€ Parameters: {n_estimators: 200, max_depth: 10}
        â”œâ”€â”€ Metrics: {f1: 0.65, auc: 0.88}
        â””â”€â”€ Artifacts: [model.pkl, config.yaml]
```

---

<a id="102-setup-y-configuracion"></a>

## 10.2 Setup y ConfiguraciÃ³n

### OpciÃ³n 1: Local (File Store)

```python
# MÃ¡s simple, para desarrollo local
import mlflow

mlflow.set_tracking_uri("file:./mlruns")  # Guarda en carpeta local
mlflow.set_experiment("my-experiment")
```

### OpciÃ³n 2: Servidor MLflow (ProducciÃ³n)

```yaml
# docker-compose.mlflow.yml del portafolio
services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.9.2
    ports:
      - "5000:5000"
    volumes:
      - mlflow-artifacts:/mlflow
    command: >
      mlflow server
      --backend-store-uri sqlite:///mlflow/mlflow.db
      --default-artifact-root /mlflow/artifacts
      --host 0.0.0.0
      --port 5000
```

```python
# Conectar al servidor
import mlflow

mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("BankChurn")
```

### ConfiguraciÃ³n en el Portafolio

```yaml
# configs/config.yaml (BankChurn)
mlflow:
  tracking_uri: "file:./mlruns"      # Local para desarrollo
  experiment_name: "bankchurn"
  enabled: true
```

```python
# src/bankchurn/config.py
class MLflowConfig(BaseModel):
    """MLflow tracking configuration."""
    tracking_uri: str = "file:./mlruns"
    experiment_name: str = "bankchurn"
    enabled: bool = True
```

---

<a id="103-logging-de-experimentos"></a>

## 10.3 Logging de Experimentos

### API BÃ¡sica

```python
import mlflow                            # Cliente de MLflow para tracking.

# Iniciar un run
with mlflow.start_run(run_name="experiment-1"):  # Context manager: auto-cierra el run al salir.
    
    # 1. LOG PARAMETERS (hiperparÃ¡metros, config)
    mlflow.log_param("n_estimators", 200)        # log_param: registra UN parÃ¡metro (key-value).
    mlflow.log_param("max_depth", 10)            # Los params son strings/nÃºmeros, no objetos.
    mlflow.log_params({                          # log_params: registra MÃšLTIPLES a la vez.
        "learning_rate": 0.1,
        "model_type": "random_forest"
    })
    
    # 2. LOG METRICS (resultados)
    mlflow.log_metric("f1_score", 0.65)          # log_metric: registra UNA mÃ©trica numÃ©rica.
    mlflow.log_metric("auc_roc", 0.88)           # Las mÃ©tricas se pueden comparar en la UI.
    mlflow.log_metrics({                         # log_metrics: mÃºltiples a la vez.
        "precision": 0.70,
        "recall": 0.61
    })
    
    # 3. LOG ARTIFACTS (archivos)
    mlflow.log_artifact("configs/config.yaml")   # log_artifact: sube archivo al servidor MLflow.
    mlflow.log_artifact("artifacts/training_results.json")  # Ãštil para reproducir el experimento.
    
    # 4. LOG MODEL (modelo serializado con metadata)
    mlflow.sklearn.log_model(                    # sklearn: "flavor" especÃ­fico para modelos sklearn.
        pipeline,                                # El objeto Pipeline fitted.
        artifact_path="model",                   # Subcarpeta dentro de artifacts del run.
        registered_model_name="BankChurnClassifier"  # Si existe, crea nueva versiÃ³n; si no, lo crea.
    )
```

### MÃ©tricas por Ã‰poca/Paso

```python
# Para modelos que entrenan por Ã©pocas
for epoch in range(100):
    train_loss = train_one_epoch()
    val_loss = validate()
    
    mlflow.log_metrics({
        "train_loss": train_loss,
        "val_loss": val_loss
    }, step=epoch)  # â† step permite graficar evoluciÃ³n
```

---

<a id="104-model-registry"></a>

## 10.4 Model Registry

### Registrar un Modelo

```python
# Durante el run
mlflow.sklearn.log_model(
    pipeline,
    artifact_path="model",
    registered_model_name="BankChurnClassifier"  # â† Registra automÃ¡ticamente
)

# O despuÃ©s del run
mlflow.register_model(
    model_uri=f"runs:/{run_id}/model",
    name="BankChurnClassifier"
)
```

### Transiciones de Estado

```python
from mlflow.tracking import MlflowClient

client = MlflowClient()

# Promover a Staging
client.transition_model_version_stage(
    name="BankChurnClassifier",
    version=1,
    stage="Staging"
)

# Promover a Production (despuÃ©s de validaciÃ³n)
client.transition_model_version_stage(
    name="BankChurnClassifier",
    version=1,
    stage="Production"
)
```

### Cargar Modelo desde Registry

```python
# Cargar versiÃ³n especÃ­fica
model = mlflow.sklearn.load_model("models:/BankChurnClassifier/1")

# Cargar stage especÃ­fico
model = mlflow.sklearn.load_model("models:/BankChurnClassifier/Production")

# Cargar Ãºltimo modelo (latest)
model = mlflow.sklearn.load_model("models:/BankChurnClassifier/latest")
```

---

<a id="105-codigo-real-del-portafolio"></a>

## 10.5 CÃ³digo Real del Portafolio

### scripts/run_mlflow.py (BankChurn)

```python
#!/usr/bin/env python3
"""Log training results to MLflow.

Este script se ejecuta DESPUÃ‰S del entrenamiento para:
1. Leer resultados de artifacts/training_results.json
2. Calcular mÃ©tricas de negocio (revenue saved, etc.)
3. Loguear todo a MLflow
4. Opcionalmente registrar el modelo

Uso:
    python scripts/run_mlflow.py

Environment Variables:
    MLFLOW_TRACKING_URI: URI del servidor MLflow
    MLFLOW_EXPERIMENT_NAME: Nombre del experimento
"""

from __future__ import annotations

import json
import os
from pathlib import Path

import joblib

try:
    import mlflow
    import mlflow.sklearn
    from mlflow.tracking import MlflowClient
except ImportError:
    mlflow = None

from sklearn.pipeline import Pipeline


def main() -> None:
    # ConfiguraciÃ³n desde environment
    tracking_uri = os.getenv("MLFLOW_TRACKING_URI", "file:./mlruns")
    experiment = os.getenv("MLFLOW_EXPERIMENT_NAME", "BankChurn")
    
    # Cargar resultados del entrenamiento
    results_path = Path("artifacts/training_results.json")
    if not results_path.exists():
        print(f"No se encontrÃ³ {results_path}. Ejecuta training primero.")
        return
    
    data = json.loads(results_path.read_text())
    
    # Extraer mÃ©tricas
    cv = data.get("cv_results", {})
    test = data.get("test_results", {}).get("metrics", {})
    
    metrics = {}
    for k, v in cv.items():
        if isinstance(v, (int, float)):
            metrics[f"cv_{k}"] = float(v)
    for k, v in test.items():
        if isinstance(v, (int, float)):
            metrics[f"test_{k}"] = float(v)
    
    # Calcular mÃ©tricas de negocio
    cm = data.get("test_results", {}).get("confusion_matrix")
    if cm and len(cm) == 2:
        tn, fp = cm[0]
        fn, tp = cm[1]
        
        # ParÃ¡metros de negocio (configurables)
        clv = float(os.getenv("BC_CLV_USD", "2300"))  # Customer Lifetime Value
        retention_rate = float(os.getenv("BC_RETENTION_RATE", "0.3"))
        
        saved_customers = tp * retention_rate
        saved_revenue = saved_customers * clv
        
        metrics.update({
            "biz_detected_churners": float(tp),
            "biz_saved_customers": saved_customers,
            "biz_saved_revenue_usd": saved_revenue,
            "biz_false_positives": float(fp),
            "biz_missed_churners": float(fn),
        })
    
    if mlflow is None:
        print("MLflow no instalado. MÃ©tricas:", metrics)
        return
    
    # Configurar MLflow
    mlflow.set_tracking_uri(tracking_uri)
    mlflow.set_experiment(experiment)
    
    # Crear run
    with mlflow.start_run(run_name="demo-logging"):
        # Log parÃ¡metros
        mlflow.log_params({
            "run_type": "demo",
            "source": "run_mlflow.py"
        })
        
        # Log mÃ©tricas
        mlflow.log_metrics(metrics)
        
        # Log artifacts
        for artifact in [
            Path("artifacts/training_results.json"),
            Path("configs/config.yaml"),
        ]:
            if artifact.exists():
                try:
                    mlflow.log_artifact(str(artifact))
                except PermissionError:
                    print(f"Skipping {artifact}: permission denied")
        
        # Log modelo si existe
        model_path = Path("models/model_v1.0.0.pkl")
        if model_path.exists():
            try:
                obj = joblib.load(model_path)
                if isinstance(obj, dict) and "pipeline" in obj:
                    pipe = obj["pipeline"]
                elif isinstance(obj, Pipeline):
                    pipe = obj
                else:
                    pipe = None
                
                if pipe:
                    mlflow.sklearn.log_model(
                        pipe,
                        artifact_path="model",
                        registered_model_name="BankChurnClassifier"
                    )
            except Exception as e:
                print(f"Model logging skipped: {e}")
        
        print(f"âœ… MLflow run logged to {tracking_uri}")
        print(f"   Experiment: {experiment}")
        print(f"   Metrics: {len(metrics)} logged")


if __name__ == "__main__":
    main()
```

### Makefile Integration

```makefile
# Makefile
.PHONY: mlflow-demo mlflow-ui

mlflow-demo:
@echo "Logging to MLflow..."
MLFLOW_TRACKING_URI=file:./mlruns python scripts/run_mlflow.py

mlflow-ui:
@echo "Starting MLflow UI at http://localhost:5000"
mlflow ui --host 0.0.0.0 --port 5000
```

---

<a id="106-ingenieria-inversa-mlflow"></a>

## 10.6 ğŸ”¬ IngenierÃ­a Inversa PedagÃ³gica: MLflow en ProducciÃ³n Real

> **Objetivo**: Entender CADA decisiÃ³n arquitectÃ³nica detrÃ¡s del setup de MLflow del portafolio.

Esta secciÃ³n aplica el mÃ©todo de "Shadow Coder Senior": diseccionamos la infraestructura MLflow real que soporta los 3 proyectos del portafolio.

### 10.6.1 ğŸ¯ El "Por QuÃ©" ArquitectÃ³nico

Â¿Por quÃ© el portafolio usa un `docker-compose.mlflow.yml` separado en lugar de un simple `mlflow ui`?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISIONES ARQUITECTÃ“NICAS DEL PORTAFOLIO                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  PROBLEMA 1: `mlflow ui` guarda todo en archivos locales (SQLite + filesystem)  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  RIESGO: PÃ©rdida de datos, no escalable, no colaborativo                        â”‚
â”‚  DECISIÃ“N: PostgreSQL como backend store                                        â”‚
â”‚  RESULTADO: Persistencia robusta, queries SQL, backups fÃ¡ciles                  â”‚
â”‚  REFERENCIA: docker-compose.mlflow.yml lÃ­neas 8-24                              â”‚
â”‚                                                                                 â”‚
â”‚  PROBLEMA 2: Artifacts grandes (modelos) saturan el disco del servidor          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  RIESGO: Sin espacio, artifacts perdidos, no replicable a la nube               â”‚
â”‚  DECISIÃ“N: MinIO (S3-compatible) como artifact store                            â”‚
â”‚  RESULTADO: Storage ilimitado, compatible con AWS S3, UI para navegar           â”‚
â”‚  REFERENCIA: docker-compose.mlflow.yml lÃ­neas 27-46                             â”‚
â”‚                                                                                 â”‚
â”‚  PROBLEMA 3: Equipos necesitan compartir experimentos y modelos                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  RIESGO: "Funciona en mi mÃ¡quina", modelos duplicados, sin trazabilidad         â”‚
â”‚  DECISIÃ“N: Servidor MLflow centralizado con Model Registry                      â”‚
â”‚  RESULTADO: Un solo punto de verdad, promociÃ³n Stagingâ†’Production               â”‚
â”‚  REFERENCIA: docker-compose.mlflow.yml lÃ­neas 66-97                             â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.6.2 ğŸ” AnatomÃ­a de `docker-compose.mlflow.yml`

**Archivo**: `ML-MLOps-Portfolio/docker-compose.mlflow.yml`

```yaml
version: '3.8'

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SERVICIO 1: PostgreSQL (Backend Store)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
services:
  postgres:
    image: postgres:13-alpine                    # Alpine = imagen ligera (~50MB vs 300MB).
    container_name: mlflow-postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-mlflow}   # ${VAR:-default}: usa variable de entorno o default.
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-mlflow_password}
      - POSTGRES_DB=${POSTGRES_DB:-mlflow}
    volumes:
      - postgres_data:/var/lib/postgresql/data  # Volumen nombrado: persiste datos entre reinicios.
    healthcheck:                                 # Docker verifica que Postgres estÃ© LISTO.
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-mlflow}"]
      interval: 10s                              # Chequea cada 10 segundos.
      timeout: 5s                                # Falla si no responde en 5s.
      retries: 5                                 # 5 intentos antes de declarar "unhealthy".
    networks:
      - mlflow-network                           # Red interna: aÃ­sla servicios del host.

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SERVICIO 2: MinIO (Artifact Store S3-Compatible)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  minio:
    image: minio/minio:latest
    container_name: mlflow-minio
    ports:
      - "9000:9000"                              # API: donde MLflow sube/descarga artifacts.
      - "9001:9001"                              # Console: UI web para navegar buckets.
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER:-minioadmin}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD:-minioadmin}
    volumes:
      - minio_data:/data                         # Artifacts persisten aquÃ­.
    command: server /data --console-address ":9001"  # Inicia servidor con UI en 9001.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
# Â¿Por quÃ© curl y no un comando interno? MinIO expone health checks HTTP nativamente.

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SERVICIO 3: Bucket Creator (Init Container)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  minio-create-bucket:
    image: minio/mc:latest                       # mc = MinIO Client (CLI).
    container_name: mlflow-minio-setup
    depends_on:
      - minio                                    # Espera a que MinIO arranque.
    entrypoint: >                                # Script inline (patrÃ³n comÃºn en docker-compose).
      /bin/sh -c "
      sleep 10;                                  # Espera adicional (MinIO puede tardar).
      /usr/bin/mc alias set myminio http://minio:9000 minioadmin minioadmin;
      /usr/bin/mc mb myminio/mlflow-artifacts --ignore-existing;  # Crea bucket si no existe.
      /usr/bin/mc anonymous set download myminio/mlflow-artifacts;  # Permite descargas.
      exit 0;
      "
# Â¿Por quÃ© un contenedor separado? PatrÃ³n "init container": ejecuta una vez y termina.

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SERVICIO 4: MLflow Server
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    container_name: mlflow-server
    ports:
      - "5000:5000"                              # UI y API en el mismo puerto.
    environment:
      # Backend store: dÃ³nde guardar metadata (runs, params, metrics).
      - MLFLOW_BACKEND_STORE_URI=postgresql://mlflow:mlflow_password@postgres:5432/mlflow
      # Artifact store: dÃ³nde guardar archivos grandes (modelos, plots).
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://mlflow-artifacts/
      # Credenciales para MinIO (simula AWS S3).
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000  # CRÃTICO: apunta a MinIO, no a AWS.
    depends_on:
      postgres:
        condition: service_healthy               # Espera a que Postgres estÃ© healthy.
      minio:
        condition: service_healthy
      minio-create-bucket:
        condition: service_completed_successfully  # Espera a que el bucket exista.
    command: >
      mlflow server
      --backend-store-uri postgresql://mlflow:mlflow_password@postgres:5432/mlflow
      --default-artifact-root s3://mlflow-artifacts/
      --host 0.0.0.0                             # Escucha en todas las interfaces.
      --port 5000
```

### 10.6.3 ğŸ” AnatomÃ­a de `scripts/run_mlflow.py`

**Archivo**: `ML-MLOps-Portfolio/BankChurn-Predictor/scripts/run_mlflow.py`

Este script es el **puente** entre el entrenamiento local y el servidor MLflow centralizado.

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE 1: ConfiguraciÃ³n Flexible vÃ­a Variables de Entorno
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main() -> None:
    tracking_uri = os.getenv("MLFLOW_TRACKING_URI", "file:./mlruns")
    # Â¿Por quÃ© os.getenv con default?
    # - En desarrollo: usa "file:./mlruns" (local, sin servidor).
    # - En CI/CD: setea MLFLOW_TRACKING_URI=http://mlflow:5000.
    # - En producciÃ³n: apunta al servidor real.
    
    experiment = os.getenv("MLFLOW_EXPERIMENT_NAME") or "BankChurn"
    # PatrÃ³n "or": si la variable estÃ¡ vacÃ­a (""), usa el default.

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE 2: Carga y TransformaciÃ³n de MÃ©tricas
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    results_path = Path("artifacts/training_results.json")
    if results_path.exists():
        data = json.loads(results_path.read_text())
        
        # Extraer mÃ©tricas de CV (cross-validation)
        cv = data.get("cv_results", {})
        for k, v in cv.items():
            if isinstance(v, (int, float)):      # Solo loguea valores numÃ©ricos.
                metrics[f"cv_{k}"] = float(v)    # Prefijo "cv_" para distinguir.
        
        # Extraer mÃ©tricas de test
        test_metrics = data.get("test_results", {}).get("metrics", {})
        for k, v in test_metrics.items():
            metrics[f"test_{k}"] = float(v)      # Prefijo "test_" para distinguir.
# Â¿Por quÃ© prefijos? En MLflow UI puedes filtrar por "cv_*" vs "test_*".

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE 3: MÃ©tricas de Negocio (Lo que distingue a un Senior)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        cm = test_results.get("confusion_matrix")  # [[TN, FP], [FN, TP]]
        if cm:
            tn, fp = cm[0]
            fn, tp = cm[1]
            
            clv = float(os.getenv("BC_CLV_USD", "2300"))  # Customer Lifetime Value.
            retention_rate = float(os.getenv("BC_RETENTION_RATE", "0.3"))
            
            saved_customers = float(tp) * retention_rate
            saved_revenue = saved_customers * clv
            
            business_metrics = {
                "biz_saved_customers_proxy": saved_customers,
                "biz_saved_revenue_proxy_usd": saved_revenue,
            }
# Â¿Por quÃ© mÃ©tricas de negocio?
# - "F1=0.85" no significa nada para el negocio.
# - "$690,000 en revenue salvado" sÃ­ justifica el proyecto.

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE 4: Logging con Manejo Robusto de Errores
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with mlflow.start_run(run_name="demo-logging"):
        mlflow.log_params({"run_type": "demo"})
        mlflow.log_metrics(metrics)
        mlflow.log_metrics(business_metrics)
        
        # Artifacts: best-effort (puede fallar si el store no es accesible)
        for p in [Path("artifacts/training_results.json"), Path("configs/config.yaml")]:
            if p.exists():
                try:
                    mlflow.log_artifact(str(p))
                except PermissionError:
                    print(f"Skipping artifact {p}: permission denied")
                    # NO crashea el script, solo advierte.
# Â¿Por quÃ© try/except en artifacts?
# - En CI/CD, el artifact store puede no ser accesible desde el runner.
# - Mejor loguear mÃ©tricas (crÃ­tico) que fallar por artifacts (nice-to-have).
```

### 10.6.4 ğŸ” AnatomÃ­a de `scripts/promote_model.py`

**Archivo**: `ML-MLOps-Portfolio/scripts/promote_model.py`

Este script implementa el **flujo CD** para modelos: validaciÃ³n â†’ registro â†’ promociÃ³n.

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE 1: ConfiguraciÃ³n Multi-Proyecto
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PROJECT_CONFIGS = {
    "bankchurn": {
        "dir": "BankChurn-Predictor",
        "model_name": "BankChurn-Classifier",
        "model_path": "models/best_model.pkl",
        "metrics_path": "artifacts/metrics.json",
        "default_thresholds": {"f1": 0.50, "auc": 0.75},  # Umbrales mÃ­nimos.
    },
    "carvision": {...},
    "telecom": {...},
}
# Â¿Por quÃ© un dict de configs?
# - Un solo script maneja los 3 proyectos del portafolio.
# - Cada proyecto tiene sus propios umbrales (clasificaciÃ³n vs regresiÃ³n).

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE 2: ValidaciÃ³n de MÃ©tricas (Quality Gate)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def validate_metrics(metrics: dict, thresholds: dict) -> tuple[bool, list[str]]:
    failures = []
    for threshold_name, threshold_value in thresholds.items():
        actual_value = metrics.get(threshold_name)
        if actual_value is not None:
            # RMSE: menor es mejor. Otros: mayor es mejor.
            if threshold_name == "rmse":
                if actual_value > threshold_value:
                    failures.append(f"{threshold_name}: {actual_value:.4f} > {threshold_value}")
            else:
                if actual_value < threshold_value:
                    failures.append(f"{threshold_name}: {actual_value:.4f} < {threshold_value}")
    return len(failures) == 0, failures
# Â¿Por quÃ© validar antes de promover?
# - Evita poner en producciÃ³n un modelo que empeorÃ³.
# - Es el "quality gate" del flujo CD para ML.

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOQUE 3: PromociÃ³n Condicional
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if promote and passed:
    client = MlflowClient()
    versions = client.search_model_versions(f"name='{model_name}'")
    if versions:
        latest_version = max(versions, key=lambda v: int(v.version))
        client.transition_model_version_stage(
            name=model_name,
            version=latest_version.version,
            stage="Production",
            archive_existing_versions=True,  # Archiva la versiÃ³n anterior.
        )
# Â¿Por quÃ© archive_existing_versions=True?
# - Solo una versiÃ³n puede estar en "Production" a la vez.
# - Las versiones anteriores se mueven a "Archived" (no se borran).
```

### 10.6.5 ğŸ§ª Laboratorio de ReplicaciÃ³n

**Tu misiÃ³n**: Levantar el stack MLflow completo y registrar tu primer modelo.

1. **Levanta la infraestructura**:
   ```bash
   cd /ruta/a/ML-MLOps-Portfolio
   docker-compose -f docker-compose.mlflow.yml up -d
   
   # Verifica que todo estÃ© healthy
   docker-compose -f docker-compose.mlflow.yml ps
   ```

2. **Accede a las UIs**:
   - MLflow: http://localhost:5000
   - MinIO Console: http://localhost:9001 (user: minioadmin, pass: minioadmin)

3. **Conecta desde Python**:
   ```python
   import mlflow
   mlflow.set_tracking_uri("http://localhost:5000")
   mlflow.set_experiment("mi-primer-experimento")
   
   with mlflow.start_run():
       mlflow.log_param("test", "valor")
       mlflow.log_metric("accuracy", 0.95)
       print(f"Run ID: {mlflow.active_run().info.run_id}")
   ```

4. **Verifica en la UI** que el run aparece con params y mÃ©tricas.

### 10.6.6 ğŸš¨ Troubleshooting Preventivo

| SÃ­ntoma | Causa Probable | SoluciÃ³n |
|---------|----------------|----------|
| **"Connection refused" al conectar a MLflow** | Servidor no arrancÃ³ o puerto bloqueado | `docker-compose logs mlflow` para ver errores. Verifica que puerto 5000 estÃ© libre. |
| **"Unable to upload artifact"** | MinIO no accesible o credenciales incorrectas | Verifica `MLFLOW_S3_ENDPOINT_URL` apunta a MinIO. Revisa user/pass. |
| **Artifacts visibles en UI pero no descargables** | Bucket sin permisos de lectura | Ejecuta `mc anonymous set download myminio/mlflow-artifacts`. |
| **Runs no aparecen en el experimento correcto** | `set_experiment()` no llamado antes de `start_run()` | Siempre llama `mlflow.set_experiment("nombre")` antes. |
| **"Model registry is not available"** | Backend store es file-based | El registry requiere una DB real (PostgreSQL/MySQL). No funciona con `file:./mlruns`. |

---

<a id="errores-habituales"></a>

## ğŸ§¨ Errores habituales y cÃ³mo depurarlos en MLflow

MLflow aÃ±ade una capa extra (servidor, rutas, artefactos), asÃ­ que muchos errores son de **configuraciÃ³n** mÃ¡s que de cÃ³digo puro.

Si alguno de estos errores te tomÃ³ **>15 minutos**, regÃ­stralo en el **[Diario de Errores](study_tools/DIARIO_ERRORES.md)** y aplica el flujo de **rescate cognitivo** de **[Protocolo E](study_tools/PROTOCOLO_E.md)**.

### 1) Runs que no aparecen en la UI (tracking_uri/experimento incorrectos)

**SÃ­ntomas tÃ­picos**

- Ejecutas training o `run_mlflow.py` y no ves nada nuevo en `http://localhost:5000`.

**CÃ³mo identificarlo**

- Imprime `mlflow.get_tracking_uri()` y el experimento actual (`mlflow.get_experiment_by_name(...)`).
- Verifica si estÃ¡s usando `file:./mlruns` mientras tienes un servidor en Docker (`http://localhost:5000`).

**CÃ³mo corregirlo**

- Define claramente en config:
  - Desarrollo local â†’ `tracking_uri: "file:./mlruns"`.
  - Demo/stack Docker â†’ `tracking_uri: "http://mlflow:5000"` o `http://localhost:5000`.
- AsegÃºrate de que tanto `ChurnTrainer` como `scripts/run_mlflow.py` lean del mismo origen (YAML/env vars).

---

### 2) Errores al registrar modelos (`MlflowException`, permisos, backend)

**SÃ­ntomas tÃ­picos**

- Al llamar `mlflow.sklearn.log_model(..., registered_model_name=...)` obtienes errores sobre base de datos o registry no configurado.

**CÃ³mo identificarlo**

- Si usas solo `file:./mlruns` sin servidor, el **registry completo** no estÃ¡ disponible.

**CÃ³mo corregirlo**

- Para desarrollo ligero, limita el uso de registry (puedes usar solo tracking + artifacts).
- Para un registry completo, usa el `docker-compose.mlflow.yml` del portafolio con backend SQLite/postgres y apunta `MLFLOW_TRACKING_URI` al servidor.

---

### 3) Artifacts que no se encuentran o no se suben

**SÃ­ntomas tÃ­picos**

- Errores tipo `FileNotFoundError` al hacer `mlflow.log_artifact`.
- No ves `training_results.json` ni `config.yaml` en la pestaÃ±a de artifacts.

**CÃ³mo identificarlo**

- Revisa rutas relativas en `run_mlflow.py` y asegÃºrate de que ejecutas el script desde la raÃ­z del proyecto.

**CÃ³mo corregirlo**

- Usa rutas consistentes (por ejemplo `artifacts/training_results.json`) y verifica que el archivo exista antes de loguearlo.
- Si corres dentro de Docker, revisa que el volumen monte correctamente `artifacts/` y `configs/`.

---

### 4) Problemas con MLflow en Docker (puertos, hostnames, permisos)

**SÃ­ntomas tÃ­picos**

- `ConnectionError` al intentar conectar a `http://localhost:5000` desde un contenedor.
- Logs que muestran errores de permisos en `/mlflow`.

**CÃ³mo identificarlo**

- Examina `docker-compose.mlflow.yml` y las variables de entorno de tus servicios.

**CÃ³mo corregirlo**

- Dentro de un contenedor, usa el hostname del servicio (`http://mlflow:5000`) en lugar de `localhost`.
- AsegÃºrate de que el volumen `mlflow-artifacts` tenga permisos de escritura correctos (usuario del contenedor).

---

### PatrÃ³n general de debugging en MLflow

1. **Comprueba tracking_uri y experimento** antes de iniciar el run.
2. **Valida artifacts y modelos**: que los paths existen y se cargan correctamente.
3. **Reproduce localmente con file store** (`file:./mlruns`) antes de ir a servidor Docker.
4. **Verifica desde la UI** que params, metrics y artifacts coincidan con lo que esperas de tu cÃ³digo.

Con este patrÃ³n, MLflow pasa de ser â€œcaja negraâ€ a una herramienta confiable para explicar, comparar y promover modelos.

---

<a id="ejercicio"></a>

## âœ… Ejercicio: Integrar MLflow en TelecomAI

1. Crea `scripts/run_mlflow.py` para TelecomAI
2. Log las mÃ©tricas: accuracy, f1, precision, recall, roc_auc
3. Calcula mÃ©tricas de negocio (customers retained, revenue saved)
4. Registra el modelo como "TelecomPlanClassifier"

---

<a id="checkpoint"></a>

## âœ… Checkpoint

- [ ] Puedo correr un run end-to-end y verlo en la UI (o en `mlflow ui`).
- [ ] Puedo explicar dÃ³nde quedan:
  - params
  - metrics
  - artifacts
- [ ] Puedo comparar 2 runs y justificar quÃ© cambiÃ³ (params â†’ mÃ©tricas).

---

## ğŸ“¦ CÃ³mo se UsÃ³ en el Portafolio

MLflow estÃ¡ integrado en los 3 proyectos del portafolio:

### ConfiguraciÃ³n MLflow en BankChurn

```python
# BankChurn-Predictor/src/bankchurn/config.py
class MLflowConfig(BaseModel):
    """MLflow tracking configuration."""
    tracking_uri: str = "file:./mlruns"  # Local por defecto
    experiment_name: str = "bankchurn"
    enabled: bool = True
```

### IntegraciÃ³n en Trainer

```python
# BankChurn-Predictor/src/bankchurn/trainer.py (extracto)
def _log_to_mlflow(self):
    """Log experimento a MLflow."""
    if not self.config.mlflow.enabled:
        return
    
    mlflow.set_tracking_uri(self.config.mlflow.tracking_uri)
    mlflow.set_experiment(self.config.mlflow.experiment_name)
    
    with mlflow.start_run():
        # ParÃ¡metros
        mlflow.log_params({
            "model_type": self.config.model.type,
            "test_size": self.config.model.test_size,
            "cv_folds": self.config.model.cv_folds,
        })
        
        # MÃ©tricas
        mlflow.log_metrics(self.metrics_)
        
        # Modelo
        mlflow.sklearn.log_model(self.model_, "model")
```

### Estructura de mlruns/

```
BankChurn-Predictor/
â””â”€â”€ mlruns/
    â”œâ”€â”€ 0/                    # Default experiment
    â””â”€â”€ 123456789/            # bankchurn experiment
        â””â”€â”€ abc123def456/     # Run ID
            â”œâ”€â”€ artifacts/
            â”‚   â””â”€â”€ model/
            â”œâ”€â”€ metrics/
            â”‚   â”œâ”€â”€ accuracy
            â”‚   â”œâ”€â”€ f1_score
            â”‚   â””â”€â”€ roc_auc
            â”œâ”€â”€ params/
            â”‚   â”œâ”€â”€ model_type
            â”‚   â””â”€â”€ cv_folds
            â””â”€â”€ meta.yaml
```

### MLflow por Proyecto

| Proyecto | Tracking URI | Experiment | MÃ©tricas Principales |
|----------|--------------|------------|---------------------|
| BankChurn | `file:./mlruns` | `bankchurn` | accuracy, f1, roc_auc |
| CarVision | `file:./mlruns` | `carvision` | mae, rmse, r2 |
| TelecomAI | `file:./mlruns` | `telecomai` | accuracy, f1_weighted |

### ğŸ”§ Ejercicio: Explora MLflow Real

```bash
# 1. Ve a BankChurn
cd BankChurn-Predictor

# 2. Entrena con MLflow habilitado
python main.py --config configs/config.yaml

# 3. Inicia la UI de MLflow
mlflow ui --backend-store-uri file:./mlruns

# 4. Abre en navegador
# http://localhost:5000

# 5. Explora:
# - Compara runs
# - Ve artifacts
# - Registra modelo en Model Registry
```

---

## ğŸ’¼ Consejos Profesionales

> **Recomendaciones para destacar en entrevistas y proyectos reales**

### Para Entrevistas

1. **MLflow vs W&B vs Neptune**: Conoce trade-offs (MLflow open-source, W&B mejor UI, Neptune escalabilidad).

2. **Model Registry**: Explica stages (Staging â†’ Production â†’ Archived).

3. **Reproducibilidad**: CÃ³mo reconstruir cualquier experimento desde el tracking.

### Para Proyectos Reales

| SituaciÃ³n | Consejo |
|-----------|---------|
| Equipo distribuido | Usa servidor MLflow centralizado |
| Muchos experimentos | Organiza con tags y naming conventions |
| Modelos grandes | Usa artifact storage externo (S3, GCS) |
| ComparaciÃ³n | Siempre registra baseline para comparar |

### QuÃ© Trackear Siempre

- **Params**: HiperparÃ¡metros, versiones de datos
- **Metrics**: Train/val/test, mÃ©tricas de negocio
- **Artifacts**: Modelo, configs, plots, requirements.txt
- **Tags**: Git commit, autor, dataset version


---

## ğŸ“º Recursos Externos del MÃ³dulo

> ğŸ·ï¸ Sistema: ğŸ”´ Obligatorio | ğŸŸ¡ Recomendado | ğŸŸ¢ Complementario

### ğŸ¬ Videos

| ğŸ·ï¸ | TÃ­tulo | Canal | DuraciÃ³n | Link |
|:--:|:-------|:------|:--------:|:-----|
| ğŸ”´ | **MLflow Tutorial** | Krish Naik | 40 min | [YouTube](https://www.youtube.com/watch?v=qdcHHrsXA48) |
| ğŸ”´ | **MLflow Complete Course** | DataTalksClub | 1.5h | [YouTube](https://www.youtube.com/watch?v=MHcqGxA6JPs) |
| ğŸŸ¡ | **Weights & Biases Quickstart** | W&B | 20 min | [YouTube](https://www.youtube.com/watch?v=BN2BT0SZSJw) |

### ğŸ“„ DocumentaciÃ³n

| ğŸ·ï¸ | Recurso | DescripciÃ³n |
|:--:|:--------|:------------|
| ğŸ”´ | [MLflow Tracking](https://mlflow.org/docs/latest/tracking.html) | GuÃ­a oficial tracking |
| ğŸ”´ | [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html) | Registro de modelos |

---

## âš–ï¸ DecisiÃ³n TÃ©cnica: ADR-010 MLflow

**Contexto**: Necesitamos trackear experimentos y versionar modelos.

**DecisiÃ³n**: Usar MLflow para experiment tracking y model registry.

**Alternativas Consideradas**:
- **Weights & Biases**: Mejor UI pero SaaS (costo)
- **Neptune**: Escalable pero pago
- **TensorBoard**: Solo para deep learning

**Consecuencias**:
- âœ… Open source, self-hosted
- âœ… Model Registry integrado
- âœ… IntegraciÃ³n con sklearn, PyTorch, etc.
- âŒ UI menos pulida que W&B

---

## ğŸ”§ Ejercicios del MÃ³dulo

### Ejercicio 10.1: MLflow BÃ¡sico
**Objetivo**: Trackear un experimento con MLflow.
**Dificultad**: â­â­

```python
import mlflow

# TU TAREA: Completar el tracking
def train_with_mlflow(X_train, y_train, X_test, y_test, params):
    # 1. Iniciar run
    # 2. Log params
    # 3. Entrenar modelo
    # 4. Log metrics
    # 5. Log model
    pass
```

<details>
<summary>ğŸ’¡ Ver soluciÃ³n</summary>

```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, accuracy_score

def train_with_mlflow(X_train, y_train, X_test, y_test, params: dict):
    """Entrena modelo con tracking completo en MLflow."""
    
    # Configurar experimento
    mlflow.set_experiment("bankchurn-classifier")
    
    with mlflow.start_run():
        # 1. Log parÃ¡metros
        mlflow.log_params(params)
        
        # 2. Entrenar modelo
        model = RandomForestClassifier(
            n_estimators=params['n_estimators'],
            max_depth=params.get('max_depth'),
            random_state=42
        )
        model.fit(X_train, y_train)
        
        # 3. Evaluar
        y_pred = model.predict(X_test)
        metrics = {
            'f1': f1_score(y_test, y_pred),
            'accuracy': accuracy_score(y_test, y_pred)
        }
        
        # 4. Log mÃ©tricas
        mlflow.log_metrics(metrics)
        
        # 5. Log modelo
        mlflow.sklearn.log_model(
            model,
            "model",
            registered_model_name="bankchurn-rf"
        )
        
        # 6. Log artifacts adicionales
        # mlflow.log_artifact("configs/config.yaml")
        
        print(f"Run ID: {mlflow.active_run().info.run_id}")
        return model, metrics

# Uso:
params = {'n_estimators': 100, 'max_depth': 10}
model, metrics = train_with_mlflow(X_train, y_train, X_test, y_test, params)
```
</details>

---

### Ejercicio 10.2: Comparar Experimentos
**Objetivo**: Ejecutar y comparar mÃºltiples configuraciones.
**Dificultad**: â­â­â­

```python
# TU TAREA: Ejecutar grid de experimentos y encontrar el mejor

configs = [
    {'n_estimators': 50, 'max_depth': 5},
    {'n_estimators': 100, 'max_depth': 10},
    {'n_estimators': 200, 'max_depth': 15},
]

# Â¿CÃ³mo organizarÃ­as estos experimentos en MLflow?
# Â¿CÃ³mo encontrarÃ­as el mejor?
```

<details>
<summary>ğŸ’¡ Ver soluciÃ³n</summary>

```python
import mlflow
from mlflow.tracking import MlflowClient

def run_experiments(X_train, y_train, X_test, y_test, configs: list):
    """Ejecuta mÃºltiples configuraciones y las compara."""
    
    mlflow.set_experiment("bankchurn-hyperparameter-search")
    
    results = []
    for config in configs:
        with mlflow.start_run():
            # Tag para identificar el experimento
            mlflow.set_tag("config_name", f"rf_{config['n_estimators']}_{config['max_depth']}")
            
            # Entrenar y evaluar
            model, metrics = train_model(X_train, y_train, X_test, y_test, config)
            
            results.append({
                'run_id': mlflow.active_run().info.run_id,
                'config': config,
                'f1': metrics['f1']
            })
    
    return results

def find_best_run(experiment_name: str, metric: str = "f1"):
    """Encuentra el mejor run de un experimento."""
    client = MlflowClient()
    experiment = client.get_experiment_by_name(experiment_name)
    
    runs = client.search_runs(
        experiment_ids=[experiment.experiment_id],
        order_by=[f"metrics.{metric} DESC"],
        max_results=1
    )
    
    if runs:
        best = runs[0]
        print(f"Best run: {best.info.run_id}")
        print(f"Best {metric}: {best.data.metrics[metric]}")
        print(f"Params: {best.data.params}")
        return best
    return None

# Ejecutar experimentos
results = run_experiments(X_train, y_train, X_test, y_test, configs)

# Encontrar el mejor
best_run = find_best_run("bankchurn-hyperparameter-search", "f1")

# Promover a producciÃ³n
client = MlflowClient()
client.transition_model_version_stage(
    name="bankchurn-rf",
    version=1,
    stage="Production"
)
```
</details>

---

## ğŸ”— Glosario del MÃ³dulo

| TÃ©rmino | DefiniciÃ³n |
|---------|------------|
| **MLflow** | Plataforma open source para gestiÃ³n del ciclo de vida ML |
| **Run** | Una ejecuciÃ³n de un experimento con parÃ¡metros especÃ­ficos |
| **Model Registry** | Sistema para versionar y gestionar modelos en producciÃ³n |
| **Artifact** | Archivo guardado junto con un run (modelo, plots, configs) |

---

## ğŸ FIN DE FASE 2: ML Engineering

> ğŸ¯ **Â¡Has completado los mÃ³dulos 07-10!**
>
> Ahora dominas las tÃ©cnicas de ML Engineering profesional:
> - âœ… Pipelines sklearn reproducibles
> - âœ… Feature engineering sin data leakage
> - âœ… Training profesional con cross-validation
> - âœ… Experiment tracking con MLflow

**Siguiente**: Fase 3 - MLOps Core (Testing, CI/CD, Docker, APIs)

---

<div align="center">

**Siguiente mÃ³dulo** â†’ [11. Testing ML](11_TESTING_ML.md)

---

[â† Volver al Ãndice](00_INDICE.md)

</div>
